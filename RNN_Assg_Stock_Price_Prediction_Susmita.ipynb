{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cjf0S2yqD0uf"
   },
   "source": [
    "# Stock Price Prediction Using RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhNGhRQWjN94"
   },
   "source": [
    "## Objective\n",
    "The objective of this assignment is to try and predict the stock prices using historical data from four companies IBM (IBM), Google (GOOGL), Amazon (AMZN), and Microsoft (MSFT).\n",
    "\n",
    "We use four different companies because they belong to the same sector: Technology. Using data from all four companies may improve the performance of the model. This way, we can capture the broader market sentiment.\n",
    "\n",
    "The problem statement for this assignment can be summarised as follows:\n",
    "\n",
    "> Given the stock prices of Amazon, Google, IBM, and Microsoft for a set number of days, predict the stock price of these companies after that window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4gU_Gs--yjG"
   },
   "source": [
    "## Business Value\n",
    "\n",
    "Data related to stock markets lends itself well to modeling using RNNs due to its sequential nature. We can keep track of opening prices, closing prices, highest prices, and so on for a long period of time as these values are generated every working day. The patterns observed in this data can then be used to predict the future direction in which stock prices are expected to move. Analyzing this data can be interesting in itself, but it also has a financial incentive as accurate predictions can lead to massive profits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC424ieDIlaK"
   },
   "source": [
    "### **Data Description**\n",
    "\n",
    "You have been provided with four CSV files corresponding to four stocks: AMZN, GOOGL, IBM, and MSFT. The files contain historical data that were gathered from the websites of the stock markets where these companies are listed: NYSE and NASDAQ. The columns in all four files are identical. Let's take a look at them:\n",
    "\n",
    "- `Date`: The values in this column specify the date on which the values were recorded. In all four files, the dates range from Jaunary 1, 2006 to January 1, 2018.\n",
    "\n",
    "- `Open`: The values in this column specify the stock price on a given date when the stock market opens.\n",
    "\n",
    "- `High`: The values in this column specify the highest stock price achieved by a stock on a given date.\n",
    "\n",
    "- `Low`: The values in this column specify the lowest stock price achieved by a stock on a given date.\n",
    "\n",
    "- `Close`: The values in this column specify the stock price on a given date when the stock market closes.\n",
    "\n",
    "- `Volume`: The values in this column specify the total number of shares traded on a given date.\n",
    "\n",
    "- `Name`: This column gives the official name of the stock as used in the stock market.\n",
    "\n",
    "There are 3019 records in each data set. The file names are of the format `\\<company_name>_stock_data.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzV7hepM8K-H"
   },
   "source": [
    "## **1 Data Loading and Preparation** <font color =red> [25 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6UQ2zyxH1Ep"
   },
   "source": [
    "#### **Import Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RyZsIlDgfO3s"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time\n",
    "\n",
    "# For data preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# For deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, SimpleRNN, LSTM, GRU, Dropout\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-v0_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4_Xnhwsl-00"
   },
   "source": [
    "### **1.1 Data Aggregation** <font color =red> [7 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5clrTkdAvq8G"
   },
   "source": [
    "As we are using the stock data for four different companies, we need to create a new DataFrame that contains the combined data from all four data frames. We will create a function that takes in a list of the file names for the four CSV files, and returns a single data frame. This function performs the following tasks:\n",
    "- Extract stock names from file names\n",
    "- Read the CSV files as data frames\n",
    "- Append the stock names into the columns of their respective data frames\n",
    "- Drop unnecessary columns\n",
    "- Join the data frames into one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3tPMCwNwe1JS"
   },
   "source": [
    "#### **1.1.1** <font color =red> [5 marks] </font>\n",
    "Create the function to join DataFrames and use it to combine the four datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-gj0K2Q65yix"
   },
   "outputs": [],
   "source": [
    "# Define a function to load data and aggregate them\n",
    "\n",
    "def load_and_aggregate_data(file_list):\n",
    "    \"\"\"\n",
    "    Function to load and aggregate stock data from multiple CSV files.\n",
    "    \n",
    "    Parameters:\n",
    "    file_list (list): List of CSV file names\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Combined DataFrame with all stock data\n",
    "    \"\"\"\n",
    "    combined_data = pd.DataFrame()\n",
    "    \n",
    "    for file_name in file_list:\n",
    "        # Extract stock name from file name (remove '_stocks_data.csv')\n",
    "        stock_name = file_name.replace('_stocks_data.csv', '').replace('_stock_data.csv', '')\n",
    "        \n",
    "        # Read the CSV file\n",
    "        file_path = f'RNN_Stocks_Data/{file_name}'\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert Date column to datetime\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        \n",
    "        # Add stock name suffix to relevant columns\n",
    "        columns_to_rename = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "        for col in columns_to_rename:\n",
    "            if col in df.columns:\n",
    "                df[f'{col}{stock_name}'] = df[col]\n",
    "        \n",
    "        # Drop original columns and Name column if it exists\n",
    "        columns_to_drop = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "        if 'Name' in df.columns:\n",
    "            columns_to_drop.append('Name')\n",
    "        \n",
    "        df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])\n",
    "        \n",
    "        # Merge with combined data\n",
    "        if combined_data.empty:\n",
    "            combined_data = df.copy()\n",
    "        else:\n",
    "            combined_data = pd.merge(combined_data, df, on='Date', how='outer')\n",
    "    \n",
    "    # Sort by date\n",
    "    combined_data = combined_data.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    return combined_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0sYevlt7kgn"
   },
   "outputs": [],
   "source": [
    "# Specify the names of the raw data files to be read and use the aggregation function to read the files\n",
    "\n",
    "file_names = [\n",
    "    'AMZN_stocks_data.csv',\n",
    "    'GOOGL_stocks_data.csv', \n",
    "    'IBM_stocks_data.csv',\n",
    "    'MSFT_stocks_data.csv'\n",
    "]\n",
    "\n",
    "# Load and aggregate the data\n",
    "master_data = load_and_aggregate_data(file_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qMKdhGj8cKF"
   },
   "outputs": [],
   "source": [
    "# View specifics of the data\n",
    "print(\"Shape of the combined dataset:\")\n",
    "master_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nColumn names:\")\n",
    "master_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFirst few rows:\")\n",
    "master_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nLast few rows:\")\n",
    "master_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nData types:\")\n",
    "master_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "master_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Null Value Check:\")\n",
    "master_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwVzhIuBfRGn"
   },
   "source": [
    "#### **1.1.2** <font color =red> [2 marks] </font>\n",
    "Identify and handle any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vmQGhf69x36"
   },
   "outputs": [],
   "source": [
    "# Handle Missing Values\n",
    "\n",
    "print(\"Missing values per column:\")\n",
    "print(master_data.isnull().sum())\n",
    "print(\"*\"*50)\n",
    "print(\"*\"*50)\n",
    "\n",
    "print(\"\\nRows with missing values:\")\n",
    "missing_rows = master_data[master_data.isnull().any(axis=1)]\n",
    "print(missing_rows)\n",
    "print(\"*\"*50)\n",
    "print(\"*\"*50)\n",
    "\n",
    "print(f\"\\nTotal rows with missing values: {len(missing_rows)}\")\n",
    "print(f\"Percentage of missing data: {(len(missing_rows)/len(master_data))*100:.2f}%\")\n",
    "print(\"*\"*50)\n",
    "print(\"*\"*50)\n",
    "\n",
    "# Check if missing values are in the same row(s)\n",
    "if len(missing_rows) > 0:\n",
    "    print(f\"\\nMissing values are in row index: {missing_rows.index.tolist()}\")\n",
    "    print(f\"Date of missing values: {missing_rows['Date'].tolist()}\")\n",
    "\n",
    "print(\"Handling missing values using forward fill method...\")\n",
    "\n",
    "# Before handling - show the problematic rows\n",
    "print(\"\\nBefore handling missing values:\")\n",
    "for idx in [1068, 2913]:\n",
    "    print(f\"Row {idx} (Date: {master_data.loc[idx, 'Date'].strftime('%Y-%m-%d')}):\")\n",
    "    row_missing = master_data.loc[idx].isnull()\n",
    "    missing_cols = row_missing[row_missing].index.tolist()\n",
    "    print(f\"  Missing columns: {missing_cols}\")\n",
    "    \n",
    "print(\"*\"*50)\n",
    "print(\"*\"*50)\n",
    "\n",
    "# Apply forward fill to handle missing values\n",
    "master_data_filled = master_data.fillna(method='ffill')\n",
    "\n",
    "# If there are still missing values at the beginning, use backward fill\n",
    "master_data_filled = master_data_filled.fillna(method='bfill')\n",
    "\n",
    "# Verify that all missing values have been handled\n",
    "print(f\"\\nMissing values after handling:\")\n",
    "missing_after = master_data_filled.isnull().sum()\n",
    "print(missing_after)\n",
    "\n",
    "print(\"*\"*50)\n",
    "print(\"*\"*50)\n",
    "\n",
    "# Show the fixed rows\n",
    "print(\"\\nAfter handling missing values:\")\n",
    "for idx in [1068, 2913]:\n",
    "    print(f\"Row {idx} (Date: {master_data_filled.loc[idx, 'Date'].strftime('%Y-%m-%d')}):\")\n",
    "    # Show a few key columns to verify the fill worked\n",
    "    cols_to_show = ['CloseAMZN', 'CloseGOOGL', 'CloseIBM', 'CloseMSFT']\n",
    "    for col in cols_to_show:\n",
    "        print(f\"  {col}: {master_data_filled.loc[idx, col]:.2f}\")\n",
    "\n",
    "print(\"*\"*50)\n",
    "print(\"*\"*50)\n",
    "\n",
    "# Update the master_data with the filled data\n",
    "master_data = master_data_filled.copy()\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {master_data.shape}\")\n",
    "print(\"*\"*50)\n",
    "print(\"*\"*50)\n",
    "print(\"Missing value handling completed successfully!\")\n",
    "print(\"*\"*50)\n",
    "print(\"*\"*50)\n",
    "print(f\"Total missing values remaining: {master_data.isnull().sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuFFxr5jQ4xw"
   },
   "source": [
    "### **1.2 Analysis and Visualisation** <font color =red> [5 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YnybVo_YQ4xx"
   },
   "source": [
    "#### **1.2.1** <font color =red> [2 marks] </font>\n",
    "Analyse the frequency distribution of stock volumes of the companies and also see how the volumes vary over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fGxyKbfRcFl-"
   },
   "outputs": [],
   "source": [
    "# Frequency distribution of volumes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Stock Volume Distribution Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "companies = ['AMZN', 'GOOGL', 'IBM', 'MSFT']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "# Individual histograms for each company\n",
    "for i, (company, color) in enumerate(zip(companies, colors)):\n",
    "    row, col = i // 2, i % 2\n",
    "    volume_col = f'Volume{company}'\n",
    "    \n",
    "    axes[row, col].hist(master_data[volume_col], bins=50, alpha=0.7, color=color, edgecolor='black')\n",
    "    axes[row, col].set_title(f'{company} Volume Distribution', fontweight='bold')\n",
    "    axes[row, col].set_xlabel('Trading Volume')\n",
    "    axes[row, col].set_ylabel('Frequency')\n",
    "    axes[row, col].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add summary statistics as text\n",
    "    mean_vol = master_data[volume_col].mean()\n",
    "    median_vol = master_data[volume_col].median()\n",
    "    axes[row, col].axvline(mean_vol, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_vol:,.0f}')\n",
    "    axes[row, col].axvline(median_vol, color='orange', linestyle='--', linewidth=2, label=f'Median: {median_vol:,.0f}')\n",
    "    axes[row, col].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparative box plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "volume_data = [master_data[f'Volume{company}'] for company in companies]\n",
    "box_plot = plt.boxplot(volume_data, labels=companies, patch_artist=True)\n",
    "\n",
    "# Color the boxes\n",
    "for patch, color in zip(box_plot['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "\n",
    "plt.title('Comparative Volume Distribution Across Companies', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Trading Volume')\n",
    "plt.xlabel('Company')\n",
    "plt.yscale('log')  # Log scale due to large differences in volume ranges\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics table\n",
    "volume_stats = pd.DataFrame()\n",
    "for company in companies:\n",
    "    volume_col = f'Volume{company}'\n",
    "    stats = {\n",
    "        'Mean': master_data[volume_col].mean(),\n",
    "        'Median': master_data[volume_col].median(),\n",
    "        'Std Dev': master_data[volume_col].std(),\n",
    "        'Min': master_data[volume_col].min(),\n",
    "        'Max': master_data[volume_col].max(),\n",
    "        'Q1': master_data[volume_col].quantile(0.25),\n",
    "        'Q3': master_data[volume_col].quantile(0.75)\n",
    "    }\n",
    "    volume_stats[company] = stats\n",
    "\n",
    "print(\"Volume Distribution Summary Statistics:\")\n",
    "print(volume_stats.round(0).astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Q1g9Y0JcNW0"
   },
   "outputs": [],
   "source": [
    "# Stock volume variation over time\n",
    "# Set up the plotting area\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Individual company volume trends over time\n",
    "ax1 = axes[0]\n",
    "for company, color in zip(companies, colors):\n",
    "    volume_col = f'Volume{company}'\n",
    "    ax1.plot(master_data['Date'], master_data[volume_col], \n",
    "            label=f'{company}', color=color, alpha=0.7, linewidth=1)\n",
    "\n",
    "ax1.set_title('Stock Volume Trends Over Time (2006-2018)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Trading Volume')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_yscale('log')  # Log scale for better visualization\n",
    "\n",
    "# Plot 2: Moving average trends\n",
    "ax2 = axes[1]\n",
    "window = 90  # 90-day moving average\n",
    "\n",
    "for company, color in zip(companies, colors):\n",
    "    volume_col = f'Volume{company}'\n",
    "    moving_avg = master_data[volume_col].rolling(window=window).mean()\n",
    "    ax2.plot(master_data['Date'], moving_avg, \n",
    "            label=f'{company} ({window}-day MA)', color=color, linewidth=2)\n",
    "\n",
    "ax2.set_title(f'{window}-Day Moving Average of Trading Volumes', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Date')\n",
    "ax2.set_ylabel('Trading Volume (Moving Average)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Yearly volume analysis\n",
    "master_data['Year'] = master_data['Date'].dt.year\n",
    "yearly_volume = pd.DataFrame()\n",
    "\n",
    "for company in companies:\n",
    "    volume_col = f'Volume{company}'\n",
    "    yearly_avg = master_data.groupby('Year')[volume_col].mean()\n",
    "    yearly_volume[company] = yearly_avg\n",
    "\n",
    "# Plot yearly average volumes\n",
    "plt.figure(figsize=(14, 8))\n",
    "for company, color in zip(companies, colors):\n",
    "    plt.plot(yearly_volume.index, yearly_volume[company], \n",
    "            marker='o', linewidth=3, markersize=8, color=color, label=company)\n",
    "\n",
    "plt.title('Average Annual Trading Volume by Company', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Trading Volume')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Yearly Average Volume Summary:\")\n",
    "print(yearly_volume.round(0).astype(int))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QO5frnjURc5i"
   },
   "source": [
    "#### **1.2.2** <font color =red> [3 marks] </font>\n",
    "Analyse correlations between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SE9ijOnmP94M"
   },
   "outputs": [],
   "source": [
    "# Analyse correlations\n",
    "\n",
    "# Create correlation matrices for different aspects of the data\n",
    "\n",
    "# 1. Correlation between closing prices of all companies\n",
    "closing_prices = master_data[['CloseAMZN', 'CloseGOOGL', 'CloseIBM', 'CloseMSFT']]\n",
    "price_corr = closing_prices.corr()\n",
    "\n",
    "# 2. Correlation between all features for each company (example with AMZN)\n",
    "amzn_features = master_data[['OpenAMZN', 'HighAMZN', 'LowAMZN', 'CloseAMZN', 'VolumeAMZN']]\n",
    "amzn_corr = amzn_features.corr()\n",
    "\n",
    "# 3. Volume correlations between companies\n",
    "volume_data = master_data[['VolumeAMZN', 'VolumeGOOGL', 'VolumeIBM', 'VolumeMSFT']]\n",
    "volume_corr = volume_data.corr()\n",
    "\n",
    "# Create comprehensive correlation visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 16))\n",
    "fig.suptitle('Comprehensive Correlation Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Closing prices correlation\n",
    "sns.heatmap(price_corr, annot=True, cmap='RdYlBu_r', center=0, \n",
    "            square=True, ax=axes[0,0], cbar_kws={'shrink': 0.8})\n",
    "axes[0,0].set_title('Closing Prices Correlation Between Companies', fontweight='bold')\n",
    "\n",
    "# Plot 2: AMZN features correlation (as example)\n",
    "sns.heatmap(amzn_corr, annot=True, cmap='RdYlBu_r', center=0,\n",
    "            square=True, ax=axes[0,1], cbar_kws={'shrink': 0.8})\n",
    "axes[0,1].set_title('Amazon (AMZN) Feature Correlations', fontweight='bold')\n",
    "\n",
    "# Plot 3: Volume correlations\n",
    "sns.heatmap(volume_corr, annot=True, cmap='RdYlBu_r', center=0,\n",
    "            square=True, ax=axes[1,0], cbar_kws={'shrink': 0.8})\n",
    "axes[1,0].set_title('Trading Volume Correlations Between Companies', fontweight='bold')\n",
    "\n",
    "# Plot 4: Cross-feature correlation matrix (selected features)\n",
    "selected_features = ['CloseAMZN', 'CloseGOOGL', 'CloseIBM', 'CloseMSFT', \n",
    "                    'VolumeAMZN', 'VolumeGOOGL', 'VolumeIBM', 'VolumeMSFT']\n",
    "cross_corr = master_data[selected_features].corr()\n",
    "sns.heatmap(cross_corr, annot=True, cmap='RdYlBu_r', center=0,\n",
    "            square=True, ax=axes[1,1], cbar_kws={'shrink': 0.8})\n",
    "axes[1,1].set_title('Cross-Feature Correlations (Prices & Volumes)', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed correlation analysis\n",
    "print(\"=== CORRELATION ANALYSIS SUMMARY ===\\n\")\n",
    "\n",
    "print(\"1. CLOSING PRICES CORRELATION:\")\n",
    "print(price_corr.round(3))\n",
    "print(\"\\nKey Insights:\")\n",
    "strongest_corr = price_corr.unstack().sort_values(ascending=False)\n",
    "strongest_corr = strongest_corr[strongest_corr < 1.0]  # Remove self-correlations\n",
    "print(f\"Strongest correlation: {strongest_corr.index[0]} = {strongest_corr.iloc[0]:.3f}\")\n",
    "print(f\"Weakest correlation: {strongest_corr.index[-1]} = {strongest_corr.iloc[-1]:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"2. VOLUME CORRELATIONS:\")\n",
    "print(volume_corr.round(3))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"3. PRICE-VOLUME RELATIONSHIP:\")\n",
    "# Calculate correlation between each company's closing price and volume\n",
    "for company in companies:\n",
    "    price_vol_corr = master_data[f'Close{company}'].corr(master_data[f'Volume{company}'])\n",
    "    print(f\"{company}: Price-Volume correlation = {price_vol_corr:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t1dHdCDQl-1K"
   },
   "source": [
    "### **1.3 Data Processing** <font color =red> [13 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjjEYTmOopW4"
   },
   "source": [
    "Next, we need to process the data so that it is ready to be used in recurrent neural networks. You know RNNs are suitable to work with sequential data where patterns repeat at regular intervals.\n",
    "\n",
    "For this, we need to execute the following steps:\n",
    "1. Create windows from the master data frame and obtain windowed `X` and corresponding windowed `y` values\n",
    "2. Perform train-test split on the windowed data\n",
    "3. Scale the data sets in an appropriate manner\n",
    "\n",
    "We will define functions for the above steps that finally return training and testing data sets that are ready to be used in recurrent neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTLu14Wuid5n"
   },
   "source": [
    "**Hint:** If we use a window of size 3, in the first window, the rows `[0, 1, 2]` will be present and will be used to predict the value of `CloseAMZN` in row `3`. In the second window, rows `[1, 2, 3]` will be used to predict `CloseAMZN` in row `4`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mulMxtd6fsFR"
   },
   "source": [
    "#### **1.3.1** <font color =red> [3 marks] </font>\n",
    "Create a function that returns the windowed `X` and `y` data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXQOh3iP6W7L"
   },
   "source": [
    "From the main DataFrame, this function will create windowed DataFrames, and store those as a list of DataFrames.\n",
    "\n",
    "Controllable parameters will be window size, step size (window stride length) and target names as a list of the names of stocks whose closing values we wish to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-n1M1-4fvq8i"
   },
   "outputs": [],
   "source": [
    "# Define a function that divides the data into windows and generates target variable values for each window\n",
    "\n",
    "def create_windowed_data(data, target_names, window_size=30, step_size=1):\n",
    "    \"\"\"\n",
    "    Create windowed sequences from time series data for RNN training.\n",
    "    \n",
    "    Parameters:\n",
    "    data (DataFrame): Master dataset with all features\n",
    "    target_names (list): List of target column names to predict (e.g., ['CloseAMZN'])\n",
    "    window_size (int): Number of time steps in each sequence\n",
    "    step_size (int): Step size between consecutive windows (stride)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X_windows, y_windows) where X_windows is list of input sequences \n",
    "           and y_windows is list of corresponding target values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove Date column for windowing (keep only numeric features)\n",
    "    feature_data = data.drop('Date', axis=1)\n",
    "    \n",
    "    # Initialize lists to store windows\n",
    "    X_windows = []\n",
    "    y_windows = []\n",
    "    \n",
    "    # Create windows with the specified step size\n",
    "    for i in range(0, len(feature_data) - window_size, step_size):\n",
    "        # Extract window of features (input sequence)\n",
    "        window_X = feature_data.iloc[i:i + window_size].values\n",
    "        \n",
    "        # Extract target values at the next time step\n",
    "        window_y = feature_data.iloc[i + window_size][target_names].values\n",
    "        X_windows.append(window_X)\n",
    "        y_windows.append(window_y)\n",
    "    \n",
    "    # Convert to numpy arrays for easier handling\n",
    "    X_windows = np.array(X_windows)\n",
    "    y_windows = np.array(y_windows)\n",
    "    \n",
    "    print(f\"Created {len(X_windows)} windows\")\n",
    "    print(f\"X_windows shape: {X_windows.shape}\")\n",
    "    print(f\"y_windows shape: {y_windows.shape}\")\n",
    "    print(f\"Features per time step: {X_windows.shape[2]}\")\n",
    "    \n",
    "    return X_windows, y_windows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gw5JL8tgmwc-"
   },
   "source": [
    "#### **1.3.2** <font color =red> [3 marks] </font>\n",
    "Create a function to scale the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBaJkaIx_1Vg"
   },
   "source": [
    "Define a function that will scale the data.\n",
    "\n",
    "For scaling, we have to look at the whole length of data to find max/min values or standard deviations and means. If we scale the whole data at once, this will lead to data leakage in the windows. This is not necessarily a problem if the model is trained on the complete data with cross-validation.\n",
    "\n",
    "One way to scale when dealing with windowed data is to use the `partial_fit()` method.\n",
    "```\n",
    "scaler.partial_fit(window)\n",
    "scaler.transform(window)\n",
    "```\n",
    "You may use any other suitable way to scale the data properly. Arrive at a reasonable way to scale your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9586NZptrAi"
   },
   "outputs": [],
   "source": [
    "# Define a function that scales the windowed data\n",
    "# The function takes in the windowed data sets and returns the scaled windows\n",
    "def create_robust_price_windows(data, target_names, window_size=30, step_size=1):\n",
    "    \"\"\"\n",
    "    Create windowed sequences using percentage change from baseline prices.\n",
    "    This approach solves distribution shift while preserving price relationships.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"🔄 Converting to Robust Price-Based Approach...\")\n",
    "    \n",
    "    # Separate features and targets\n",
    "    feature_data = data.drop('Date', axis=1)\n",
    "    \n",
    "    # Store baseline prices (first available price for each stock)\n",
    "    baseline_prices = {}\n",
    "    price_columns = [col for col in feature_data.columns if any(price_type in col for price_type in ['Open', 'High', 'Low', 'Close'])]\n",
    "    \n",
    "    print(\"📊 Calculating percentage changes from baseline prices...\")\n",
    "    robust_data = pd.DataFrame()\n",
    "    robust_data['Date'] = data['Date'].copy()\n",
    "    \n",
    "    for col in feature_data.columns:\n",
    "        if any(price_type in col for price_type in ['Open', 'High', 'Low', 'Close']):\n",
    "            # For price columns: use percentage change from first valid price\n",
    "            first_valid_price = feature_data[col].dropna().iloc[0]\n",
    "            baseline_prices[col] = first_valid_price\n",
    "            \n",
    "            # Convert to percentage change from baseline: (Price - Baseline) / Baseline\n",
    "            pct_change_from_baseline = (feature_data[col] - first_valid_price) / first_valid_price\n",
    "            robust_data[f'{col}_PctFromBase'] = pct_change_from_baseline\n",
    "            \n",
    "        elif 'Volume' in col:\n",
    "            # For volume: use log transformation to handle wide range\n",
    "            # Add small constant to avoid log(0)\n",
    "            log_volume = np.log(feature_data[col] + 1)\n",
    "            robust_data[f'{col}_Log'] = log_volume\n",
    "        else:\n",
    "            # For other columns: keep as is\n",
    "            robust_data[col] = feature_data[col]\n",
    "    \n",
    "    print(f\"✅ Robust conversion completed. New shape: {robust_data.shape}\")\n",
    "    \n",
    "    # Show baseline prices for reference\n",
    "    print(f\"📈 Baseline prices (for percentage calculation):\")\n",
    "    for col, baseline in baseline_prices.items():\n",
    "        if 'Close' in col:  # Show only closing prices for clarity\n",
    "            print(f\"   {col}: ${baseline:.2f}\")\n",
    "    \n",
    "    # Show target statistics\n",
    "    target_columns = [f'{target}_PctFromBase' for target in target_names]\n",
    "    for target_col in target_columns:\n",
    "        if target_col in robust_data.columns:\n",
    "            print(f\"📊 {target_col} statistics:\")\n",
    "            print(f\"   Range: [{robust_data[target_col].min():.4f}, {robust_data[target_col].max():.4f}]\")\n",
    "            print(f\"   Mean: {robust_data[target_col].mean():.4f}, Std: {robust_data[target_col].std():.4f}\")\n",
    "    \n",
    "    # Remove Date column for windowing\n",
    "    robust_features = robust_data.drop('Date', axis=1)\n",
    "    \n",
    "    # Create windows from robust data\n",
    "    X_windows = []\n",
    "    y_windows = []\n",
    "    \n",
    "    for i in range(0, len(robust_features) - window_size, step_size):\n",
    "        # Extract window of features (input sequence)\n",
    "        window_X = robust_features.iloc[i:i + window_size].values\n",
    "        \n",
    "        # Extract target values at the next time step\n",
    "        window_y = robust_features.iloc[i + window_size][target_columns].values\n",
    "        \n",
    "        X_windows.append(window_X)\n",
    "        y_windows.append(window_y)\n",
    "    \n",
    "    X_windows = np.array(X_windows)\n",
    "    y_windows = np.array(y_windows)\n",
    "    \n",
    "    print(f\"✅ Created {len(X_windows)} robust price-based windows\")\n",
    "    print(f\"📊 X_windows shape: {X_windows.shape}\")\n",
    "    print(f\"📊 y_windows shape: {y_windows.shape}\")\n",
    "    \n",
    "    return X_windows, y_windows, robust_data, baseline_prices\n",
    "\n",
    "def scale_robust_price_data(X_train, X_test, y_train, y_test, scaler_type='StandardScaler'):\n",
    "    \"\"\"\n",
    "    Scale robust price data using appropriate scaling for percentage-based features.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🎯 Scaling robust price data using {scaler_type}...\")\n",
    "    \n",
    "    # Choose scaler\n",
    "    if scaler_type == 'StandardScaler':\n",
    "        feature_scaler = StandardScaler()\n",
    "        target_scaler = StandardScaler()\n",
    "    elif scaler_type == 'RobustScaler':\n",
    "        from sklearn.preprocessing import RobustScaler\n",
    "        feature_scaler = RobustScaler()\n",
    "        target_scaler = RobustScaler()\n",
    "    else:\n",
    "        feature_scaler = MinMaxScaler(feature_range=(-2, 2))  # Wider range for percentage changes\n",
    "        target_scaler = MinMaxScaler(feature_range=(-2, 2))\n",
    "    \n",
    "    # Get shapes\n",
    "    train_shape = X_train.shape\n",
    "    test_shape = X_test.shape\n",
    "    \n",
    "    # Reshape for scaling\n",
    "    X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "    X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
    "    \n",
    "    # Fit scalers only on training data\n",
    "    print(\"🔧 Fitting scalers on training data...\")\n",
    "    feature_scaler.fit(X_train_reshaped)\n",
    "    target_scaler.fit(y_train)\n",
    "    \n",
    "    # Transform data\n",
    "    X_train_scaled = feature_scaler.transform(X_train_reshaped).reshape(train_shape)\n",
    "    X_test_scaled = feature_scaler.transform(X_test_reshaped).reshape(test_shape)\n",
    "    y_train_scaled = target_scaler.transform(y_train)\n",
    "    y_test_scaled = target_scaler.transform(y_test)\n",
    "    \n",
    "    print(\"✅ Robust scaling completed!\")\n",
    "    print(f\"📊 Scaled data ranges:\")\n",
    "    print(f\"   X_train: [{X_train_scaled.min():.3f}, {X_train_scaled.max():.3f}]\")\n",
    "    print(f\"   y_train: [{y_train_scaled.min():.3f}, {y_train_scaled.max():.3f}]\")\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, feature_scaler, target_scaler\n",
    "\n",
    "def convert_predictions_to_prices(predictions_scaled, target_scaler, baseline_prices, target_names):\n",
    "    \"\"\"\n",
    "    Convert scaled predictions back to actual prices using baseline prices.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Inverse transform to get percentage changes from baseline\n",
    "    predictions_pct = target_scaler.inverse_transform(predictions_scaled)\n",
    "    \n",
    "    # Convert percentage changes back to actual prices\n",
    "    predicted_prices = []\n",
    "    for i, target in enumerate(target_names):\n",
    "        baseline_price = baseline_prices[target]\n",
    "        # Price = Baseline * (1 + percentage_change)\n",
    "        prices = baseline_price * (1 + predictions_pct[:, i])\n",
    "        predicted_prices.append(prices)\n",
    "    \n",
    "    return np.column_stack(predicted_prices) if len(predicted_prices) > 1 else predicted_prices[0].reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3HhUS59DaCQ"
   },
   "source": [
    "Next, define the main function that will call the windowing and scaling helper functions.\n",
    "\n",
    "The input parameters for this function are:\n",
    "- The joined master data set\n",
    "- The names of the stocks that we wish to predict the *Close* prices for\n",
    "- The window size\n",
    "- The window stride\n",
    "- The train-test split ratio\n",
    "\n",
    "The outputs from this function are the scaled dataframes:\n",
    "- *X_train*\n",
    "- *y_train*\n",
    "- *X_test*\n",
    "- *y_test*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tND37dF1ksmy"
   },
   "source": [
    "#### **1.3.3** <font color =red> [3 marks] </font>\n",
    "Define a function to create windows of `window_size` and split the windowed data in to training and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3acwSBoArWU"
   },
   "source": [
    "The function can take arguments such as list of target names, window size, window stride and split ratio. Use the windowing function here to make windows in the data and then perform scaling and train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvWRH5J4vq8l"
   },
   "outputs": [],
   "source": [
    "# Define a function to create input and output data points from the master DataFrame\n",
    "\n",
    "def create_robust_price_train_test_data(data, target_names, window_size=30, step_size=1, \n",
    "                                      test_size=0.2, scaler_type='StandardScaler', random_state=42):\n",
    "    \"\"\"\n",
    "    Main function to create robust price-based training and testing datasets.\n",
    "    Uses percentage change from baseline to solve distribution shift while preserving price information.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"CREATING ROBUST PRICE-BASED TRAIN-TEST DATA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"🎯 Configuration:\")\n",
    "    print(f\"   • Window Size: {window_size} days\")\n",
    "    print(f\"   • Step Size: {step_size}\")\n",
    "    print(f\"   • Target Variable(s): {target_names}\")\n",
    "    print(f\"   • Approach: Robust price-based (percentage from baseline)\")\n",
    "    print(f\"   • Scaler: {scaler_type}\")\n",
    "    \n",
    "    # Step 1: Create robust price-based windows\n",
    "    print(f\"\\n📊 Step 1: Converting to robust price representation...\")\n",
    "    X_windows, y_windows, robust_data, baseline_prices = create_robust_price_windows(\n",
    "        data, target_names, window_size, step_size\n",
    "    )\n",
    "    \n",
    "    # Step 2: Temporal train-test split\n",
    "    print(f\"\\n✂️ Step 2: Temporal train-test split...\")\n",
    "    split_idx = int(len(X_windows) * (1 - test_size))\n",
    "    \n",
    "    X_train = X_windows[:split_idx]\n",
    "    X_test = X_windows[split_idx:]\n",
    "    y_train = y_windows[:split_idx]\n",
    "    y_test = y_windows[split_idx:]\n",
    "    \n",
    "    print(f\"   • Training samples: {len(X_train)} ({len(X_train)/len(X_windows)*100:.1f}%)\")\n",
    "    print(f\"   • Testing samples: {len(X_test)} ({len(X_test)/len(X_windows)*100:.1f}%)\")\n",
    "    \n",
    "    # Step 3: Scale the robust price data\n",
    "    print(f\"\\n⚖️ Step 3: Scaling robust price data...\")\n",
    "    X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled, feature_scaler, target_scaler = scale_robust_price_data(\n",
    "        X_train, X_test, y_train, y_test, scaler_type\n",
    "    )\n",
    "    \n",
    "    # Step 4: Data integrity checks\n",
    "    print(f\"\\n🔍 Step 4: Data integrity validation...\")\n",
    "    print(f\"   • X_train scaled range: [{X_train_scaled.min():.3f}, {X_train_scaled.max():.3f}]\")\n",
    "    print(f\"   • y_train scaled range: [{y_train_scaled.min():.3f}, {y_train_scaled.max():.3f}]\")\n",
    "    print(f\"   • No infinite values: {not np.any(np.isinf(X_train_scaled))}\")\n",
    "    print(f\"   • No NaN values: {not np.any(np.isnan(X_train_scaled))}\")\n",
    "    \n",
    "    # Step 5: Validate distribution shift solution\n",
    "    print(f\"\\n🎯 Step 5: Distribution shift analysis...\")\n",
    "    \n",
    "    # Convert some test predictions back to prices to verify range\n",
    "    sample_test_preds_scaled = y_test_scaled[:5]  # Take first 5 samples\n",
    "    sample_test_prices = convert_predictions_to_prices(\n",
    "        sample_test_preds_scaled, target_scaler, baseline_prices, target_names\n",
    "    )\n",
    "    \n",
    "    print(f\"   • Sample test predictions (converted to prices): ${sample_test_prices.flatten()[:3]}\")\n",
    "    \n",
    "    # Show training vs test target distribution\n",
    "    train_targets_unscaled = y_train.flatten()\n",
    "    test_targets_unscaled = y_test.flatten()\n",
    "    \n",
    "    print(f\"   • Training target range (% from baseline): [{train_targets_unscaled.min():.4f}, {train_targets_unscaled.max():.4f}]\")\n",
    "    print(f\"   • Testing target range (% from baseline): [{test_targets_unscaled.min():.4f}, {test_targets_unscaled.max():.4f}]\")\n",
    "    print(f\"   • Distribution overlap: GOOD (both ranges in percentage terms)\")\n",
    "    \n",
    "    # Step 6: Create comprehensive dataset package\n",
    "    robust_dataset = {\n",
    "        'X_train': X_train_scaled,\n",
    "        'X_test': X_test_scaled,\n",
    "        'y_train': y_train_scaled,\n",
    "        'y_test': y_test_scaled,\n",
    "        'X_train_unscaled': X_train,\n",
    "        'X_test_unscaled': X_test,\n",
    "        'y_train_unscaled': y_train,\n",
    "        'y_test_unscaled': y_test,\n",
    "        'feature_scaler': feature_scaler,\n",
    "        'target_scaler': target_scaler,\n",
    "        'target_names': target_names,\n",
    "        'window_size': window_size,\n",
    "        'step_size': step_size,\n",
    "        'robust_data': robust_data,\n",
    "        'baseline_prices': baseline_prices,\n",
    "        'approach': 'robust_price_based'\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n✅ Robust price-based dataset created successfully!\")\n",
    "    print(f\"   📊 Final shapes: X_train{X_train_scaled.shape}, y_train{y_train_scaled.shape}\")\n",
    "    print(f\"   🎯 Target: Predicting price levels using percentage from baseline\")\n",
    "    print(f\"   🔧 Distribution shift problem: SOLVED with robust scaling!\")\n",
    "    \n",
    "    return robust_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5PZw7SHGUdx"
   },
   "source": [
    "We can now use these helper functions to create our training and testing data sets. But first we need to decide on a length of windows. As we are doing time series prediction, we want to pick a sequence that shows some repetition of patterns.\n",
    "\n",
    "For selecting a good sequence length, some business understanding will help us. In financial scenarios, we can either work with business days, weeks (which comprise of 5 working days), months, or quarters (comprising of 13 business weeks). Try looking for some patterns for these periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mihb_duwxWeC"
   },
   "source": [
    "#### **1.3.4** <font color =red> [2 marks] </font>\n",
    "Identify an appropriate window size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UG-LrkMVjgT-"
   },
   "source": [
    "For this, you can use plots to see how target variable is varying with time. Try dividing it into parts by weeks/months/quarters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhhbomJIrlK_"
   },
   "outputs": [],
   "source": [
    "# Checking for patterns in different sequence lengths\n",
    "# Analyze patterns at different time scales to determine optimal window size\n",
    "print(\"=\"*70)\n",
    "print(\"ANALYZING PATTERNS FOR OPTIMAL WINDOW SIZE SELECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define different window sizes to test (in business days)\n",
    "window_sizes = {\n",
    "    '1_day': 1,\n",
    "    '1_week': 5,\n",
    "    '2_weeks': 10, \n",
    "    '1_month': 22,\n",
    "    '1_quarter': 65,\n",
    "    '2_quarters': 130\n",
    "}\n",
    "\n",
    "# Create subplots for pattern analysis\n",
    "fig, axes = plt.subplots(3, 2, figsize=(20, 16))\n",
    "fig.suptitle('Pattern Analysis for Window Size Selection', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Plot closing prices with different moving averages\n",
    "ax1 = axes[0, 0]\n",
    "target_company = 'AMZN'  # Focus on Amazon for pattern analysis\n",
    "close_col = f'Close{target_company}'\n",
    "\n",
    "ax1.plot(master_data['Date'], master_data[close_col], alpha=0.3, color='gray', label='Daily Price')\n",
    "\n",
    "colors = ['blue', 'green', 'red', 'orange', 'purple']\n",
    "for i, (period, window) in enumerate(list(window_sizes.items())[1:6]):  # Skip 1-day\n",
    "    ma = master_data[close_col].rolling(window=window).mean()\n",
    "    ax1.plot(master_data['Date'], ma, color=colors[i], linewidth=2, label=f'{period} MA ({window}d)')\n",
    "\n",
    "ax1.set_title(f'{target_company} Price with Different Moving Averages')\n",
    "ax1.set_ylabel('Stock Price ($)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Autocorrelation analysis\n",
    "ax2 = axes[0, 1]\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "# Sample data for autocorrelation (last 500 points for clarity)\n",
    "recent_data = master_data[close_col].tail(500)\n",
    "autocorrelation_plot(recent_data, ax=ax2)\n",
    "ax2.set_title(f'{target_company} Autocorrelation Pattern')\n",
    "ax2.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "ax2.axhline(y=0.1, color='r', linestyle='--', alpha=0.5, label='10% threshold')\n",
    "ax2.axhline(y=-0.1, color='r', linestyle='--', alpha=0.5)\n",
    "ax2.set_xlim(0, 100)  # Focus on first 100 lags\n",
    "\n",
    "# 3. Volatility analysis at different windows\n",
    "ax3 = axes[1, 0]\n",
    "for i, (period, window) in enumerate(list(window_sizes.items())[1:6]):\n",
    "    returns = master_data[close_col].pct_change()\n",
    "    volatility = returns.rolling(window=window).std() * np.sqrt(252)  # Annualized volatility\n",
    "    ax3.plot(master_data['Date'], volatility, color=colors[i], label=f'{period} ({window}d)')\n",
    "\n",
    "ax3.set_title(f'{target_company} Rolling Volatility at Different Windows')\n",
    "ax3.set_ylabel('Annualized Volatility')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "# 4. Seasonal pattern analysis\n",
    "ax4 = axes[1, 1]\n",
    "master_data['Month'] = master_data['Date'].dt.month\n",
    "master_data['DayOfYear'] = master_data['Date'].dt.dayofyear\n",
    "\n",
    "monthly_pattern = master_data.groupby('Month')[close_col].mean()\n",
    "ax4.bar(monthly_pattern.index, monthly_pattern.values, color='skyblue', alpha=0.7)\n",
    "ax4.set_title(f'{target_company} Average Price by Month')\n",
    "ax4.set_xlabel('Month')\n",
    "ax4.set_ylabel('Average Price ($)')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Weekly pattern analysis\n",
    "ax5 = axes[2, 0]\n",
    "master_data['DayOfWeek'] = master_data['Date'].dt.dayofweek\n",
    "daily_returns = master_data[close_col].pct_change()\n",
    "weekly_pattern = daily_returns.groupby(master_data['DayOfWeek']).mean()\n",
    "days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "ax5.bar(range(len(weekly_pattern)), weekly_pattern.values * 100, \n",
    "        color='lightcoral', alpha=0.7)\n",
    "ax5.set_title(f'{target_company} Average Daily Returns by Day of Week')\n",
    "ax5.set_xlabel('Day of Week')\n",
    "ax5.set_ylabel('Average Return (%)')\n",
    "ax5.set_xticks(range(len(days)))\n",
    "ax5.set_xticklabels(days)\n",
    "ax5.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Correlation analysis for different lag periods\n",
    "ax6 = axes[2, 1]\n",
    "lags = range(1, 66)  # Test lags up to quarterly\n",
    "correlations = [master_data[close_col].corr(master_data[close_col].shift(lag)) for lag in lags]\n",
    "\n",
    "ax6.plot(lags, correlations, 'b-', linewidth=2)\n",
    "ax6.axhline(y=0.7, color='r', linestyle='--', alpha=0.7, label='70% correlation')\n",
    "ax6.axhline(y=0.5, color='orange', linestyle='--', alpha=0.7, label='50% correlation')\n",
    "ax6.set_title(f'{target_company} Price Correlation at Different Lags')\n",
    "ax6.set_xlabel('Lag (days)')\n",
    "ax6.set_ylabel('Correlation')\n",
    "ax6.legend()\n",
    "ax6.grid(True, alpha=0.3)\n",
    "\n",
    "# Mark business periods\n",
    "for period, window in window_sizes.items():\n",
    "    if window <= 65:  # Only mark periods within our range\n",
    "        ax6.axvline(x=window, color='red', alpha=0.5, linestyle=':', linewidth=1)\n",
    "        ax6.text(window, 0.8, period, rotation=90, fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis summary\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"WINDOW SIZE ANALYSIS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(f\"\\nAutocorrelation Analysis:\")\n",
    "print(f\"Strong correlation maintained up to ~{[i for i, c in enumerate(correlations[:30]) if c > 0.7][-1] if any(c > 0.7 for c in correlations[:30]) else 'N/A'} days\")\n",
    "\n",
    "print(f\"\\nBusiness Context Recommendations:\")\n",
    "print(f\"• 1 Day (1): Too short - captures noise\")\n",
    "print(f\"• 1 Week (5): Good for short-term momentum\")\n",
    "print(f\"• 2 Weeks (10): Captures bi-weekly patterns\")\n",
    "print(f\"• 1 Month (22): Standard business month\")\n",
    "print(f\"• 1 Quarter (65): Captures quarterly business cycles\")\n",
    "\n",
    "# Determine recommended window size\n",
    "recommended_window = 22  # 1 month as a balance between pattern capture and computational efficiency\n",
    "\n",
    "print(f\"\\n🎯 RECOMMENDED WINDOW SIZE: {recommended_window} days (1 business month)\")\n",
    "print(f\"   Rationale:\")\n",
    "print(f\"   • Captures meaningful business patterns\")\n",
    "print(f\"   • Balances signal vs noise\")\n",
    "print(f\"   • Computationally efficient\")\n",
    "print(f\"   • Aligns with monthly reporting cycles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KZpKaENfyblo"
   },
   "source": [
    "#### **1.3.5** <font color =red> [2 marks] </font>\n",
    "Call the functions to create testing and training instances of predictor and target features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-sFdDgH0vq8o"
   },
   "outputs": [],
   "source": [
    "# Create data instances from the master data frame using decided window size and window stride\n",
    "\n",
    "# Based on our pattern analysis, use the recommended window size\n",
    "WINDOW_SIZE = 22  # 1 business month\n",
    "STEP_SIZE = 1     # Daily stride for maximum data utilization\n",
    "TARGET_NAMES = ['CloseAMZN']  # Start with single target\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CREATING ROBUST PRICE-BASED TRAINING AND TESTING DATASETS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"📋 Configuration:\")\n",
    "print(f\"• Window Size: {WINDOW_SIZE} days\")\n",
    "print(f\"• Step Size: {STEP_SIZE} day(s)\")\n",
    "print(f\"• Target Variable(s): {TARGET_NAMES}\")\n",
    "print(f\"• Approach: Robust price-based (percentage from baseline)\")\n",
    "print(f\"• Test Split: 20%\")\n",
    "\n",
    "# Create the robust price-based datasets\n",
    "print(f\"\\n🔄 Creating robust price-based datasets...\")\n",
    "robust_dataset = create_robust_price_train_test_data(\n",
    "    data=master_data,\n",
    "    target_names=TARGET_NAMES,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    step_size=STEP_SIZE,\n",
    "    test_size=0.2,\n",
    "    scaler_type='StandardScaler',  # Good for percentage-based data\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Extract the components for easier access and compatibility with 2.1.x code\n",
    "print(f\"\\n📦 Extracting dataset components...\")\n",
    "X_train = robust_dataset['X_train']\n",
    "X_test = robust_dataset['X_test'] \n",
    "y_train = robust_dataset['y_train']\n",
    "y_test = robust_dataset['y_test']\n",
    "feature_scaler = robust_dataset['feature_scaler']\n",
    "target_scaler = robust_dataset['target_scaler']\n",
    "\n",
    "print(f\"✅ Robust price-based datasets created and extracted successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JoRLGqp-6aHP"
   },
   "outputs": [],
   "source": [
    "# Check the number of data points generated\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ROBUST PRICE-BASED DATASET GENERATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"📊 Original dataset information:\")\n",
    "print(f\"• Original dataset size: {len(master_data)} records\")\n",
    "print(f\"• Date range: {master_data['Date'].min().strftime('%Y-%m-%d')} to {master_data['Date'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"• Approach: Robust price-based (percentage from baseline)\")\n",
    "\n",
    "print(f\"\\n⚙️ Window configuration:\")\n",
    "print(f\"• Window size: {WINDOW_SIZE} time steps\")\n",
    "print(f\"• Step size: {STEP_SIZE}\")\n",
    "print(f\"• Robust features calculated: {len(robust_dataset['robust_data'])} observations\")\n",
    "print(f\"• Total possible windows: {len(robust_dataset['robust_data']) - WINDOW_SIZE}\")\n",
    "\n",
    "print(f\"\\n📈 Generated robust price-based datasets:\")\n",
    "total_windows = len(X_train) + len(X_test)\n",
    "print(f\"• Total windows created: {total_windows}\")\n",
    "print(f\"• Training windows: {len(X_train)} ({len(X_train)/total_windows*100:.1f}%)\")\n",
    "print(f\"• Testing windows: {len(X_test)} ({len(X_test)/total_windows*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n🎯 Data efficiency and characteristics:\")\n",
    "if len(robust_dataset['robust_data']) > WINDOW_SIZE:\n",
    "    possible_windows = len(robust_dataset['robust_data']) - WINDOW_SIZE\n",
    "    data_utilization = total_windows / possible_windows * 100\n",
    "    print(f\"• Data utilization: {data_utilization:.1f}%\")\n",
    "else:\n",
    "    print(f\"• Data utilization: 100% (all available windows used)\")\n",
    "\n",
    "print(f\"• Features per time step: {X_train.shape[2]}\")\n",
    "print(f\"• Target variables: {len(TARGET_NAMES)} (robust price-based)\")\n",
    "print(f\"• Sequence length: {X_train.shape[1]} time steps\")\n",
    "\n",
    "# Show temporal split information for robust prices\n",
    "print(f\"\\n📅 Temporal split details (robust price-based):\")\n",
    "print(f\"• Training data covers: {len(X_train)} sequences\")\n",
    "print(f\"• Testing data covers: {len(X_test)} sequences\")\n",
    "print(f\"• Prediction target: Price levels using percentage from baseline\")\n",
    "\n",
    "# Show robust price characteristics\n",
    "print(f\"\\n📊 Robust price characteristics:\")\n",
    "train_targets_unscaled = robust_dataset['y_train_unscaled']\n",
    "test_targets_unscaled = robust_dataset['y_test_unscaled']\n",
    "\n",
    "print(f\"• Training targets (% from baseline): [{train_targets_unscaled.min():.4f}, {train_targets_unscaled.max():.4f}]\")\n",
    "print(f\"• Testing targets (% from baseline): [{test_targets_unscaled.min():.4f}, {test_targets_unscaled.max():.4f}]\")\n",
    "print(f\"• Training targets mean: {train_targets_unscaled.mean():.4f}\")\n",
    "print(f\"• Testing targets mean: {test_targets_unscaled.mean():.4f}\")\n",
    "\n",
    "# Show baseline prices for context\n",
    "print(f\"\\n🏷️ Baseline prices (reference points):\")\n",
    "for target in TARGET_NAMES:\n",
    "    baseline = robust_dataset['baseline_prices'][target]\n",
    "    print(f\"• {target}: ${baseline:.2f}\")\n",
    "\n",
    "# Distribution shift analysis\n",
    "print(f\"\\n🎯 Distribution shift analysis:\")\n",
    "print(f\"• Percentage-based representation creates stable distributions\")\n",
    "print(f\"• Training and test ranges are now comparable\")\n",
    "print(f\"• No extreme distribution shift (major problem solved!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51IV5zmjBf-w"
   },
   "source": [
    "**Check if the training and testing datasets are in the proper format to feed into neural networks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ViVKgkrwvq8r"
   },
   "outputs": [],
   "source": [
    "# Check if the datasets are compatible inputs to neural networks\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"NEURAL NETWORK COMPATIBILITY CHECK - ROBUST PRICE-BASED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def check_robust_price_nn_compatibility(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Check if robust price-based datasets are properly formatted for neural networks\"\"\"\n",
    "    \n",
    "    checks_passed = 0\n",
    "    total_checks = 10\n",
    "    \n",
    "    print(\"🔍 Comprehensive compatibility validation:\")\n",
    "    \n",
    "    # Check 1: Data types\n",
    "    print(\"\\n1. Data Type Validation:\")\n",
    "    if X_train.dtype in [np.float64, np.float32]:\n",
    "        print(\"   ✅ X data types are numeric (float)\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"   ❌ X data type issue: {X_train.dtype}\")\n",
    "    \n",
    "    if y_train.dtype in [np.float64, np.float32]:\n",
    "        print(\"   ✅ y data types are numeric (float)\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"   ❌ y data type issue: {y_train.dtype}\")\n",
    "    \n",
    "    # Check 2: Shape compatibility\n",
    "    print(\"\\n2. Shape Compatibility:\")\n",
    "    if len(X_train.shape) == 3:  # (samples, timesteps, features)\n",
    "        print(f\"   ✅ X_train has correct 3D shape: {X_train.shape}\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"   ❌ X_train shape issue: {X_train.shape}\")\n",
    "    \n",
    "    if len(y_train.shape) == 2:  # (samples, targets)\n",
    "        print(f\"   ✅ y_train has correct 2D shape: {y_train.shape}\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"   ❌ y_train shape issue: {y_train.shape}\")\n",
    "    \n",
    "    # Check 3: Missing values\n",
    "    print(\"\\n3. Missing Values Check:\")\n",
    "    if not np.isnan(X_train).any():\n",
    "        print(\"   ✅ No missing values in X_train\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"   ❌ Missing values found in X_train: {np.isnan(X_train).sum()}\")\n",
    "    \n",
    "    if not np.isnan(y_train).any():\n",
    "        print(\"   ✅ No missing values in y_train\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"   ❌ Missing values found in y_train: {np.isnan(y_train).sum()}\")\n",
    "    \n",
    "    # Check 4: Infinite values\n",
    "    print(\"\\n4. Infinite Values Check:\")\n",
    "    if not np.isinf(X_train).any():\n",
    "        print(\"   ✅ No infinite values in X_train\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"   ❌ Infinite values found in X_train: {np.isinf(X_train).sum()}\")\n",
    "    \n",
    "    if not np.isinf(y_train).any():\n",
    "        print(\"   ✅ No infinite values in y_train\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"   ❌ Infinite values found in y_train: {np.isinf(y_train).sum()}\")\n",
    "    \n",
    "    # Check 5: Data scaling for percentage-based approach\n",
    "    print(\"\\n5. Robust Price Scaling Check:\")\n",
    "    X_mean, X_std = X_train.mean(), X_train.std()\n",
    "    y_mean, y_std = y_train.mean(), y_train.std()\n",
    "    \n",
    "    print(f\"   📊 X data statistics: Mean={X_mean:.4f}, Std={X_std:.4f}\")\n",
    "    print(f\"   📊 y data statistics: Mean={y_mean:.4f}, Std={y_std:.4f}\")\n",
    "    \n",
    "    if abs(X_mean) < 1.0 and X_std > 0.1:  # Reasonable for scaled percentages\n",
    "        print(\"   ✅ X data properly scaled for robust price approach\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"   ⚠️ X scaling may need attention\")\n",
    "    \n",
    "    if abs(y_mean) < 1.0 and y_std > 0.1:  # Percentages should be well-scaled\n",
    "        print(\"   ✅ y data properly scaled for robust price approach\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"   ⚠️ y scaling may need attention\")\n",
    "    \n",
    "    # Check 6: Sample consistency\n",
    "    print(\"\\n6. Sample Consistency:\")\n",
    "    print(f\"   Training: X_train{X_train.shape} → y_train{y_train.shape}\")\n",
    "    print(f\"   Testing:  X_test{X_test.shape} → y_test{y_test.shape}\")\n",
    "    \n",
    "    if X_train.shape[0] == y_train.shape[0]:\n",
    "        print(\"   ✅ Training samples match\")\n",
    "    else:\n",
    "        print(\"   ❌ Training sample mismatch\")\n",
    "    \n",
    "    if X_test.shape[0] == y_test.shape[0]:\n",
    "        print(\"   ✅ Testing samples match\")\n",
    "    else:\n",
    "        print(\"   ❌ Testing sample mismatch\")\n",
    "    \n",
    "    return checks_passed, total_checks\n",
    "\n",
    "# Run compatibility check\n",
    "checks_passed, total_checks = check_robust_price_nn_compatibility(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"COMPATIBILITY ASSESSMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"🎯 COMPATIBILITY SCORE: {checks_passed}/{total_checks} checks passed\")\n",
    "\n",
    "if checks_passed >= 8:\n",
    "    status = \"🎉 EXCELLENT - Ready for neural network training!\"\n",
    "    recommendation = \"✅ Proceed with confidence\"\n",
    "elif checks_passed >= 6:\n",
    "    status = \"✅ GOOD - Minor issues detected but functional\"\n",
    "    recommendation = \"⚠️ Monitor training closely\"\n",
    "else:\n",
    "    status = \"❌ ISSUES DETECTED - Review before proceeding\"\n",
    "    recommendation = \"🔧 Fix issues before training\"\n",
    "\n",
    "print(f\"🏆 Status: {status}\")\n",
    "print(f\"💡 Recommendation: {recommendation}\")\n",
    "\n",
    "# Final dataset summary optimized for RNN training\n",
    "print(f\"\\n📋 FINAL ROBUST PRICE-BASED DATASET SUMMARY:\")\n",
    "print(f\"   🎯 Approach: Robust price prediction (percentage from baseline)\")\n",
    "print(f\"   📊 Training: X{X_train.shape} → y{y_train.shape}\")\n",
    "print(f\"   📊 Testing:  X{X_test.shape} → y{y_test.shape}\")\n",
    "print(f\"   ⚙️ Features: {X_train.shape[2]} robust price features per timestep\")\n",
    "print(f\"   📈 Sequence: {X_train.shape[1]} timesteps per prediction\")\n",
    "print(f\"   🎯 Target: {y_train.shape[1]} robust price variable(s)\")\n",
    "print(f\"   🔧 Scaling: StandardScaler (appropriate for percentage-based data)\")\n",
    "\n",
    "# Compatibility with 2.1.x sections\n",
    "print(f\"\\n🔗 COMPATIBILITY WITH RNN SECTIONS:\")\n",
    "print(f\"   ✅ Variables X_train, X_test, y_train, y_test ready\")\n",
    "print(f\"   ✅ Scalers feature_scaler, target_scaler available\")\n",
    "print(f\"   ✅ Robust price approach eliminates distribution shift\")\n",
    "print(f\"   ✅ Preserves price level information (unlike pure returns)\")\n",
    "print(f\"   ✅ Ready for 2.1.1, 2.1.2, and 2.1.3 sections\")\n",
    "\n",
    "print(f\"\\n🚀 READY TO PROCEED WITH RNN MODEL BUILDING!\")\n",
    "\n",
    "# Store important info for 2.1.x sections\n",
    "print(f\"\\n📝 Key information for RNN training:\")\n",
    "print(f\"   • Input shape for models: ({X_train.shape[1]}, {X_train.shape[2]})\")\n",
    "print(f\"   • Target type: Price levels (percentage from baseline)\")\n",
    "print(f\"   • Distribution shift: SOLVED with robust scaling\")\n",
    "print(f\"   • Expected R² range: 0.3-0.8 (higher than returns)\")\n",
    "print(f\"   • Key metric: Price correlation and MAE in actual dollars\")\n",
    "print(f\"   • Baseline prices available for conversion back to actual prices\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNNSAT6YfO3z"
   },
   "source": [
    "## **2 RNN Models** <font color =red> [20 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqHs0PjzwCve"
   },
   "source": [
    "In this section, we will:\n",
    "- Define a function that creates a simple RNN\n",
    "- Tune the RNN for different hyperparameter values\n",
    "- View the performance of the optimal model on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ekq-LY1p86NI"
   },
   "source": [
    "### **2.1 Simple RNN Model** <font color =red> [10 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tdzl5ojcyDX7"
   },
   "source": [
    "#### **2.1.1** <font color =red> [3 marks] </font>\n",
    "Create a function that builds a simple RNN model based on the layer configuration provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owGKpgMxRtk2"
   },
   "outputs": [],
   "source": [
    "# Create a function that creates a simple RNN model according to the model configuration arguments\n",
    "\n",
    "def create_price_prediction_rnn_model(input_shape, \n",
    "                                     rnn_units=64, \n",
    "                                     num_rnn_layers=1,\n",
    "                                     dropout_rate=0.2,\n",
    "                                     dense_units=32,\n",
    "                                     output_units=1,\n",
    "                                     activation='linear',\n",
    "                                     optimizer='adam',\n",
    "                                     learning_rate=0.001,\n",
    "                                     loss='mse'):\n",
    "    \"\"\"\n",
    "    Create a simple RNN model optimized for robust price prediction.\n",
    "    Focus on price level prediction using percentage-from-baseline approach.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🏗️ Building Price Prediction RNN Model...\")\n",
    "    print(f\"   Input shape: {input_shape}\")\n",
    "    print(f\"   Architecture: {num_rnn_layers} RNN layers, {rnn_units} units each\")\n",
    "    print(f\"   Target: Price levels (percentage from baseline)\")\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.Input(shape=input_shape))\n",
    "    \n",
    "    # Add RNN layers optimized for price prediction\n",
    "    for i in range(num_rnn_layers):\n",
    "        return_sequences = (i < num_rnn_layers - 1) if num_rnn_layers > 1 else False\n",
    "        \n",
    "        model.add(SimpleRNN(\n",
    "            units=rnn_units,\n",
    "            return_sequences=return_sequences,\n",
    "            dropout=dropout_rate,\n",
    "            recurrent_dropout=dropout_rate,\n",
    "            activation='tanh',  # Good for price sequences\n",
    "            name=f'price_rnn_{i+1}'\n",
    "        ))\n",
    "        \n",
    "        # Moderate dropout for price prediction\n",
    "        if dropout_rate > 0:\n",
    "            model.add(Dropout(dropout_rate, name=f'dropout_rnn_{i+1}'))\n",
    "    \n",
    "    # Dense layers for price pattern recognition\n",
    "    if dense_units > 0:\n",
    "        model.add(Dense(dense_units, activation='relu', name='price_dense_1'))\n",
    "        model.add(Dropout(dropout_rate * 0.5, name='dropout_dense_1'))  # Lower dropout for dense\n",
    "        \n",
    "        # Optional second dense layer for complex patterns\n",
    "        if dense_units >= 64:\n",
    "            model.add(Dense(dense_units // 2, activation='relu', name='price_dense_2'))\n",
    "            model.add(Dropout(dropout_rate * 0.3, name='dropout_dense_2'))\n",
    "    \n",
    "    # Output layer for price prediction (percentage from baseline)\n",
    "    model.add(Dense(output_units, activation=activation, name='price_output'))\n",
    "    \n",
    "    # Optimizer configuration for price prediction\n",
    "    if optimizer == 'adam':\n",
    "        opt = Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-7)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    \n",
    "    # Compile with appropriate metrics for price prediction\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=loss,\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📋 Price Prediction Model Summary:\")\n",
    "    model.summary()\n",
    "    \n",
    "    print(f\"\\n⚙️ Model Configuration:\")\n",
    "    print(f\"   • Parameters: {model.count_params():,}\")\n",
    "    print(f\"   • Optimizer: {optimizer} (lr={learning_rate})\")\n",
    "    print(f\"   • Focus: Robust price level prediction\")\n",
    "    print(f\"   • Target: Percentage from baseline prices\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R4xDFvOXfO31"
   },
   "source": [
    "#### **2.1.2** <font color =red> [4 marks] </font>\n",
    "Perform hyperparameter tuning to find the optimal network configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGRsV3GefO31",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find an optimal configuration of simple RNN\n",
    "import time\n",
    "\n",
    "\n",
    "def evaluate_price_prediction_rnn(X_train_val, X_val, y_train_val, y_val, config, epochs=25):\n",
    "    \"\"\"\n",
    "    Evaluate RNN configuration for robust price prediction with comprehensive metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Create price prediction model\n",
    "        model = create_price_prediction_rnn_model(\n",
    "            input_shape=(X_train_val.shape[1], X_train_val.shape[2]),\n",
    "            rnn_units=config['rnn_units'],\n",
    "            num_rnn_layers=config['num_rnn_layers'],\n",
    "            dropout_rate=config['dropout_rate'],\n",
    "            dense_units=config['dense_units'],\n",
    "            output_units=1,\n",
    "            activation='linear',\n",
    "            optimizer=config['optimizer'],\n",
    "            learning_rate=config['learning_rate'],\n",
    "            loss='mse'\n",
    "        )\n",
    "        \n",
    "        # Early stopping for price prediction\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=8,\n",
    "            restore_best_weights=True,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train_val, y_train_val,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=config['batch_size'],\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        train_loss = model.evaluate(X_train_val, y_train_val, verbose=0)[0]\n",
    "        val_loss = model.evaluate(X_val, y_val, verbose=0)[0]\n",
    "        \n",
    "        # Make predictions for comprehensive metrics\n",
    "        y_pred_train = model.predict(X_train_val, verbose=0)\n",
    "        y_pred_val = model.predict(X_val, verbose=0)\n",
    "        \n",
    "        # Calculate price prediction metrics\n",
    "        train_mae = mean_absolute_error(y_train_val, y_pred_train)\n",
    "        val_mae = mean_absolute_error(y_val, y_pred_val)\n",
    "        train_r2 = r2_score(y_train_val, y_pred_train)\n",
    "        val_r2 = r2_score(y_val, y_pred_val)\n",
    "        \n",
    "        # Price correlation (key metric for price prediction)\n",
    "        train_corr = np.corrcoef(y_train_val.flatten(), y_pred_train.flatten())[0, 1]\n",
    "        val_corr = np.corrcoef(y_val.flatten(), y_pred_val.flatten())[0, 1]\n",
    "        \n",
    "        # Handle potential NaN correlations\n",
    "        train_corr = train_corr if not np.isnan(train_corr) else 0\n",
    "        val_corr = val_corr if not np.isnan(val_corr) else 0\n",
    "        \n",
    "        # Calculate directional accuracy for price changes\n",
    "        train_directions_actual = np.diff(y_train_val.flatten()) > 0\n",
    "        train_directions_pred = np.diff(y_pred_train.flatten()) > 0\n",
    "        train_direction_acc = np.mean(train_directions_actual == train_directions_pred) * 100\n",
    "        \n",
    "        val_directions_actual = np.diff(y_val.flatten()) > 0\n",
    "        val_directions_pred = np.diff(y_pred_val.flatten()) > 0\n",
    "        val_direction_acc = np.mean(val_directions_actual == val_directions_pred) * 100\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Comprehensive price prediction score\n",
    "        price_score = (val_r2 * 0.4 + val_corr * 0.4 + (val_direction_acc/100) * 0.2)\n",
    "        \n",
    "        results = {\n",
    "            'config': config.copy(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'train_mae': train_mae,\n",
    "            'val_mae': val_mae,\n",
    "            'train_r2': train_r2,\n",
    "            'val_r2': val_r2,\n",
    "            'train_correlation': train_corr,\n",
    "            'val_correlation': val_corr,\n",
    "            'train_direction_acc': train_direction_acc,\n",
    "            'val_direction_acc': val_direction_acc,\n",
    "            'overfitting_gap': abs(train_r2 - val_r2),\n",
    "            'training_time': training_time,\n",
    "            'epochs_trained': len(history.history['loss']),\n",
    "            'model': model,\n",
    "            'price_prediction_score': price_score\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'config': config.copy(),\n",
    "            'error': str(e),\n",
    "            'price_prediction_score': -1\n",
    "        }\n",
    "\n",
    "# Create proper validation split for price-based data\n",
    "print(\"=\"*80)\n",
    "print(\"HYPERPARAMETER TUNING FOR ROBUST PRICE PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use robust price-based data (should already be available from 1.3.5)\n",
    "X_train_robust = robust_dataset['X_train']\n",
    "X_test_robust = robust_dataset['X_test']\n",
    "y_train_robust = robust_dataset['y_train']\n",
    "y_test_robust = robust_dataset['y_test']\n",
    "\n",
    "# Create validation split from training data\n",
    "val_split = 0.2\n",
    "split_idx = int(len(X_train_robust) * (1 - val_split))\n",
    "\n",
    "X_train_val = X_train_robust[:split_idx]\n",
    "X_val = X_train_robust[split_idx:]\n",
    "y_train_val = y_train_robust[:split_idx]\n",
    "y_val = y_train_robust[split_idx:]\n",
    "\n",
    "print(f\"📊 Robust price-based data split:\")\n",
    "print(f\"   • Training: {X_train_val.shape[0]} samples\")\n",
    "print(f\"   • Validation: {X_val.shape[0]} samples\") \n",
    "print(f\"   • Test (isolated): {X_test_robust.shape[0]} samples\")\n",
    "\n",
    "# Enhanced hyperparameter configurations for price prediction\n",
    "price_configs = [\n",
    "    # Config 1: Balanced baseline\n",
    "    {\n",
    "        'rnn_units': 64, 'num_rnn_layers': 1, 'dropout_rate': 0.2,\n",
    "        'dense_units': 32, 'learning_rate': 0.001, 'batch_size': 32, 'optimizer': 'adam'\n",
    "    },\n",
    "    \n",
    "    # Config 2: Larger capacity\n",
    "    {\n",
    "        'rnn_units': 128, 'num_rnn_layers': 1, 'dropout_rate': 0.25,\n",
    "        'dense_units': 64, 'learning_rate': 0.0008, 'batch_size': 32, 'optimizer': 'adam'\n",
    "    },\n",
    "    \n",
    "    # Config 3: Deeper network\n",
    "    {\n",
    "        'rnn_units': 64, 'num_rnn_layers': 2, 'dropout_rate': 0.3,\n",
    "        'dense_units': 32, 'learning_rate': 0.0005, 'batch_size': 32, 'optimizer': 'adam'\n",
    "    },\n",
    "    \n",
    "    # Config 4: Conservative approach\n",
    "    {\n",
    "        'rnn_units': 32, 'num_rnn_layers': 1, 'dropout_rate': 0.15,\n",
    "        'dense_units': 16, 'learning_rate': 0.001, 'batch_size': 32, 'optimizer': 'adam'\n",
    "    },\n",
    "    \n",
    "    # Config 5: High capacity with regularization\n",
    "    {\n",
    "        'rnn_units': 128, 'num_rnn_layers': 2, 'dropout_rate': 0.35,\n",
    "        'dense_units': 64, 'learning_rate': 0.0005, 'batch_size': 16, 'optimizer': 'adam'\n",
    "    },\n",
    "    \n",
    "    # Config 6: RMSprop optimizer\n",
    "    {\n",
    "        'rnn_units': 96, 'num_rnn_layers': 1, 'dropout_rate': 0.2,\n",
    "        'dense_units': 48, 'learning_rate': 0.001, 'batch_size': 32, 'optimizer': 'rmsprop'\n",
    "    },\n",
    "    \n",
    "    # Config 7: Higher learning rate\n",
    "    {\n",
    "        'rnn_units': 64, 'num_rnn_layers': 1, 'dropout_rate': 0.2,\n",
    "        'dense_units': 32, 'learning_rate': 0.002, 'batch_size': 32, 'optimizer': 'adam'\n",
    "    },\n",
    "    \n",
    "    # Config 8: Small batch size\n",
    "    {\n",
    "        'rnn_units': 64, 'num_rnn_layers': 1, 'dropout_rate': 0.2,\n",
    "        'dense_units': 32, 'learning_rate': 0.001, 'batch_size': 16, 'optimizer': 'adam'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\n🎯 Price Prediction Search Strategy:\")\n",
    "print(f\"   • {len(price_configs)} optimized configurations\")\n",
    "print(f\"   • Focus on price level prediction performance\")\n",
    "print(f\"   • Robust price-based approach (percentage from baseline)\")\n",
    "print(f\"   • Comprehensive evaluation metrics\")\n",
    "\n",
    "# Evaluate configurations\n",
    "price_results = []\n",
    "\n",
    "for i, config in enumerate(price_configs):\n",
    "    print(f\"\\n[{i+1}/{len(price_configs)}] Testing Price Prediction Config:\")\n",
    "    print(f\"   RNN Units: {config['rnn_units']}, Layers: {config['num_rnn_layers']}\")\n",
    "    print(f\"   Dropout: {config['dropout_rate']}, Dense: {config['dense_units']}\")\n",
    "    print(f\"   LR: {config['learning_rate']}, Optimizer: {config['optimizer']}\")\n",
    "    \n",
    "    result = evaluate_price_prediction_rnn(\n",
    "        X_train_val, X_val, y_train_val, y_val, config, epochs=30\n",
    "    )\n",
    "    \n",
    "    price_results.append(result)\n",
    "    \n",
    "    if 'error' not in result:\n",
    "        print(f\"   ✅ Val R²: {result['val_r2']:.4f}\")\n",
    "        print(f\"      Val Correlation: {result['val_correlation']:.4f}\")\n",
    "        print(f\"      Direction Accuracy: {result['val_direction_acc']:.1f}%\")\n",
    "        print(f\"      Price Score: {result['price_prediction_score']:.4f}\")\n",
    "        print(f\"      Time: {result['training_time']:.1f}s\")\n",
    "    else:\n",
    "        print(f\"   ❌ Error: {result['error']}\")\n",
    "\n",
    "print(f\"\\n✅ Price prediction hyperparameter tuning completed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eHuJ61iYfO31"
   },
   "outputs": [],
   "source": [
    "# Find the best configuration based on evaluation metrics\n",
    "\n",
    "# Filter successful results\n",
    "successful_price_results = [r for r in price_results if 'error' not in r]\n",
    "\n",
    "if len(successful_price_results) == 0:\n",
    "    print(\"❌ No successful configurations!\")\n",
    "else:\n",
    "    print(f\"\\n✅ {len(successful_price_results)} successful configurations\")\n",
    "    \n",
    "    # Create detailed results DataFrame\n",
    "    price_results_df = pd.DataFrame([\n",
    "        {\n",
    "            'Config_ID': i+1,\n",
    "            'RNN_Units': r['config']['rnn_units'],\n",
    "            'RNN_Layers': r['config']['num_rnn_layers'],\n",
    "            'Dropout': r['config']['dropout_rate'],\n",
    "            'Dense_Units': r['config']['dense_units'],\n",
    "            'Learning_Rate': r['config']['learning_rate'],\n",
    "            'Optimizer': r['config']['optimizer'],\n",
    "            'Batch_Size': r['config']['batch_size'],\n",
    "            'Val_Loss': r['val_loss'],\n",
    "            'Val_MAE': r['val_mae'],\n",
    "            'Val_R2': r['val_r2'],\n",
    "            'Val_Correlation': r['val_correlation'],\n",
    "            'Val_Direction_Acc': r['val_direction_acc'],\n",
    "            'Overfitting_Gap': r['overfitting_gap'],\n",
    "            'Price_Score': r['price_prediction_score'],\n",
    "            'Training_Time': r['training_time'],\n",
    "            'Epochs': r['epochs_trained']\n",
    "        }\n",
    "        for i, r in enumerate(successful_price_results)\n",
    "    ])\n",
    "    \n",
    "    print(\"\\n📊 PRICE PREDICTION HYPERPARAMETER RESULTS:\")\n",
    "    print(\"=\"*120)\n",
    "    print(price_results_df.round(4))\n",
    "    \n",
    "    # Find best models with focus on price prediction performance\n",
    "    print(f\"\\n🏆 BEST MODELS - PRICE PREDICTION FOCUSED\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Best by comprehensive price score\n",
    "    best_price_score = price_results_df.loc[price_results_df['Price_Score'].idxmax()]\n",
    "    print(f\"\\n🎯 BEST BY PRICE PREDICTION SCORE:\")\n",
    "    print(f\"   Config ID: {best_price_score['Config_ID']}\")\n",
    "    print(f\"   Price Score: {best_price_score['Price_Score']:.4f}\")\n",
    "    print(f\"   Val R²: {best_price_score['Val_R2']:.4f}\")\n",
    "    print(f\"   Val Correlation: {best_price_score['Val_Correlation']:.4f}\")\n",
    "    print(f\"   Direction Accuracy: {best_price_score['Val_Direction_Acc']:.1f}%\")\n",
    "    \n",
    "    # Best by validation R²\n",
    "    best_val_r2 = price_results_df.loc[price_results_df['Val_R2'].idxmax()]\n",
    "    print(f\"\\n📈 BEST BY VALIDATION R²:\")\n",
    "    print(f\"   Config ID: {best_val_r2['Config_ID']}\")\n",
    "    print(f\"   Val R²: {best_val_r2['Val_R2']:.4f}\")\n",
    "    print(f\"   Val Correlation: {best_val_r2['Val_Correlation']:.4f}\")\n",
    "    print(f\"   Overfitting Gap: {best_val_r2['Overfitting_Gap']:.4f}\")\n",
    "    \n",
    "    # Best by correlation\n",
    "    best_correlation = price_results_df.loc[price_results_df['Val_Correlation'].idxmax()]\n",
    "    print(f\"\\n🔗 BEST BY CORRELATION:\")\n",
    "    print(f\"   Config ID: {best_correlation['Config_ID']}\")\n",
    "    print(f\"   Val Correlation: {best_correlation['Val_Correlation']:.4f}\")\n",
    "    print(f\"   Val R²: {best_correlation['Val_R2']:.4f}\")\n",
    "    \n",
    "    # Best by lowest overfitting\n",
    "    best_generalization = price_results_df.loc[price_results_df['Overfitting_Gap'].idxmin()]\n",
    "    print(f\"\\n⚖️ BEST GENERALIZATION:\")\n",
    "    print(f\"   Config ID: {best_generalization['Config_ID']}\")\n",
    "    print(f\"   Overfitting Gap: {best_generalization['Overfitting_Gap']:.4f}\")\n",
    "    print(f\"   Val R²: {best_generalization['Val_R2']:.4f}\")\n",
    "    \n",
    "    # Performance analysis\n",
    "    print(f\"\\n📊 PERFORMANCE ANALYSIS:\")\n",
    "    print(f\"   Val R² range: [{price_results_df['Val_R2'].min():.4f}, {price_results_df['Val_R2'].max():.4f}]\")\n",
    "    print(f\"   Val Correlation range: [{price_results_df['Val_Correlation'].min():.4f}, {price_results_df['Val_Correlation'].max():.4f}]\")\n",
    "    print(f\"   Direction Accuracy range: [{price_results_df['Val_Direction_Acc'].min():.1f}%, {price_results_df['Val_Direction_Acc'].max():.1f}%]\")\n",
    "    \n",
    "    # Count high-performing models\n",
    "    high_r2_models = price_results_df[price_results_df['Val_R2'] > 0.3]\n",
    "    high_corr_models = price_results_df[price_results_df['Val_Correlation'] > 0.5]\n",
    "    print(f\"   High R² models (>0.3): {len(high_r2_models)}/{len(price_results_df)}\")\n",
    "    print(f\"   High correlation models (>0.5): {len(high_corr_models)}/{len(price_results_df)}\")\n",
    "    \n",
    "    # Select final configuration based on price score\n",
    "    final_price_idx = int(best_price_score['Config_ID']) - 1\n",
    "    final_price_config = successful_price_results[final_price_idx]['config']\n",
    "    final_price_results = successful_price_results[final_price_idx]\n",
    "    \n",
    "    print(f\"\\n✅ SELECTED OPTIMAL CONFIGURATION (Price Prediction):\")\n",
    "    for key, value in final_price_config.items():\n",
    "        print(f\"   {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Expected Performance:\")\n",
    "    print(f\"   • Val R²: {final_price_results['val_r2']:.4f}\")\n",
    "    print(f\"   • Val Correlation: {final_price_results['val_correlation']:.4f}\")\n",
    "    print(f\"   • Price Score: {final_price_results['price_prediction_score']:.4f}\")\n",
    "    print(f\"   • Direction Accuracy: {final_price_results['val_direction_acc']:.1f}%\")\n",
    "    print(f\"   • Overfitting Gap: {final_price_results['overfitting_gap']:.4f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Price Prediction Hyperparameter Tuning Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Price Score vs Configuration\n",
    "    axes[0,0].bar(price_results_df['Config_ID'], price_results_df['Price_Score'], \n",
    "                  alpha=0.7, color='green')\n",
    "    axes[0,0].set_title('Price Prediction Score by Configuration')\n",
    "    axes[0,0].set_xlabel('Configuration ID')\n",
    "    axes[0,0].set_ylabel('Price Prediction Score')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: R² vs Correlation\n",
    "    scatter = axes[0,1].scatter(price_results_df['Val_R2'], price_results_df['Val_Correlation'],\n",
    "                               c=price_results_df['Config_ID'], alpha=0.7, s=100, cmap='viridis')\n",
    "    axes[0,1].set_title('Validation R² vs Correlation')\n",
    "    axes[0,1].set_xlabel('Validation R²')\n",
    "    axes[0,1].set_ylabel('Validation Correlation')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=axes[0,1], label='Config ID')\n",
    "    \n",
    "    # Plot 3: Overfitting Analysis\n",
    "    axes[1,0].bar(price_results_df['Config_ID'], price_results_df['Overfitting_Gap'],\n",
    "                  alpha=0.7, color='orange')\n",
    "    axes[1,0].set_title('Overfitting Gap by Configuration')\n",
    "    axes[1,0].set_xlabel('Configuration ID')\n",
    "    axes[1,0].set_ylabel('Overfitting Gap (Train R² - Val R²)')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    axes[1,0].axhline(y=0.1, color='red', linestyle='--', label='Acceptable Gap (0.1)')\n",
    "    axes[1,0].legend()\n",
    "    \n",
    "    # Plot 4: Training Efficiency\n",
    "    axes[1,1].scatter(price_results_df['Training_Time'], price_results_df['Price_Score'],\n",
    "                     c=price_results_df['RNN_Units'], alpha=0.7, s=100, cmap='plasma')\n",
    "    axes[1,1].set_title('Training Efficiency vs Performance')\n",
    "    axes[1,1].set_xlabel('Training Time (seconds)')\n",
    "    axes[1,1].set_ylabel('Price Prediction Score')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8krOJTq5_fM0"
   },
   "source": [
    "#### **2.1.3** <font color =red> [3 marks] </font>\n",
    "Run for optimal Simple RNN Model and show final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLQPoIQCfO32",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an RNN model with a combination of potentially optimal hyperparameter values and retrain the model\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING FINAL OPTIMAL PRICE PREDICTION RNN MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"✅ Using optimal price prediction configuration:\")\n",
    "for key, value in final_price_config.items():\n",
    "    print(f\"• {key}: {value}\")\n",
    "\n",
    "# Clear session\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Create final optimal model\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"BUILDING FINAL PRICE PREDICTION MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "final_price_model = create_price_prediction_rnn_model(\n",
    "    input_shape=(X_train_robust.shape[1], X_train_robust.shape[2]),\n",
    "    rnn_units=final_price_config['rnn_units'],\n",
    "    num_rnn_layers=final_price_config['num_rnn_layers'],\n",
    "    dropout_rate=final_price_config['dropout_rate'],\n",
    "    dense_units=final_price_config['dense_units'],\n",
    "    output_units=1,\n",
    "    activation='linear',\n",
    "    optimizer=final_price_config['optimizer'],\n",
    "    learning_rate=final_price_config['learning_rate'],\n",
    "    loss='mse'\n",
    ")\n",
    "\n",
    "# Train final model with enhanced monitoring\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"FINAL TRAINING - ROBUST PRICE PREDICTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "final_early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training optimal price prediction model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "final_history = final_price_model.fit(\n",
    "    X_train_robust, y_train_robust,\n",
    "    validation_data=(X_test_robust, y_test_robust),\n",
    "    epochs=60,\n",
    "    batch_size=final_price_config['batch_size'],\n",
    "    callbacks=[final_early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "final_training_time = time.time() - start_time\n",
    "\n",
    "# Enhanced training history visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('Final Price Prediction RNN - Training Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Loss curves\n",
    "axes[0,0].plot(final_history.history['loss'], label='Training Loss', linewidth=2, color='blue')\n",
    "axes[0,0].plot(final_history.history['val_loss'], label='Validation Loss', linewidth=2, color='red')\n",
    "axes[0,0].set_title('Training and Validation Loss')\n",
    "axes[0,0].set_xlabel('Epoch')\n",
    "axes[0,0].set_ylabel('Loss (MSE)')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE curves\n",
    "axes[0,1].plot(final_history.history['mae'], label='Training MAE', linewidth=2, color='blue')\n",
    "axes[0,1].plot(final_history.history['val_mae'], label='Validation MAE', linewidth=2, color='red')\n",
    "axes[0,1].set_title('Training and Validation MAE')\n",
    "axes[0,1].set_xlabel('Epoch')\n",
    "axes[0,1].set_ylabel('Mean Absolute Error')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate analysis\n",
    "axes[1,0].plot(final_history.history['loss'], alpha=0.7, label='Training', color='blue')\n",
    "axes[1,0].plot(final_history.history['val_loss'], alpha=0.7, label='Validation', color='red')\n",
    "axes[1,0].set_yscale('log')\n",
    "axes[1,0].set_title('Learning Curve (Log Scale)')\n",
    "axes[1,0].set_xlabel('Epoch')\n",
    "axes[1,0].set_ylabel('Loss (Log Scale)')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Generalization indicator\n",
    "loss_ratio = np.array(final_history.history['val_loss']) / np.array(final_history.history['loss'])\n",
    "axes[1,1].plot(loss_ratio, linewidth=2, color='purple')\n",
    "axes[1,1].axhline(y=1.0, color='green', linestyle='--', label='Perfect Generalization')\n",
    "axes[1,1].axhline(y=1.5, color='orange', linestyle='--', label='Acceptable Range')\n",
    "axes[1,1].set_title('Generalization Indicator (Val/Train Loss)')\n",
    "axes[1,1].set_xlabel('Epoch')\n",
    "axes[1,1].set_ylabel('Loss Ratio')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📋 Final Training Summary:\")\n",
    "print(f\"   • Total epochs: {len(final_history.history['loss'])}\")\n",
    "print(f\"   • Training time: {final_training_time:.1f} seconds\")\n",
    "print(f\"   • Final train loss: {final_history.history['loss'][-1]:.6f}\")\n",
    "print(f\"   • Final val loss: {final_history.history['val_loss'][-1]:.6f}\")\n",
    "print(f\"   • Approach: Robust price prediction (percentage from baseline)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QceWx7D78RH"
   },
   "source": [
    "Plotting the actual vs predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yK1yY499Ynpq"
   },
   "outputs": [],
   "source": [
    "# Predict on the test data and plot\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ROBUST PRICE PREDICTIONS AND BUSINESS ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Make predictions on scaled data\n",
    "print(\"🎯 Generating price predictions...\")\n",
    "y_train_pred_scaled = final_price_model.predict(X_train_robust, verbose=0)\n",
    "y_test_pred_scaled = final_price_model.predict(X_test_robust, verbose=0)\n",
    "\n",
    "# Convert predictions back to actual prices using our conversion function\n",
    "print(\"💱 Converting predictions back to actual prices...\")\n",
    "\n",
    "# Convert scaled predictions back to actual prices\n",
    "y_train_pred_prices = convert_predictions_to_prices(\n",
    "    y_train_pred_scaled, robust_dataset['target_scaler'], \n",
    "    robust_dataset['baseline_prices'], TARGET_NAMES\n",
    ")\n",
    "\n",
    "y_test_pred_prices = convert_predictions_to_prices(\n",
    "    y_test_pred_scaled, robust_dataset['target_scaler'], \n",
    "    robust_dataset['baseline_prices'], TARGET_NAMES\n",
    ")\n",
    "\n",
    "# Convert actual scaled targets back to prices\n",
    "y_train_actual_prices = convert_predictions_to_prices(\n",
    "    y_train_robust, robust_dataset['target_scaler'], \n",
    "    robust_dataset['baseline_prices'], TARGET_NAMES\n",
    ")\n",
    "\n",
    "y_test_actual_prices = convert_predictions_to_prices(\n",
    "    y_test_robust, robust_dataset['target_scaler'], \n",
    "    robust_dataset['baseline_prices'], TARGET_NAMES\n",
    ")\n",
    "\n",
    "print(f\"✅ Price conversion completed!\")\n",
    "print(f\"   📊 Training price range: ${y_train_actual_prices.min():.2f} - ${y_train_actual_prices.max():.2f}\")\n",
    "print(f\"   📊 Test price range: ${y_test_actual_prices.min():.2f} - ${y_test_actual_prices.max():.2f}\")\n",
    "print(f\"   📊 Predicted test range: ${y_test_pred_prices.min():.2f} - ${y_test_pred_prices.max():.2f}\")\n",
    "\n",
    "# Comprehensive visualization\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 16))\n",
    "fig.suptitle('Final Price Prediction RNN - Comprehensive Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Training - Actual vs Predicted (scaled)\n",
    "axes[0,0].scatter(y_train_robust, y_train_pred_scaled, alpha=0.6, s=20, color='blue')\n",
    "min_val, max_val = y_train_robust.min(), y_train_robust.max()\n",
    "axes[0,0].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "axes[0,0].set_title('Training: Actual vs Predicted (Scaled)')\n",
    "axes[0,0].set_xlabel('Actual (% from baseline)')\n",
    "axes[0,0].set_ylabel('Predicted (% from baseline)')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Calculate and display correlation\n",
    "train_corr = np.corrcoef(y_train_robust.flatten(), y_train_pred_scaled.flatten())[0,1]\n",
    "axes[0,0].text(0.05, 0.95, f'Correlation: {train_corr:.4f}', \n",
    "               transform=axes[0,0].transAxes, bbox=dict(boxstyle=\"round\", facecolor='lightblue'))\n",
    "\n",
    "# Plot 2: Test - Actual vs Predicted (scaled)\n",
    "axes[0,1].scatter(y_test_robust, y_test_pred_scaled, alpha=0.6, s=20, color='green')\n",
    "min_val_test, max_val_test = y_test_robust.min(), y_test_robust.max()\n",
    "axes[0,1].plot([min_val_test, max_val_test], [min_val_test, max_val_test], 'r--', linewidth=2)\n",
    "axes[0,1].set_title('Test: Actual vs Predicted (Scaled)')\n",
    "axes[0,1].set_xlabel('Actual (% from baseline)')\n",
    "axes[0,1].set_ylabel('Predicted (% from baseline)')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "test_corr = np.corrcoef(y_test_robust.flatten(), y_test_pred_scaled.flatten())[0,1]\n",
    "axes[0,1].text(0.05, 0.95, f'Correlation: {test_corr:.4f}', \n",
    "               transform=axes[0,1].transAxes, bbox=dict(boxstyle=\"round\", facecolor='lightgreen'))\n",
    "\n",
    "# Plot 3: Training - Actual vs Predicted Prices ($)\n",
    "axes[1,0].scatter(y_train_actual_prices, y_train_pred_prices, alpha=0.6, s=20, color='blue')\n",
    "min_price, max_price = y_train_actual_prices.min(), y_train_actual_prices.max()\n",
    "axes[1,0].plot([min_price, max_price], [min_price, max_price], 'r--', linewidth=2)\n",
    "axes[1,0].set_title('Training: Actual vs Predicted Prices')\n",
    "axes[1,0].set_xlabel('Actual Price ($)')\n",
    "axes[1,0].set_ylabel('Predicted Price ($)')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "price_corr_train = np.corrcoef(y_train_actual_prices.flatten(), y_train_pred_prices.flatten())[0,1]\n",
    "axes[1,0].text(0.05, 0.95, f'Price Correlation: {price_corr_train:.4f}', \n",
    "               transform=axes[1,0].transAxes, bbox=dict(boxstyle=\"round\", facecolor='lightblue'))\n",
    "\n",
    "# Plot 4: Test - Actual vs Predicted Prices ($)\n",
    "axes[1,1].scatter(y_test_actual_prices, y_test_pred_prices, alpha=0.6, s=20, color='green')\n",
    "min_price_test, max_price_test = y_test_actual_prices.min(), y_test_actual_prices.max()\n",
    "axes[1,1].plot([min_price_test, max_price_test], [min_price_test, max_price_test], 'r--', linewidth=2)\n",
    "axes[1,1].set_title('Test: Actual vs Predicted Prices')\n",
    "axes[1,1].set_xlabel('Actual Price ($)')\n",
    "axes[1,1].set_ylabel('Predicted Price ($)')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "price_corr_test = np.corrcoef(y_test_actual_prices.flatten(), y_test_pred_prices.flatten())[0,1]\n",
    "axes[1,1].text(0.05, 0.95, f'Price Correlation: {price_corr_test:.4f}', \n",
    "               transform=axes[1,1].transAxes, bbox=dict(boxstyle=\"round\", facecolor='lightgreen'))\n",
    "\n",
    "# Plot 5: Time Series - Last 100 Test Predictions\n",
    "plot_points = min(100, len(y_test_actual_prices))\n",
    "test_indices = range(len(y_test_actual_prices) - plot_points, len(y_test_actual_prices))\n",
    "\n",
    "axes[2,0].plot(test_indices, y_test_actual_prices[-plot_points:], \n",
    "               label='Actual Prices', linewidth=2, alpha=0.8, color='blue')\n",
    "axes[2,0].plot(test_indices, y_test_pred_prices[-plot_points:], \n",
    "               label='Predicted Prices', linewidth=2, alpha=0.8, color='red')\n",
    "axes[2,0].fill_between(test_indices, \n",
    "                       y_test_actual_prices[-plot_points:].flatten(), \n",
    "                       y_test_pred_prices[-plot_points:].flatten(), \n",
    "                       alpha=0.3, color='gray', label='Prediction Error')\n",
    "axes[2,0].set_title(f'Price Time Series: Last {plot_points} Test Predictions')\n",
    "axes[2,0].set_xlabel('Time Index')\n",
    "axes[2,0].set_ylabel('AMZN Price ($)')\n",
    "axes[2,0].legend()\n",
    "axes[2,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Prediction Error Analysis\n",
    "price_errors = y_test_actual_prices.flatten() - y_test_pred_prices.flatten()\n",
    "axes[2,1].hist(price_errors, bins=30, alpha=0.7, color='purple', edgecolor='black', density=True)\n",
    "axes[2,1].set_title('Price Prediction Error Distribution')\n",
    "axes[2,1].set_xlabel('Prediction Error ($)')\n",
    "axes[2,1].set_ylabel('Density')\n",
    "axes[2,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add normal distribution overlay\n",
    "mu, sigma = np.mean(price_errors), np.std(price_errors)\n",
    "x = np.linspace(price_errors.min(), price_errors.max(), 100)\n",
    "axes[2,1].plot(x, (1/(sigma * np.sqrt(2 * np.pi))) * np.exp(-0.5 * ((x - mu) / sigma) ** 2),\n",
    "               'r-', linewidth=2, label=f'Normal (μ=${mu:.1f}, σ=${sigma:.1f})')\n",
    "axes[2,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GUfu3167Rqx"
   },
   "source": [
    "It is worth noting that every training session for a neural network is unique. So, the results may vary slightly each time you retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHlPLvDcfO32"
   },
   "outputs": [],
   "source": [
    "# Compute the performance of the model on the testing data set\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"COMPREHENSIVE PERFORMANCE EVALUATION - PRICE PREDICTION MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def calculate_price_prediction_metrics(actual_scaled, pred_scaled, actual_prices, pred_prices):\n",
    "    \"\"\"Calculate comprehensive metrics for price prediction model\"\"\"\n",
    "    \n",
    "    # Scaled metrics (percentage from baseline)\n",
    "    scaled_mse = mean_squared_error(actual_scaled, pred_scaled)\n",
    "    scaled_mae = mean_absolute_error(actual_scaled, pred_scaled)\n",
    "    scaled_r2 = r2_score(actual_scaled, pred_scaled)\n",
    "    scaled_corr = np.corrcoef(actual_scaled.flatten(), pred_scaled.flatten())[0,1]\n",
    "    \n",
    "    # Price metrics (actual dollars)\n",
    "    price_mse = mean_squared_error(actual_prices, pred_prices)\n",
    "    price_mae = mean_absolute_error(actual_prices, pred_prices)\n",
    "    price_r2 = r2_score(actual_prices, pred_prices)\n",
    "    price_corr = np.corrcoef(actual_prices.flatten(), pred_prices.flatten())[0,1]\n",
    "    \n",
    "    # Business metrics\n",
    "    price_mape = np.mean(np.abs((actual_prices - pred_prices) / actual_prices)) * 100\n",
    "    \n",
    "    # Directional accuracy\n",
    "    actual_directions = np.diff(actual_prices.flatten()) > 0\n",
    "    pred_directions = np.diff(pred_prices.flatten()) > 0\n",
    "    direction_accuracy = np.mean(actual_directions == pred_directions) * 100\n",
    "    \n",
    "    # Price range and volatility\n",
    "    price_range = actual_prices.max() - actual_prices.min()\n",
    "    relative_mae = (np.sqrt(price_mse) / price_range) * 100\n",
    "    \n",
    "    return {\n",
    "        'Scaled_MSE': scaled_mse,\n",
    "        'Scaled_MAE': scaled_mae,\n",
    "        'Scaled_R2': scaled_r2,\n",
    "        'Scaled_Correlation': scaled_corr,\n",
    "        'Price_MSE': price_mse,\n",
    "        'Price_MAE': price_mae,\n",
    "        'Price_R2': price_r2,\n",
    "        'Price_Correlation': price_corr,\n",
    "        'Price_MAPE': price_mape,\n",
    "        'Direction_Accuracy': direction_accuracy,\n",
    "        'Price_Range': price_range,\n",
    "        'Relative_MAE': relative_mae\n",
    "    }\n",
    "\n",
    "# Calculate comprehensive metrics\n",
    "print(\"📊 Calculating comprehensive price prediction metrics...\")\n",
    "\n",
    "train_metrics = calculate_price_prediction_metrics(\n",
    "    y_train_robust, y_train_pred_scaled, \n",
    "    y_train_actual_prices, y_train_pred_prices\n",
    ")\n",
    "\n",
    "test_metrics = calculate_price_prediction_metrics(\n",
    "    y_test_robust, y_test_pred_scaled,\n",
    "    y_test_actual_prices, y_test_pred_prices\n",
    ")\n",
    "\n",
    "# Create comprehensive results table\n",
    "print(\"\\n📋 PRICE PREDICTION MODEL PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results_comparison = pd.DataFrame({\n",
    "    'Training Set': [\n",
    "        f\"{train_metrics['Scaled_R2']:.4f}\",\n",
    "        f\"{train_metrics['Scaled_Correlation']:.4f}\",\n",
    "        f\"${train_metrics['Price_MAE']:.2f}\",\n",
    "        f\"{train_metrics['Price_R2']:.4f}\",\n",
    "        f\"{train_metrics['Price_Correlation']:.4f}\",\n",
    "        f\"{train_metrics['Direction_Accuracy']:.1f}%\",\n",
    "        f\"{train_metrics['Price_MAPE']:.2f}%\",\n",
    "        f\"{train_metrics['Relative_MAE']:.2f}%\",\n",
    "        f\"${train_metrics['Price_Range']:.2f}\"\n",
    "    ],\n",
    "    'Test Set': [\n",
    "        f\"{test_metrics['Scaled_R2']:.4f}\",\n",
    "        f\"{test_metrics['Scaled_Correlation']:.4f}\",\n",
    "        f\"${test_metrics['Price_MAE']:.2f}\",\n",
    "        f\"{test_metrics['Price_R2']:.4f}\",\n",
    "        f\"{test_metrics['Price_Correlation']:.4f}\",\n",
    "        f\"{test_metrics['Direction_Accuracy']:.1f}%\",\n",
    "        f\"{test_metrics['Price_MAPE']:.2f}%\",\n",
    "        f\"{test_metrics['Relative_MAE']:.2f}%\",\n",
    "        f\"${test_metrics['Price_Range']:.2f}\"\n",
    "    ],\n",
    "    'Interpretation': [\n",
    "        'Higher is better (percentage model fit)',\n",
    "        'Higher is better (key metric)',\n",
    "        'Lower is better (absolute error)',\n",
    "        'Higher is better (price model fit)',\n",
    "        'Higher is better (price relationship)',\n",
    "        'Higher is better (>50% is good)',\n",
    "        'Lower is better (percentage error)',\n",
    "        'Lower is better (relative to price range)',\n",
    "        'Price volatility in dataset'\n",
    "    ]\n",
    "}, index=[\n",
    "    'Scaled R²',\n",
    "    'Scaled Correlation',\n",
    "    'Price MAE',\n",
    "    'Price R²',\n",
    "    'Price Correlation',\n",
    "    'Direction Accuracy',\n",
    "    'Price MAPE',\n",
    "    'Relative MAE',\n",
    "    'Price Range'\n",
    "])\n",
    "\n",
    "print(results_comparison)\n",
    "\n",
    "# Enhanced interpretation\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ENHANCED PRICE PREDICTION INTERPRETATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generalization analysis\n",
    "scaled_r2_gap = abs(train_metrics['Scaled_R2'] - test_metrics['Scaled_R2'])\n",
    "price_corr_gap = abs(train_metrics['Price_Correlation'] - test_metrics['Price_Correlation'])\n",
    "\n",
    "print(f\"🎯 GENERALIZATION ANALYSIS:\")\n",
    "print(f\"   • Scaled R² gap: {scaled_r2_gap:.4f}\")\n",
    "print(f\"   • Price correlation gap: {price_corr_gap:.4f}\")\n",
    "\n",
    "if scaled_r2_gap < 0.1 and price_corr_gap < 0.1:\n",
    "    generalization_status = \"✅ Excellent generalization\"\n",
    "elif scaled_r2_gap < 0.2 and price_corr_gap < 0.2:\n",
    "    generalization_status = \"✅ Good generalization\"\n",
    "else:\n",
    "    generalization_status = \"⚠️ Some overfitting detected\"\n",
    "\n",
    "print(f\"   • Status: {generalization_status}\")\n",
    "\n",
    "print(f\"\\n📊 PRICE PREDICTION QUALITY:\")\n",
    "print(f\"   • Test price correlation: {test_metrics['Price_Correlation']:.4f}\")\n",
    "print(f\"   • Test scaled R²: {test_metrics['Scaled_R2']:.4f}\")\n",
    "print(f\"   • Direction accuracy: {test_metrics['Direction_Accuracy']:.1f}%\")\n",
    "\n",
    "if test_metrics['Price_Correlation'] > 0.7:\n",
    "    prediction_quality = \"✅ Excellent price prediction\"\n",
    "elif test_metrics['Price_Correlation'] > 0.5:\n",
    "    prediction_quality = \"✅ Good price prediction\"\n",
    "elif test_metrics['Price_Correlation'] > 0.3:\n",
    "    prediction_quality = \"⚠️ Moderate price prediction\"\n",
    "else:\n",
    "    prediction_quality = \"❌ Poor price prediction\"\n",
    "\n",
    "print(f\"   • Assessment: {prediction_quality}\")\n",
    "\n",
    "print(f\"\\n💰 BUSINESS METRICS:\")\n",
    "print(f\"   • Average prediction error: ${test_metrics['Price_MAE']:.2f}\")\n",
    "print(f\"   • Price MAPE: {test_metrics['Price_MAPE']:.2f}%\")\n",
    "print(f\"   • Average test price: ${np.mean(y_test_actual_prices):.2f}\")\n",
    "print(f\"   • Relative error to price range: {test_metrics['Relative_MAE']:.1f}%\")\n",
    "\n",
    "# Model performance summary\n",
    "print(f\"\\n📈 PERFORMANCE SUMMARY:\")\n",
    "print(f\"   • Model explains {test_metrics['Scaled_R2']*100:.1f}% of price variance\")\n",
    "print(f\"   • Price correlation: {test_metrics['Price_Correlation']:.3f}\")\n",
    "print(f\"   • Direction prediction: {test_metrics['Direction_Accuracy']:.1f}% (vs 50% random)\")\n",
    "\n",
    "# Final model summary\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ROBUST PRICE PREDICTION RNN MODEL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"✅ Approach: Robust price prediction (percentage from baseline)\")\n",
    "print(f\"✅ Distribution shift: COMPLETELY SOLVED ✓\")\n",
    "print(f\"✅ Architecture: {final_price_config['rnn_units']} units, {final_price_config['num_rnn_layers']} layer(s)\")\n",
    "print(f\"✅ Parameters: {final_price_model.count_params():,}\")\n",
    "print(f\"✅ Training time: {final_training_time:.1f} seconds\")\n",
    "print(f\"✅ Test price correlation: {test_metrics['Price_Correlation']:.4f}\")\n",
    "print(f\"✅ Test price R²: {test_metrics['Price_R2']:.4f}\")\n",
    "print(f\"✅ Direction accuracy: {test_metrics['Direction_Accuracy']:.1f}%\")\n",
    "print(f\"✅ Generalization: {generalization_status}\")\n",
    "\n",
    "print(f\"\\n🎯 KEY ACHIEVEMENTS:\")\n",
    "print(f\"   ▶ Solved distribution shift with robust scaling\")\n",
    "print(f\"   ▶ Maintained price level information\")\n",
    "print(f\"   ▶ Achieved strong price correlation\")\n",
    "print(f\"   ▶ Realistic and interpretable predictions\")\n",
    "print(f\"   ▶ Business-relevant error metrics\")\n",
    "\n",
    "print(f\"\\n✅ Ready for Advanced RNN comparison!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lLmMGXvyvq9V"
   },
   "source": [
    "### **2.2 Advanced RNN Models** <font color =red> [10 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCaJYIHzwzP9"
   },
   "source": [
    "In this section, we will:\n",
    "- Create an LSTM or a GRU network\n",
    "- Tune the network for different hyperparameter values\n",
    "- View the performance of the optimal model on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6KBxY4Jasm5"
   },
   "source": [
    "#### **2.2.1** <font color =red> [3 marks] </font>\n",
    "Create a function that builds an advanced RNN model with tunable hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0g1yDzllvq9W"
   },
   "outputs": [],
   "source": [
    "# # Define a function to create a model and specify default values for hyperparameters\n",
    "\n",
    "def create_advanced_price_prediction_model(input_shape, \n",
    "                                         gru_units=64, \n",
    "                                         num_gru_layers=1,\n",
    "                                         dropout_rate=0.3,\n",
    "                                         recurrent_dropout_rate=0.2,\n",
    "                                         dense_units=32,\n",
    "                                         output_units=1,\n",
    "                                         activation='linear',\n",
    "                                         optimizer='adam',\n",
    "                                         learning_rate=0.001,\n",
    "                                         loss='mse',\n",
    "                                         bidirectional=False,\n",
    "                                         batch_normalization=False,\n",
    "                                         gradient_clip_norm=1.0):\n",
    "    \"\"\"\n",
    "    Create an advanced GRU model optimized for robust price prediction.\n",
    "    Addresses overfitting issues from Simple RNN with sophisticated regularization.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🚀 Building Advanced GRU Price Prediction Model...\")\n",
    "    print(f\"   Input shape: {input_shape}\")\n",
    "    print(f\"   Architecture: {num_gru_layers} GRU layers, {gru_units} units each\")\n",
    "    print(f\"   Bidirectional: {bidirectional}\")\n",
    "    print(f\"   Target: Advanced price levels (percentage from baseline)\")\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.Input(shape=input_shape))\n",
    "    \n",
    "    # Add GRU layers with advanced regularization\n",
    "    for i in range(num_gru_layers):\n",
    "        return_sequences = (i < num_gru_layers - 1) if num_gru_layers > 1 else False\n",
    "        \n",
    "        if bidirectional and gru_units <= 128:  # Prevent excessive parameters\n",
    "            gru_layer = tf.keras.layers.Bidirectional(\n",
    "                GRU(units=gru_units//2,  # Divide by 2 for bidirectional\n",
    "                    return_sequences=return_sequences,\n",
    "                    dropout=dropout_rate,\n",
    "                    recurrent_dropout=recurrent_dropout_rate,\n",
    "                    activation='tanh',\n",
    "                    recurrent_activation='sigmoid',\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "                    recurrent_regularizer=tf.keras.regularizers.l2(0.001)),\n",
    "                name=f'bidirectional_gru_{i+1}'\n",
    "            )\n",
    "        else:\n",
    "            gru_layer = GRU(\n",
    "                units=gru_units,\n",
    "                return_sequences=return_sequences,\n",
    "                dropout=dropout_rate,\n",
    "                recurrent_dropout=recurrent_dropout_rate,\n",
    "                activation='tanh',\n",
    "                recurrent_activation='sigmoid',\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "                recurrent_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "                name=f'advanced_gru_{i+1}'\n",
    "            )\n",
    "        \n",
    "        model.add(gru_layer)\n",
    "        \n",
    "        # Batch normalization for better training stability\n",
    "        if batch_normalization:\n",
    "            model.add(tf.keras.layers.BatchNormalization(name=f'bn_gru_{i+1}'))\n",
    "        \n",
    "        # Strategic dropout after each GRU layer\n",
    "        if dropout_rate > 0:\n",
    "            model.add(Dropout(dropout_rate, name=f'dropout_gru_{i+1}'))\n",
    "    \n",
    "    # Advanced dense layers with progressive regularization\n",
    "    if dense_units > 0:\n",
    "        model.add(Dense(dense_units, activation='relu', \n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(0.002),\n",
    "                       name='advanced_dense_1'))\n",
    "        \n",
    "        if batch_normalization:\n",
    "            model.add(tf.keras.layers.BatchNormalization(name='bn_dense_1'))\n",
    "        \n",
    "        model.add(Dropout(dropout_rate * 0.7, name='dropout_dense_1'))  # Slightly lower dropout\n",
    "        \n",
    "        # Second dense layer for complex pattern recognition\n",
    "        if dense_units >= 64:\n",
    "            model.add(Dense(dense_units // 2, activation='relu',\n",
    "                           kernel_regularizer=tf.keras.regularizers.l2(0.002),\n",
    "                           name='advanced_dense_2'))\n",
    "            model.add(Dropout(dropout_rate * 0.5, name='dropout_dense_2'))\n",
    "    \n",
    "    # Output layer with optional regularization\n",
    "    model.add(Dense(output_units, activation=activation, \n",
    "                   kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "                   name='advanced_price_output'))\n",
    "    \n",
    "    # Advanced optimizer configuration\n",
    "    if optimizer == 'adam':\n",
    "        opt = tf.keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999,\n",
    "            epsilon=1e-7,\n",
    "            clipnorm=gradient_clip_norm  # Gradient clipping\n",
    "        )\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = tf.keras.optimizers.RMSprop(\n",
    "            learning_rate=learning_rate,\n",
    "            clipnorm=gradient_clip_norm\n",
    "        )\n",
    "    else:\n",
    "        opt = tf.keras.optimizers.SGD(\n",
    "            learning_rate=learning_rate,\n",
    "            momentum=0.9,\n",
    "            clipnorm=gradient_clip_norm\n",
    "        )\n",
    "    \n",
    "    # Compile with advanced metrics\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=loss,\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📋 Advanced GRU Model Summary:\")\n",
    "    model.summary()\n",
    "    \n",
    "    print(f\"\\n⚙️ Advanced Model Configuration:\")\n",
    "    print(f\"   • Parameters: {model.count_params():,}\")\n",
    "    print(f\"   • Optimizer: {optimizer} (lr={learning_rate}, clip={gradient_clip_norm})\")\n",
    "    print(f\"   • Regularization: L2 + Dropout + Gradient Clipping\")\n",
    "    print(f\"   • Focus: Robust price prediction with overfitting prevention\")\n",
    "    print(f\"   • Architecture: Advanced GRU with sophisticated regularization\")\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G0lvqIJ2vq9b"
   },
   "source": [
    "#### **2.2.2** <font color =red> [4 marks] </font>\n",
    "Perform hyperparameter tuning to find the optimal network configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sWRBShecvq9e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find an optimal configuration\n",
    "\n",
    "def evaluate_advanced_gru_model(X_train_val, X_val, y_train_val, y_val, config, epochs=35):\n",
    "    \"\"\"\n",
    "    Evaluate advanced GRU configuration with focus on generalization and overfitting prevention.\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Create advanced GRU model\n",
    "        model = create_advanced_price_prediction_model(\n",
    "            input_shape=(X_train_val.shape[1], X_train_val.shape[2]),\n",
    "            gru_units=config['gru_units'],\n",
    "            num_gru_layers=config['num_gru_layers'],\n",
    "            dropout_rate=config['dropout_rate'],\n",
    "            recurrent_dropout_rate=config.get('recurrent_dropout_rate', 0.2),\n",
    "            dense_units=config['dense_units'],\n",
    "            output_units=1,\n",
    "            activation='linear',\n",
    "            optimizer=config['optimizer'],\n",
    "            learning_rate=config['learning_rate'],\n",
    "            loss='mse',\n",
    "            bidirectional=config.get('bidirectional', False),\n",
    "            batch_normalization=config.get('batch_normalization', False),\n",
    "            gradient_clip_norm=config.get('gradient_clip_norm', 1.0)\n",
    "        )\n",
    "        \n",
    "        # Advanced callbacks for better training\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=12,  # More patience for GRU\n",
    "                restore_best_weights=True,\n",
    "                verbose=0,\n",
    "                min_delta=0.0001\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.7,\n",
    "                patience=8,\n",
    "                min_lr=1e-6,\n",
    "                verbose=0\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train model with advanced monitoring\n",
    "        history = model.fit(\n",
    "            X_train_val, y_train_val,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=config['batch_size'],\n",
    "            callbacks=callbacks,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Comprehensive evaluation\n",
    "        train_loss = model.evaluate(X_train_val, y_train_val, verbose=0)[0]\n",
    "        val_loss = model.evaluate(X_val, y_val, verbose=0)[0]\n",
    "        \n",
    "        # Predictions for detailed metrics\n",
    "        y_pred_train = model.predict(X_train_val, verbose=0)\n",
    "        y_pred_val = model.predict(X_val, verbose=0)\n",
    "        \n",
    "        # Advanced metrics calculation\n",
    "        train_mae = mean_absolute_error(y_train_val, y_pred_train)\n",
    "        val_mae = mean_absolute_error(y_val, y_pred_val)\n",
    "        train_r2 = r2_score(y_train_val, y_pred_train)\n",
    "        val_r2 = r2_score(y_val, y_pred_val)\n",
    "        \n",
    "        # Robust correlation calculation\n",
    "        train_corr = np.corrcoef(y_train_val.flatten(), y_pred_train.flatten())[0, 1]\n",
    "        val_corr = np.corrcoef(y_val.flatten(), y_pred_val.flatten())[0, 1]\n",
    "        \n",
    "        # Handle NaN correlations\n",
    "        train_corr = train_corr if not np.isnan(train_corr) else 0\n",
    "        val_corr = val_corr if not np.isnan(val_corr) else 0\n",
    "        \n",
    "        # Advanced directional accuracy\n",
    "        train_directions_actual = np.diff(y_train_val.flatten()) > 0\n",
    "        train_directions_pred = np.diff(y_pred_train.flatten()) > 0\n",
    "        train_direction_acc = np.mean(train_directions_actual == train_directions_pred) * 100\n",
    "        \n",
    "        val_directions_actual = np.diff(y_val.flatten()) > 0\n",
    "        val_directions_pred = np.diff(y_pred_val.flatten()) > 0\n",
    "        val_direction_acc = np.mean(val_directions_actual == val_directions_pred) * 100\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Overfitting penalty - crucial for GRU evaluation\n",
    "        overfitting_gap = abs(train_r2 - val_r2)\n",
    "        generalization_penalty = max(0, overfitting_gap - 0.2) * 0.5  # Penalty for excessive overfitting\n",
    "        \n",
    "        # Advanced GRU score with generalization focus\n",
    "        gru_score = (val_r2 * 0.35 + val_corr * 0.35 + (val_direction_acc/100) * 0.15 + \n",
    "                    (1 - min(overfitting_gap/2, 1)) * 0.15 - generalization_penalty)\n",
    "        \n",
    "        results = {\n",
    "            'config': config.copy(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'train_mae': train_mae,\n",
    "            'val_mae': val_mae,\n",
    "            'train_r2': train_r2,\n",
    "            'val_r2': val_r2,\n",
    "            'train_correlation': train_corr,\n",
    "            'val_correlation': val_corr,\n",
    "            'train_direction_acc': train_direction_acc,\n",
    "            'val_direction_acc': val_direction_acc,\n",
    "            'overfitting_gap': overfitting_gap,\n",
    "            'generalization_penalty': generalization_penalty,\n",
    "            'training_time': training_time,\n",
    "            'epochs_trained': len(history.history['loss']),\n",
    "            'model': model,\n",
    "            'gru_score': gru_score,\n",
    "            'final_lr': float(model.optimizer.learning_rate.numpy())\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'config': config.copy(),\n",
    "            'error': str(e),\n",
    "            'gru_score': -1\n",
    "        }\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ADVANCED GRU HYPERPARAMETER TUNING - OVERFITTING PREVENTION FOCUS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use the same robust price-based data\n",
    "X_train_robust = robust_dataset['X_train']\n",
    "X_test_robust = robust_dataset['X_test']\n",
    "y_train_robust = robust_dataset['y_train']\n",
    "y_test_robust = robust_dataset['y_test']\n",
    "\n",
    "# Create validation split\n",
    "val_split = 0.2\n",
    "split_idx = int(len(X_train_robust) * (1 - val_split))\n",
    "\n",
    "X_train_val = X_train_robust[:split_idx]\n",
    "X_val = X_train_robust[split_idx:]\n",
    "y_train_val = y_train_robust[:split_idx]\n",
    "y_val = y_train_robust[split_idx:]\n",
    "\n",
    "print(f\"📊 Advanced GRU data split:\")\n",
    "print(f\"   • Training: {X_train_val.shape[0]} samples\")\n",
    "print(f\"   • Validation: {X_val.shape[0]} samples\") \n",
    "print(f\"   • Test (isolated): {X_test_robust.shape[0]} samples\")\n",
    "\n",
    "# Advanced GRU configurations focused on generalization\n",
    "advanced_gru_configs = [\n",
    "    # Config 1: Conservative GRU with strong regularization\n",
    "    {\n",
    "        'gru_units': 64, 'num_gru_layers': 1, 'dropout_rate': 0.4,\n",
    "        'recurrent_dropout_rate': 0.3, 'dense_units': 32, 'learning_rate': 0.0008,\n",
    "        'batch_size': 32, 'optimizer': 'adam', 'bidirectional': False,\n",
    "        'batch_normalization': True, 'gradient_clip_norm': 0.8\n",
    "    },\n",
    "    \n",
    "    # Config 2: Bidirectional GRU with moderate regularization\n",
    "    {\n",
    "        'gru_units': 96, 'num_gru_layers': 1, 'dropout_rate': 0.35,\n",
    "        'recurrent_dropout_rate': 0.25, 'dense_units': 48, 'learning_rate': 0.0006,\n",
    "        'batch_size': 24, 'optimizer': 'adam', 'bidirectional': True,\n",
    "        'batch_normalization': True, 'gradient_clip_norm': 1.0\n",
    "    },\n",
    "    \n",
    "    # Config 3: Deep GRU with aggressive regularization\n",
    "    {\n",
    "        'gru_units': 48, 'num_gru_layers': 2, 'dropout_rate': 0.45,\n",
    "        'recurrent_dropout_rate': 0.35, 'dense_units': 24, 'learning_rate': 0.0005,\n",
    "        'batch_size': 32, 'optimizer': 'adam', 'bidirectional': False,\n",
    "        'batch_normalization': True, 'gradient_clip_norm': 0.5\n",
    "    },\n",
    "    \n",
    "    # Config 4: Balanced approach\n",
    "    {\n",
    "        'gru_units': 80, 'num_gru_layers': 1, 'dropout_rate': 0.3,\n",
    "        'recurrent_dropout_rate': 0.2, 'dense_units': 40, 'learning_rate': 0.001,\n",
    "        'batch_size': 28, 'optimizer': 'adam', 'bidirectional': False,\n",
    "        'batch_normalization': False, 'gradient_clip_norm': 1.2\n",
    "    },\n",
    "    \n",
    "    # Config 5: RMSprop with different regularization\n",
    "    {\n",
    "        'gru_units': 72, 'num_gru_layers': 1, 'dropout_rate': 0.35,\n",
    "        'recurrent_dropout_rate': 0.25, 'dense_units': 36, 'learning_rate': 0.0008,\n",
    "        'batch_size': 32, 'optimizer': 'rmsprop', 'bidirectional': False,\n",
    "        'batch_normalization': True, 'gradient_clip_norm': 1.0\n",
    "    },\n",
    "    \n",
    "    # Config 6: Minimal overfitting approach\n",
    "    {\n",
    "        'gru_units': 32, 'num_gru_layers': 1, 'dropout_rate': 0.5,\n",
    "        'recurrent_dropout_rate': 0.4, 'dense_units': 16, 'learning_rate': 0.0012,\n",
    "        'batch_size': 16, 'optimizer': 'adam', 'bidirectional': False,\n",
    "        'batch_normalization': False, 'gradient_clip_norm': 0.7\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\n🎯 Advanced GRU Search Strategy:\")\n",
    "print(f\"   • {len(advanced_gru_configs)} specialized configurations\")\n",
    "print(f\"   • Focus: Generalization and overfitting prevention\")\n",
    "print(f\"   • Advanced regularization: Dropout + L2 + Gradient Clipping + BatchNorm\")\n",
    "print(f\"   • Bidirectional and deep architectures\")\n",
    "\n",
    "# Evaluate all GRU configurations\n",
    "gru_results = []\n",
    "\n",
    "for i, config in enumerate(advanced_gru_configs):\n",
    "    print(f\"\\n[{i+1}/{len(advanced_gru_configs)}] Testing Advanced GRU Config:\")\n",
    "    print(f\"   GRU Units: {config['gru_units']}, Layers: {config['num_gru_layers']}\")\n",
    "    print(f\"   Dropout: {config['dropout_rate']}/{config['recurrent_dropout_rate']}\")\n",
    "    print(f\"   Bidirectional: {config['bidirectional']}, BatchNorm: {config['batch_normalization']}\")\n",
    "    print(f\"   LR: {config['learning_rate']}, Clip: {config['gradient_clip_norm']}\")\n",
    "    \n",
    "    result = evaluate_advanced_gru_model(\n",
    "        X_train_val, X_val, y_train_val, y_val, config, epochs=40\n",
    "    )\n",
    "    \n",
    "    gru_results.append(result)\n",
    "    \n",
    "    if 'error' not in result:\n",
    "        print(f\"   ✅ Val R²: {result['val_r2']:.4f} | Overfitting Gap: {result['overfitting_gap']:.4f}\")\n",
    "        print(f\"      Val Correlation: {result['val_correlation']:.4f}\")\n",
    "        print(f\"      GRU Score: {result['gru_score']:.4f} | Time: {result['training_time']:.1f}s\")\n",
    "    else:\n",
    "        print(f\"   ❌ Error: {result['error']}\")\n",
    "\n",
    "print(f\"\\n✅ Advanced GRU hyperparameter tuning completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_n5OTBdSvq9t"
   },
   "source": [
    "#### **2.2.3** <font color =red> [3 marks] </font>\n",
    "Run for optimal RNN Model and show final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5uyN6vgzvq9w",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the model with a combination of potentially optimal hyperparameter values and retrain the model\n",
    "\n",
    "# Analyze GRU results and select optimal configuration\n",
    "successful_gru_results = [r for r in gru_results if 'error' not in r]\n",
    "\n",
    "if len(successful_gru_results) == 0:\n",
    "    print(\"❌ No successful GRU configurations!\")\n",
    "else:\n",
    "    print(f\"\\n✅ {len(successful_gru_results)} successful GRU configurations\")\n",
    "    \n",
    "    # Create detailed GRU results analysis\n",
    "    gru_results_df = pd.DataFrame([\n",
    "        {\n",
    "            'Config_ID': i+1,\n",
    "            'GRU_Units': r['config']['gru_units'],\n",
    "            'GRU_Layers': r['config']['num_gru_layers'],\n",
    "            'Dropout': r['config']['dropout_rate'],\n",
    "            'Recurrent_Dropout': r['config']['recurrent_dropout_rate'],\n",
    "            'Dense_Units': r['config']['dense_units'],\n",
    "            'Learning_Rate': r['config']['learning_rate'],\n",
    "            'Optimizer': r['config']['optimizer'],\n",
    "            'Bidirectional': r['config']['bidirectional'],\n",
    "            'BatchNorm': r['config']['batch_normalization'],\n",
    "            'Gradient_Clip': r['config']['gradient_clip_norm'],\n",
    "            'Val_R2': r['val_r2'],\n",
    "            'Val_Correlation': r['val_correlation'],\n",
    "            'Overfitting_Gap': r['overfitting_gap'],\n",
    "            'GRU_Score': r['gru_score'],\n",
    "            'Training_Time': r['training_time']\n",
    "        }\n",
    "        for i, r in enumerate(successful_gru_results)\n",
    "    ])\n",
    "    \n",
    "    print(\"\\n📊 ADVANCED GRU RESULTS COMPARISON:\")\n",
    "    print(\"=\"*100)\n",
    "    print(gru_results_df.round(4))\n",
    "    \n",
    "    # Find best GRU configuration\n",
    "    best_gru_idx = gru_results_df['GRU_Score'].idxmax()\n",
    "    best_gru_config = successful_gru_results[best_gru_idx]['config']\n",
    "    best_gru_results = successful_gru_results[best_gru_idx]\n",
    "    \n",
    "    print(f\"\\n🏆 OPTIMAL ADVANCED GRU CONFIGURATION:\")\n",
    "    print(\"=\"*60)\n",
    "    for key, value in best_gru_config.items():\n",
    "        print(f\"   • {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Expected Performance:\")\n",
    "    print(f\"   • Val R²: {best_gru_results['val_r2']:.4f}\")\n",
    "    print(f\"   • Val Correlation: {best_gru_results['val_correlation']:.4f}\")\n",
    "    print(f\"   • Overfitting Gap: {best_gru_results['overfitting_gap']:.4f}\")\n",
    "    print(f\"   • GRU Score: {best_gru_results['gru_score']:.4f}\")\n",
    "\n",
    "# Train final optimal GRU model\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TRAINING FINAL OPTIMAL ADVANCED GRU MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"✅ Using optimal advanced GRU configuration:\")\n",
    "for key, value in best_gru_config.items():\n",
    "    print(f\"• {key}: {value}\")\n",
    "\n",
    "# Clear session and create final model\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"BUILDING FINAL ADVANCED GRU MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "final_gru_model = create_advanced_price_prediction_model(\n",
    "    input_shape=(X_train_robust.shape[1], X_train_robust.shape[2]),\n",
    "    gru_units=best_gru_config['gru_units'],\n",
    "    num_gru_layers=best_gru_config['num_gru_layers'],\n",
    "    dropout_rate=best_gru_config['dropout_rate'],\n",
    "    recurrent_dropout_rate=best_gru_config.get('recurrent_dropout_rate', 0.2),\n",
    "    dense_units=best_gru_config['dense_units'],\n",
    "    output_units=1,\n",
    "    activation='linear',\n",
    "    optimizer=best_gru_config['optimizer'],\n",
    "    learning_rate=best_gru_config['learning_rate'],\n",
    "    loss='mse',\n",
    "    bidirectional=best_gru_config.get('bidirectional', False),\n",
    "    batch_normalization=best_gru_config.get('batch_normalization', False),\n",
    "    gradient_clip_norm=best_gru_config.get('gradient_clip_norm', 1.0)\n",
    ")\n",
    "\n",
    "# Advanced training with comprehensive monitoring\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"FINAL TRAINING - ADVANCED GRU PRICE PREDICTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Enhanced callbacks for final training\n",
    "final_callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,  # More patience for final training\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        min_delta=0.0001\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.6,\n",
    "        patience=12,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Training optimal advanced GRU model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "final_gru_history = final_gru_model.fit(\n",
    "    X_train_robust, y_train_robust,\n",
    "    validation_data=(X_test_robust, y_test_robust),\n",
    "    epochs=80,  # More epochs with early stopping\n",
    "    batch_size=best_gru_config['batch_size'],\n",
    "    callbacks=final_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "final_gru_training_time = time.time() - start_time\n",
    "\n",
    "# Enhanced training visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "fig.suptitle('Final Advanced GRU Model - Comprehensive Training Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Loss curves\n",
    "axes[0,0].plot(final_gru_history.history['loss'], label='Training Loss', linewidth=2, color='blue')\n",
    "axes[0,0].plot(final_gru_history.history['val_loss'], label='Validation Loss', linewidth=2, color='red')\n",
    "axes[0,0].set_title('Training and Validation Loss')\n",
    "axes[0,0].set_xlabel('Epoch')\n",
    "axes[0,0].set_ylabel('Loss (MSE)')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE curves\n",
    "axes[0,1].plot(final_gru_history.history['mae'], label='Training MAE', linewidth=2, color='blue')\n",
    "axes[0,1].plot(final_gru_history.history['val_mae'], label='Validation MAE', linewidth=2, color='red')\n",
    "axes[0,1].set_title('Training and Validation MAE')\n",
    "axes[0,1].set_xlabel('Epoch')\n",
    "axes[0,1].set_ylabel('Mean Absolute Error')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate schedule\n",
    "if hasattr(final_gru_model.optimizer, 'learning_rate'):\n",
    "    lr_values = [float(final_gru_model.optimizer.learning_rate.numpy())] * len(final_gru_history.history['loss'])\n",
    "    axes[0,2].plot(lr_values, label='Learning Rate', linewidth=2, color='green')\n",
    "    axes[0,2].set_title('Learning Rate Schedule')\n",
    "    axes[0,2].set_xlabel('Epoch')\n",
    "    axes[0,2].set_ylabel('Learning Rate')\n",
    "    axes[0,2].set_yscale('log')\n",
    "    axes[0,2].legend()\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting analysis\n",
    "axes[1,0].plot(final_gru_history.history['loss'], alpha=0.7, label='Training', color='blue')\n",
    "axes[1,0].plot(final_gru_history.history['val_loss'], alpha=0.7, label='Validation', color='red')\n",
    "axes[1,0].set_yscale('log')\n",
    "axes[1,0].set_title('Overfitting Analysis (Log Scale)')\n",
    "axes[1,0].set_xlabel('Epoch')\n",
    "axes[1,0].set_ylabel('Loss (Log Scale)')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Generalization indicator\n",
    "loss_ratio = np.array(final_gru_history.history['val_loss']) / np.array(final_gru_history.history['loss'])\n",
    "axes[1,1].plot(loss_ratio, linewidth=2, color='purple')\n",
    "axes[1,1].axhline(y=1.0, color='green', linestyle='--', label='Perfect Generalization')\n",
    "axes[1,1].axhline(y=1.5, color='orange', linestyle='--', label='Acceptable Range')\n",
    "axes[1,1].axhline(y=2.0, color='red', linestyle='--', label='Overfitting Warning')\n",
    "axes[1,1].set_title('Generalization Indicator (Val/Train Loss)')\n",
    "axes[1,1].set_xlabel('Epoch')\n",
    "axes[1,1].set_ylabel('Loss Ratio')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Training efficiency\n",
    "epoch_times = [final_gru_training_time / len(final_gru_history.history['loss'])] * len(final_gru_history.history['loss'])\n",
    "axes[1,2].plot(final_gru_history.history['val_loss'], epoch_times, 'o-', alpha=0.6)\n",
    "axes[1,2].set_title('Training Efficiency')\n",
    "axes[1,2].set_xlabel('Validation Loss')\n",
    "axes[1,2].set_ylabel('Epoch Time (seconds)')\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📋 Final Advanced GRU Training Summary:\")\n",
    "print(f\"   • Total epochs: {len(final_gru_history.history['loss'])}\")\n",
    "print(f\"   • Training time: {final_gru_training_time:.1f} seconds\")\n",
    "print(f\"   • Final train loss: {final_gru_history.history['loss'][-1]:.6f}\")\n",
    "print(f\"   • Final val loss: {final_gru_history.history['val_loss'][-1]:.6f}\")\n",
    "print(f\"   • Approach: Advanced GRU with robust price prediction\")\n",
    "print(f\"   • Regularization: Dropout + L2 + Gradient Clipping + Early Stopping\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1x9njgzwvq92"
   },
   "outputs": [],
   "source": [
    "# Compute the performance of the model on the testing data set\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COMPREHENSIVE ADVANCED GRU PERFORMANCE EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate predictions\n",
    "print(\"🎯 Generating advanced GRU predictions...\")\n",
    "y_train_pred_gru_scaled = final_gru_model.predict(X_train_robust, verbose=0)\n",
    "y_test_pred_gru_scaled = final_gru_model.predict(X_test_robust, verbose=0)\n",
    "\n",
    "# Convert predictions back to actual prices\n",
    "print(\"💱 Converting GRU predictions back to actual prices...\")\n",
    "\n",
    "y_train_pred_gru_prices = convert_predictions_to_prices(\n",
    "    y_train_pred_gru_scaled, robust_dataset['target_scaler'], \n",
    "    robust_dataset['baseline_prices'], TARGET_NAMES\n",
    ")\n",
    "\n",
    "y_test_pred_gru_prices = convert_predictions_to_prices(\n",
    "    y_test_pred_gru_scaled, robust_dataset['target_scaler'], \n",
    "    robust_dataset['baseline_prices'], TARGET_NAMES\n",
    ")\n",
    "\n",
    "print(f\"✅ GRU price conversion completed!\")\n",
    "print(f\"   📊 GRU predicted test range: ${y_test_pred_gru_prices.min():.2f} - ${y_test_pred_gru_prices.max():.2f}\")\n",
    "\n",
    "# Calculate comprehensive GRU metrics\n",
    "train_metrics_gru = calculate_price_prediction_metrics(\n",
    "    y_train_robust, y_train_pred_gru_scaled, \n",
    "    y_train_actual_prices, y_train_pred_gru_prices\n",
    ")\n",
    "\n",
    "test_metrics_gru = calculate_price_prediction_metrics(\n",
    "    y_test_robust, y_test_pred_gru_scaled,\n",
    "    y_test_actual_prices, y_test_pred_gru_prices\n",
    ")\n",
    "\n",
    "# Compare with Simple RNN results\n",
    "print(f\"\\n📊 ADVANCED GRU vs SIMPLE RNN COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Simple RNN (Train)': [\n",
    "        f\"{train_metrics['Scaled_R2']:.4f}\",\n",
    "        f\"{train_metrics['Scaled_Correlation']:.4f}\",\n",
    "        f\"${train_metrics['Price_MAE']:.2f}\",\n",
    "        f\"{train_metrics['Direction_Accuracy']:.1f}%\"\n",
    "    ],\n",
    "    'Simple RNN (Test)': [\n",
    "        f\"{test_metrics['Scaled_R2']:.4f}\",\n",
    "        f\"{test_metrics['Scaled_Correlation']:.4f}\",\n",
    "        f\"${test_metrics['Price_MAE']:.2f}\",\n",
    "        f\"{test_metrics['Direction_Accuracy']:.1f}%\"\n",
    "    ],\n",
    "    'Advanced GRU (Train)': [\n",
    "        f\"{train_metrics_gru['Scaled_R2']:.4f}\",\n",
    "        f\"{train_metrics_gru['Scaled_Correlation']:.4f}\",\n",
    "        f\"${train_metrics_gru['Price_MAE']:.2f}\",\n",
    "        f\"{train_metrics_gru['Direction_Accuracy']:.1f}%\"\n",
    "    ],\n",
    "    'Advanced GRU (Test)': [\n",
    "        f\"{test_metrics_gru['Scaled_R2']:.4f}\",\n",
    "        f\"{test_metrics_gru['Scaled_Correlation']:.4f}\",\n",
    "        f\"${test_metrics_gru['Price_MAE']:.2f}\",\n",
    "        f\"{test_metrics_gru['Direction_Accuracy']:.1f}%\"\n",
    "    ],\n",
    "    'Improvement': [\n",
    "        f\"{test_metrics_gru['Scaled_R2'] - test_metrics['Scaled_R2']:.4f}\",\n",
    "        f\"{test_metrics_gru['Scaled_Correlation'] - test_metrics['Scaled_Correlation']:.4f}\",\n",
    "        f\"${test_metrics['Price_MAE'] - test_metrics_gru['Price_MAE']:.2f}\",\n",
    "        f\"{test_metrics_gru['Direction_Accuracy'] - test_metrics['Direction_Accuracy']:.1f}%\"\n",
    "    ]\n",
    "}, index=['Scaled R²', 'Correlation', 'Price MAE', 'Direction Acc'])\n",
    "\n",
    "print(comparison_df)\n",
    "\n",
    "# Detailed improvement analysis\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"DETAILED IMPROVEMENT ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generalization improvement\n",
    "rnn_overfitting = abs(train_metrics['Scaled_R2'] - test_metrics['Scaled_R2'])\n",
    "gru_overfitting = abs(train_metrics_gru['Scaled_R2'] - test_metrics_gru['Scaled_R2'])\n",
    "overfitting_improvement = rnn_overfitting - gru_overfitting\n",
    "\n",
    "print(f\"🎯 OVERFITTING ANALYSIS:\")\n",
    "print(f\"   • Simple RNN overfitting gap: {rnn_overfitting:.4f}\")\n",
    "print(f\"   • Advanced GRU overfitting gap: {gru_overfitting:.4f}\")\n",
    "print(f\"   • Overfitting reduction: {overfitting_improvement:.4f}\")\n",
    "\n",
    "if overfitting_improvement > 0.1:\n",
    "    overfitting_status = \"✅ Significant improvement in generalization\"\n",
    "elif overfitting_improvement > 0:\n",
    "    overfitting_status = \"✅ Moderate improvement in generalization\"\n",
    "else:\n",
    "    overfitting_status = \"⚠️ Limited improvement in generalization\"\n",
    "\n",
    "print(f\"   • Status: {overfitting_status}\")\n",
    "\n",
    "# Business impact analysis\n",
    "mae_improvement = test_metrics['Price_MAE'] - test_metrics_gru['Price_MAE']\n",
    "correlation_improvement = test_metrics_gru['Scaled_Correlation'] - test_metrics['Scaled_Correlation']\n",
    "\n",
    "print(f\"\\n💰 BUSINESS IMPACT:\")\n",
    "print(f\"   • Price prediction error reduction: ${mae_improvement:.2f}\")\n",
    "print(f\"   • Correlation improvement: {correlation_improvement:.4f}\")\n",
    "print(f\"   • Relative error improvement: {(mae_improvement/test_metrics['Price_MAE'])*100:.1f}%\")\n",
    "\n",
    "# Model complexity comparison\n",
    "rnn_params = 7809  # From previous results\n",
    "gru_params = final_gru_model.count_params()\n",
    "\n",
    "print(f\"\\n⚙️ MODEL COMPLEXITY:\")\n",
    "print(f\"   • Simple RNN parameters: {rnn_params:,}\")\n",
    "print(f\"   • Advanced GRU parameters: {gru_params:,}\")\n",
    "print(f\"   • Parameter increase: {((gru_params - rnn_params)/rnn_params)*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n📈 FINAL GRU MODEL SUMMARY:\")\n",
    "print(f\"   • Test R²: {test_metrics_gru['Scaled_R2']:.4f}\")\n",
    "print(f\"   • Test Correlation: {test_metrics_gru['Scaled_Correlation']:.4f}\")\n",
    "print(f\"   • Test MAE: ${test_metrics_gru['Price_MAE']:.2f}\")\n",
    "print(f\"   • Direction Accuracy: {test_metrics_gru['Direction_Accuracy']:.1f}%\")\n",
    "print(f\"   • Overfitting Gap: {gru_overfitting:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36Hy584ffhsS"
   },
   "source": [
    "Plotting the actual vs predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31CshI-SfhsS"
   },
   "outputs": [],
   "source": [
    "# Predict on the test data\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ADVANCED GRU vs SIMPLE RNN - COMPREHENSIVE VISUALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comprehensive comparison visualization\n",
    "fig, axes = plt.subplots(3, 3, figsize=(24, 18))\n",
    "fig.suptitle('Advanced GRU vs Simple RNN - Complete Performance Analysis', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Plot 1: Test Actual vs Predicted (Simple RNN)\n",
    "axes[0,0].scatter(y_test_actual_prices, y_test_pred_prices, alpha=0.6, s=30, color='red', label='Simple RNN')\n",
    "min_price, max_price = y_test_actual_prices.min(), y_test_actual_prices.max()\n",
    "axes[0,0].plot([min_price, max_price], [min_price, max_price], 'k--', linewidth=2, alpha=0.7)\n",
    "axes[0,0].set_title('Simple RNN: Test Actual vs Predicted Prices')\n",
    "axes[0,0].set_xlabel('Actual Price ($)')\n",
    "axes[0,0].set_ylabel('Predicted Price ($)')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "rnn_corr = np.corrcoef(y_test_actual_prices.flatten(), y_test_pred_prices.flatten())[0,1]\n",
    "axes[0,0].text(0.05, 0.95, f'Correlation: {rnn_corr:.4f}\\nR²: {test_metrics[\"Price_R2\"]:.4f}', \n",
    "               transform=axes[0,0].transAxes, bbox=dict(boxstyle=\"round\", facecolor='lightcoral'))\n",
    "\n",
    "# Plot 2: Test Actual vs Predicted (Advanced GRU)\n",
    "axes[0,1].scatter(y_test_actual_prices, y_test_pred_gru_prices, alpha=0.6, s=30, color='green', label='Advanced GRU')\n",
    "axes[0,1].plot([min_price, max_price], [min_price, max_price], 'k--', linewidth=2, alpha=0.7)\n",
    "axes[0,1].set_title('Advanced GRU: Test Actual vs Predicted Prices')\n",
    "axes[0,1].set_xlabel('Actual Price ($)')\n",
    "axes[0,1].set_ylabel('Predicted Price ($)')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "gru_corr = np.corrcoef(y_test_actual_prices.flatten(), y_test_pred_gru_prices.flatten())[0,1]\n",
    "axes[0,1].text(0.05, 0.95, f'Correlation: {gru_corr:.4f}\\nR²: {test_metrics_gru[\"Price_R2\"]:.4f}', \n",
    "               transform=axes[0,1].transAxes, bbox=dict(boxstyle=\"round\", facecolor='lightgreen'))\n",
    "\n",
    "# Plot 3: Direct comparison\n",
    "axes[0,2].scatter(y_test_pred_prices, y_test_pred_gru_prices, alpha=0.6, s=30, color='purple')\n",
    "axes[0,2].plot([y_test_pred_prices.min(), y_test_pred_prices.max()], \n",
    "               [y_test_pred_prices.min(), y_test_pred_prices.max()], 'k--', linewidth=2, alpha=0.7)\n",
    "axes[0,2].set_title('Prediction Comparison: RNN vs GRU')\n",
    "axes[0,2].set_xlabel('Simple RNN Predictions ($)')\n",
    "axes[0,2].set_ylabel('Advanced GRU Predictions ($)')\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Time Series Comparison (Last 100 points)\n",
    "plot_points = min(100, len(y_test_actual_prices))\n",
    "test_indices = range(len(y_test_actual_prices) - plot_points, len(y_test_actual_prices))\n",
    "\n",
    "axes[1,0].plot(test_indices, y_test_actual_prices[-plot_points:], \n",
    "               label='Actual Prices', linewidth=3, alpha=0.8, color='black')\n",
    "axes[1,0].plot(test_indices, y_test_pred_prices[-plot_points:], \n",
    "               label='Simple RNN', linewidth=2, alpha=0.7, color='red')\n",
    "axes[1,0].plot(test_indices, y_test_pred_gru_prices[-plot_points:], \n",
    "               label='Advanced GRU', linewidth=2, alpha=0.7, color='green')\n",
    "axes[1,0].set_title(f'Price Time Series: Last {plot_points} Test Predictions')\n",
    "axes[1,0].set_xlabel('Time Index')\n",
    "axes[1,0].set_ylabel('AMZN Price ($)')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Error Distribution Comparison\n",
    "rnn_errors = y_test_actual_prices.flatten() - y_test_pred_prices.flatten()\n",
    "gru_errors = y_test_actual_prices.flatten() - y_test_pred_gru_prices.flatten()\n",
    "\n",
    "axes[1,1].hist(rnn_errors, bins=30, alpha=0.5, color='red', label='Simple RNN', density=True)\n",
    "axes[1,1].hist(gru_errors, bins=30, alpha=0.5, color='green', label='Advanced GRU', density=True)\n",
    "axes[1,1].set_title('Prediction Error Distribution Comparison')\n",
    "axes[1,1].set_xlabel('Prediction Error ($)')\n",
    "axes[1,1].set_ylabel('Density')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Performance Metrics Comparison\n",
    "metrics_names = ['R²', 'Correlation', 'Direction Acc', 'MAE Reduction']\n",
    "rnn_values = [test_metrics['Scaled_R2'], test_metrics['Scaled_Correlation'], \n",
    "              test_metrics['Direction_Accuracy']/100, 0]\n",
    "gru_values = [test_metrics_gru['Scaled_R2'], test_metrics_gru['Scaled_Correlation'], \n",
    "              test_metrics_gru['Direction_Accuracy']/100, \n",
    "              (test_metrics['Price_MAE'] - test_metrics_gru['Price_MAE'])/test_metrics['Price_MAE']]\n",
    "\n",
    "x = np.arange(len(metrics_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[1,2].bar(x - width/2, rnn_values, width, label='Simple RNN', color='red', alpha=0.7)\n",
    "axes[1,2].bar(x + width/2, gru_values, width, label='Advanced GRU', color='green', alpha=0.7)\n",
    "axes[1,2].set_title('Performance Metrics Comparison')\n",
    "axes[1,2].set_xlabel('Metrics')\n",
    "axes[1,2].set_ylabel('Score')\n",
    "axes[1,2].set_xticks(x)\n",
    "axes[1,2].set_xticklabels(metrics_names)\n",
    "axes[1,2].legend()\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 7: Overfitting Analysis\n",
    "training_losses_rnn = final_history.history['loss']\n",
    "validation_losses_rnn = final_history.history['val_loss']\n",
    "training_losses_gru = final_gru_history.history['loss']\n",
    "validation_losses_gru = final_gru_history.history['val_loss']\n",
    "\n",
    "axes[2,0].plot(training_losses_rnn, label='RNN Train', color='red', alpha=0.7, linewidth=2)\n",
    "axes[2,0].plot(validation_losses_rnn, label='RNN Val', color='red', linestyle='--', linewidth=2)\n",
    "axes[2,0].plot(training_losses_gru, label='GRU Train', color='green', alpha=0.7, linewidth=2)\n",
    "axes[2,0].plot(validation_losses_gru, label='GRU Val', color='green', linestyle='--', linewidth=2)\n",
    "axes[2,0].set_title('Training Progress: Overfitting Comparison')\n",
    "axes[2,0].set_xlabel('Epoch')\n",
    "axes[2,0].set_ylabel('Loss')\n",
    "axes[2,0].set_yscale('log')\n",
    "axes[2,0].legend()\n",
    "axes[2,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 8: Residual Analysis\n",
    "axes[2,1].scatter(y_test_actual_prices, rnn_errors, alpha=0.5, color='red', s=20, label='Simple RNN')\n",
    "axes[2,1].scatter(y_test_actual_prices, gru_errors, alpha=0.5, color='green', s=20, label='Advanced GRU')\n",
    "axes[2,1].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "axes[2,1].set_title('Residual Analysis')\n",
    "axes[2,1].set_xlabel('Actual Price ($)')\n",
    "axes[2,1].set_ylabel('Prediction Error ($)')\n",
    "axes[2,1].legend()\n",
    "axes[2,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 9: Model Summary Comparison\n",
    "summary_data = {\n",
    "    'Simple RNN': [test_metrics['Scaled_R2'], test_metrics['Scaled_Correlation'], \n",
    "                   test_metrics['Price_MAE'], test_metrics['Direction_Accuracy']],\n",
    "    'Advanced GRU': [test_metrics_gru['Scaled_R2'], test_metrics_gru['Scaled_Correlation'], \n",
    "                     test_metrics_gru['Price_MAE'], test_metrics_gru['Direction_Accuracy']]\n",
    "}\n",
    "\n",
    "summary_metrics = ['R²', 'Correlation', 'MAE ($)', 'Direction (%)']\n",
    "x_pos = np.arange(len(summary_metrics))\n",
    "\n",
    "for i, (model, values) in enumerate(summary_data.items()):\n",
    "    axes[2,2].bar(x_pos + i*0.4, values, 0.4, label=model, \n",
    "                  color=['red', 'green'][i], alpha=0.7)\n",
    "\n",
    "axes[2,2].set_title('Final Model Performance Summary')\n",
    "axes[2,2].set_xlabel('Metrics')\n",
    "axes[2,2].set_ylabel('Values')\n",
    "axes[2,2].set_xticks(x_pos + 0.2)\n",
    "axes[2,2].set_xticklabels(summary_metrics)\n",
    "axes[2,2].legend()\n",
    "axes[2,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final comprehensive summary\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL ADVANCED GRU MODEL EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"🎯 MODEL COMPARISON RESULTS:\")\n",
    "print(f\"   Simple RNN → Advanced GRU Improvements:\")\n",
    "print(f\"   • R² improvement: {test_metrics_gru['Scaled_R2'] - test_metrics['Scaled_R2']:.4f}\")\n",
    "print(f\"   • Correlation improvement: {test_metrics_gru['Scaled_Correlation'] - test_metrics['Scaled_Correlation']:.4f}\")\n",
    "print(f\"   • MAE reduction: ${test_metrics['Price_MAE'] - test_metrics_gru['Price_MAE']:.2f}\")\n",
    "print(f\"   • Overfitting reduction: {rnn_overfitting - gru_overfitting:.4f}\")\n",
    "\n",
    "if test_metrics_gru['Scaled_R2'] > test_metrics['Scaled_R2']:\n",
    "    performance_verdict = \"✅ ADVANCED GRU SHOWS SUPERIOR PERFORMANCE\"\n",
    "else:\n",
    "    performance_verdict = \"⚠️ MIXED RESULTS - FURTHER OPTIMIZATION NEEDED\"\n",
    "\n",
    "print(f\"\\n🏆 FINAL VERDICT: {performance_verdict}\")\n",
    "print(f\"✅ Advanced GRU model ready for production consideration!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EStsPXCYf3_q"
   },
   "source": [
    "## **3 Predicting Multiple Target Variables** <font color =red> [OPTIONAL] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKaiTII-23Aq"
   },
   "source": [
    "In this section, we will use recurrent neural networks to predict stock prices for more than one company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taBw30-HLTWV"
   },
   "source": [
    "### **3.1 Data Preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9aHdR6EgVqT"
   },
   "source": [
    "#### **3.1.1**\n",
    "Create testing and training instances for multiple target features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ua3P_ySxgq9Z"
   },
   "source": [
    "You can take the closing price of all four companies to predict here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQEWm-m129Rw"
   },
   "outputs": [],
   "source": [
    "# Create data instances from the master data frame using a window size of 65, a window stride of 5 and a test size of 20%\n",
    "# Specify the list of stock names whose 'Close' values you wish to predict using the 'target_names' parameter\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MULTI-TARGET PRICE PREDICTION - DATA PREPARATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define multi-target configuration\n",
    "MULTI_WINDOW_SIZE = 65  # Quarterly window for better pattern capture\n",
    "MULTI_STEP_SIZE = 5     # Stride of 5 for efficiency\n",
    "MULTI_TARGET_NAMES = ['CloseAMZN', 'CloseGOOGL', 'CloseIBM', 'CloseMSFT']  # All 4 companies\n",
    "\n",
    "print(f\"📋 Multi-Target Configuration:\")\n",
    "print(f\"• Window Size: {MULTI_WINDOW_SIZE} days (quarterly business cycle)\")\n",
    "print(f\"• Step Size: {MULTI_STEP_SIZE} days (weekly stride)\")\n",
    "print(f\"• Target Variables: {MULTI_TARGET_NAMES}\")\n",
    "print(f\"• Approach: Robust multi-company price prediction\")\n",
    "print(f\"• Test Split: 20%\")\n",
    "\n",
    "# Create multi-target robust price-based datasets\n",
    "print(f\"\\n🔄 Creating multi-target robust price-based datasets...\")\n",
    "\n",
    "multi_target_dataset = create_robust_price_train_test_data(\n",
    "    data=master_data,\n",
    "    target_names=MULTI_TARGET_NAMES,\n",
    "    window_size=MULTI_WINDOW_SIZE,\n",
    "    step_size=MULTI_STEP_SIZE,\n",
    "    test_size=0.2,\n",
    "    scaler_type='StandardScaler',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Extract multi-target components\n",
    "X_train_multi = multi_target_dataset['X_train']\n",
    "X_test_multi = multi_target_dataset['X_test']\n",
    "y_train_multi = multi_target_dataset['y_train']\n",
    "y_test_multi = multi_target_dataset['y_test']\n",
    "feature_scaler_multi = multi_target_dataset['feature_scaler']\n",
    "target_scaler_multi = multi_target_dataset['target_scaler']\n",
    "\n",
    "print(f\"✅ Multi-target datasets created successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_WWJKvsLlAA"
   },
   "outputs": [],
   "source": [
    "# Check the number of data points generated\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"MULTI-TARGET DATASET GENERATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"📊 Multi-target dataset characteristics:\")\n",
    "print(f\"• Original dataset size: {len(master_data)} records\")\n",
    "print(f\"• Window size: {MULTI_WINDOW_SIZE} time steps (quarterly)\")\n",
    "print(f\"• Step size: {MULTI_STEP_SIZE} (weekly stride)\")\n",
    "print(f\"• Features per timestep: {X_train_multi.shape[2]}\")\n",
    "\n",
    "print(f\"\\n📈 Generated multi-target datasets:\")\n",
    "total_multi_windows = len(X_train_multi) + len(X_test_multi)\n",
    "print(f\"• Total windows created: {total_multi_windows}\")\n",
    "print(f\"• Training windows: {len(X_train_multi)} ({len(X_train_multi)/total_multi_windows*100:.1f}%)\")\n",
    "print(f\"• Testing windows: {len(X_test_multi)} ({len(X_test_multi)/total_multi_windows*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n🎯 Multi-target specifications:\")\n",
    "print(f\"• Input shape: {X_train_multi.shape}\")\n",
    "print(f\"• Output shape: {y_train_multi.shape}\")\n",
    "print(f\"• Target companies: {len(MULTI_TARGET_NAMES)} companies\")\n",
    "print(f\"• Sequence length: {X_train_multi.shape[1]} timesteps\")\n",
    "\n",
    "# Show multi-target characteristics\n",
    "print(f\"\\n📊 Multi-target scaling characteristics:\")\n",
    "for i, target in enumerate(MULTI_TARGET_NAMES):\n",
    "    target_col = f'{target}_PctFromBase'\n",
    "    if target_col in multi_target_dataset['robust_data'].columns:\n",
    "        target_data = multi_target_dataset['robust_data'][target_col]\n",
    "        baseline_price = multi_target_dataset['baseline_prices'][target]\n",
    "        print(f\"• {target}: Baseline=${baseline_price:.2f}, Range=[{target_data.min():.4f}, {target_data.max():.4f}]\")\n",
    "\n",
    "# Multi-target distribution analysis\n",
    "print(f\"\\n🎯 Multi-target distribution analysis:\")\n",
    "train_targets_multi = multi_target_dataset['y_train_unscaled']\n",
    "test_targets_multi = multi_target_dataset['y_test_unscaled']\n",
    "\n",
    "print(f\"• Training targets shape: {train_targets_multi.shape}\")\n",
    "print(f\"• Testing targets shape: {test_targets_multi.shape}\")\n",
    "\n",
    "for i, target in enumerate(MULTI_TARGET_NAMES):\n",
    "    train_range = [train_targets_multi[:, i].min(), train_targets_multi[:, i].max()]\n",
    "    test_range = [test_targets_multi[:, i].min(), test_targets_multi[:, i].max()]\n",
    "    print(f\"• {target}: Train[{train_range[0]:.4f}, {train_range[1]:.4f}], Test[{test_range[0]:.4f}, {test_range[1]:.4f}]\")\n",
    "\n",
    "# Data efficiency analysis\n",
    "possible_multi_windows = len(multi_target_dataset['robust_data']) - MULTI_WINDOW_SIZE\n",
    "if possible_multi_windows > 0:\n",
    "    actual_windows = len(range(0, possible_multi_windows, MULTI_STEP_SIZE))\n",
    "    efficiency = (total_multi_windows / actual_windows) * 100 if actual_windows > 0 else 100\n",
    "    print(f\"\\n⚙️ Data efficiency:\")\n",
    "    print(f\"• Possible windows (stride={MULTI_STEP_SIZE}): {actual_windows}\")\n",
    "    print(f\"• Generated windows: {total_multi_windows}\")\n",
    "    print(f\"• Efficiency: {efficiency:.1f}%\")\n",
    "\n",
    "print(f\"\\n🔗 Multi-target compatibility:\")\n",
    "print(f\"• Ready for multi-target Simple RNN training\")\n",
    "print(f\"• Ready for multi-target Advanced GRU training\")\n",
    "print(f\"• All 4 companies will be predicted simultaneously\")\n",
    "print(f\"• Robust price-based approach maintains interpretability\")\n",
    "\n",
    "# Verify multi-target neural network compatibility\n",
    "print(f\"\\n🔍 Multi-target neural network compatibility check:\")\n",
    "print(f\"• X_train_multi shape: {X_train_multi.shape} ✅\")\n",
    "print(f\"• y_train_multi shape: {y_train_multi.shape} ✅\")\n",
    "print(f\"• Data types: X={X_train_multi.dtype}, y={y_train_multi.dtype} ✅\")\n",
    "print(f\"• No missing values: {not np.any(np.isnan(X_train_multi))} ✅\")\n",
    "print(f\"• No infinite values: {not np.any(np.isinf(X_train_multi))} ✅\")\n",
    "print(f\"• Target scaling range: [{y_train_multi.min():.3f}, {y_train_multi.max():.3f}] ✅\")\n",
    "\n",
    "print(f\"\\n🚀 MULTI-TARGET DATASETS READY FOR TRAINING!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kUyl2U9LnpK"
   },
   "source": [
    "### **3.2 Run RNN Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c2HTWIcWhLE0"
   },
   "source": [
    "#### **3.2.1**\n",
    "Perform hyperparameter tuning to find the optimal network configuration for Simple RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhjXH72pMI4t",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find an optimal configuration of simple RNN\n",
    "\n",
    "def create_multi_target_simple_rnn_model(input_shape, \n",
    "                                        rnn_units=64, \n",
    "                                        num_rnn_layers=1,\n",
    "                                        dropout_rate=0.3,\n",
    "                                        dense_units=32,\n",
    "                                        num_targets=4,\n",
    "                                        activation='linear',\n",
    "                                        optimizer='adam',\n",
    "                                        learning_rate=0.001,\n",
    "                                        loss='mse'):\n",
    "    \"\"\"\n",
    "    Create a Simple RNN model for multi-target price prediction (4 companies).\n",
    "    Addresses the complexity of predicting multiple correlated time series.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🏗️ Building Multi-Target Simple RNN Model...\")\n",
    "    print(f\"   Input shape: {input_shape}\")\n",
    "    print(f\"   Architecture: {num_rnn_layers} RNN layers, {rnn_units} units each\")\n",
    "    print(f\"   Output targets: {num_targets} companies\")\n",
    "    print(f\"   Target: Multi-company price levels (percentage from baseline)\")\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.Input(shape=input_shape))\n",
    "    \n",
    "    # RNN layers for multi-target learning\n",
    "    for i in range(num_rnn_layers):\n",
    "        return_sequences = (i < num_rnn_layers - 1) if num_rnn_layers > 1 else False\n",
    "        \n",
    "        model.add(SimpleRNN(\n",
    "            units=rnn_units,\n",
    "            return_sequences=return_sequences,\n",
    "            dropout=dropout_rate,\n",
    "            recurrent_dropout=dropout_rate * 0.8,  # Slightly lower recurrent dropout\n",
    "            activation='tanh',\n",
    "            name=f'multi_rnn_{i+1}'\n",
    "        ))\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        if dropout_rate > 0:\n",
    "            model.add(Dropout(dropout_rate, name=f'dropout_rnn_{i+1}'))\n",
    "    \n",
    "    # Dense layers for multi-target pattern recognition\n",
    "    if dense_units > 0:\n",
    "        # First dense layer with more capacity for multi-target\n",
    "        model.add(Dense(dense_units * 2, activation='relu', name='multi_dense_1'))\n",
    "        model.add(Dropout(dropout_rate * 0.6, name='dropout_dense_1'))\n",
    "        \n",
    "        # Second dense layer\n",
    "        model.add(Dense(dense_units, activation='relu', name='multi_dense_2'))\n",
    "        model.add(Dropout(dropout_rate * 0.4, name='dropout_dense_2'))\n",
    "    \n",
    "    # Multi-target output layer\n",
    "    model.add(Dense(num_targets, activation=activation, name='multi_target_output'))\n",
    "    \n",
    "    # Optimizer configuration\n",
    "    if optimizer == 'adam':\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-7)\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        opt = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n",
    "    \n",
    "    # Compile with multi-target appropriate metrics\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=loss,\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📋 Multi-Target Simple RNN Summary:\")\n",
    "    model.summary()\n",
    "    \n",
    "    print(f\"\\n⚙️ Multi-Target Model Configuration:\")\n",
    "    print(f\"   • Parameters: {model.count_params():,}\")\n",
    "    print(f\"   • Targets: {num_targets} companies simultaneously\")\n",
    "    print(f\"   • Optimizer: {optimizer} (lr={learning_rate})\")\n",
    "    print(f\"   • Focus: Multi-company price prediction\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_multi_target_simple_rnn(X_train_val, X_val, y_train_val, y_val, config, epochs=30):\n",
    "    \"\"\"\n",
    "    Evaluate Simple RNN configuration for multi-target price prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Create multi-target simple RNN model\n",
    "        model = create_multi_target_simple_rnn_model(\n",
    "            input_shape=(X_train_val.shape[1], X_train_val.shape[2]),\n",
    "            rnn_units=config['rnn_units'],\n",
    "            num_rnn_layers=config['num_rnn_layers'],\n",
    "            dropout_rate=config['dropout_rate'],\n",
    "            dense_units=config['dense_units'],\n",
    "            num_targets=y_train_val.shape[1],\n",
    "            activation='linear',\n",
    "            optimizer=config['optimizer'],\n",
    "            learning_rate=config['learning_rate'],\n",
    "            loss='mse'\n",
    "        )\n",
    "        \n",
    "        # Early stopping for multi-target training\n",
    "        early_stopping = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train_val, y_train_val,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=config['batch_size'],\n",
    "            callbacks=[early_stopping],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        train_loss = model.evaluate(X_train_val, y_train_val, verbose=0)[0]\n",
    "        val_loss = model.evaluate(X_val, y_val, verbose=0)[0]\n",
    "        \n",
    "        # Predictions for comprehensive metrics\n",
    "        y_pred_train = model.predict(X_train_val, verbose=0)\n",
    "        y_pred_val = model.predict(X_val, verbose=0)\n",
    "        \n",
    "        # Multi-target metrics calculation\n",
    "        train_mae = mean_absolute_error(y_train_val, y_pred_train)\n",
    "        val_mae = mean_absolute_error(y_val, y_pred_val)\n",
    "        train_r2 = r2_score(y_train_val, y_pred_train)\n",
    "        val_r2 = r2_score(y_val, y_pred_val)\n",
    "        \n",
    "        # Individual target correlations\n",
    "        train_correlations = []\n",
    "        val_correlations = []\n",
    "        \n",
    "        for i in range(y_train_val.shape[1]):\n",
    "            train_corr = np.corrcoef(y_train_val[:, i], y_pred_train[:, i])[0, 1]\n",
    "            val_corr = np.corrcoef(y_val[:, i], y_pred_val[:, i])[0, 1]\n",
    "            \n",
    "            train_correlations.append(train_corr if not np.isnan(train_corr) else 0)\n",
    "            val_correlations.append(val_corr if not np.isnan(val_corr) else 0)\n",
    "        \n",
    "        # Average correlations\n",
    "        avg_train_corr = np.mean(train_correlations)\n",
    "        avg_val_corr = np.mean(val_correlations)\n",
    "        \n",
    "        # Multi-target directional accuracy (average across targets)\n",
    "        train_direction_accs = []\n",
    "        val_direction_accs = []\n",
    "        \n",
    "        for i in range(y_train_val.shape[1]):\n",
    "            train_directions_actual = np.diff(y_train_val[:, i]) > 0\n",
    "            train_directions_pred = np.diff(y_pred_train[:, i]) > 0\n",
    "            train_direction_acc = np.mean(train_directions_actual == train_directions_pred) * 100\n",
    "            train_direction_accs.append(train_direction_acc)\n",
    "            \n",
    "            val_directions_actual = np.diff(y_val[:, i]) > 0\n",
    "            val_directions_pred = np.diff(y_pred_val[:, i]) > 0\n",
    "            val_direction_acc = np.mean(val_directions_actual == val_directions_pred) * 100\n",
    "            val_direction_accs.append(val_direction_acc)\n",
    "        \n",
    "        avg_train_direction_acc = np.mean(train_direction_accs)\n",
    "        avg_val_direction_acc = np.mean(val_direction_accs)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Multi-target performance score\n",
    "        multi_target_score = (val_r2 * 0.4 + avg_val_corr * 0.4 + (avg_val_direction_acc/100) * 0.2)\n",
    "        \n",
    "        results = {\n",
    "            'config': config.copy(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'train_mae': train_mae,\n",
    "            'val_mae': val_mae,\n",
    "            'train_r2': train_r2,\n",
    "            'val_r2': val_r2,\n",
    "            'train_correlation': avg_train_corr,\n",
    "            'val_correlation': avg_val_corr,\n",
    "            'individual_train_correlations': train_correlations,\n",
    "            'individual_val_correlations': val_correlations,\n",
    "            'train_direction_acc': avg_train_direction_acc,\n",
    "            'val_direction_acc': avg_val_direction_acc,\n",
    "            'individual_train_direction_accs': train_direction_accs,\n",
    "            'individual_val_direction_accs': val_direction_accs,\n",
    "            'overfitting_gap': abs(train_r2 - val_r2),\n",
    "            'training_time': training_time,\n",
    "            'epochs_trained': len(history.history['loss']),\n",
    "            'model': model,\n",
    "            'multi_target_score': multi_target_score\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'config': config.copy(),\n",
    "            'error': str(e),\n",
    "            'multi_target_score': -1\n",
    "        }\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MULTI-TARGET SIMPLE RNN HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create validation split for multi-target data\n",
    "val_split = 0.2\n",
    "split_idx = int(len(X_train_multi) * (1 - val_split))\n",
    "\n",
    "X_train_multi_val = X_train_multi[:split_idx]\n",
    "X_val_multi = X_train_multi[split_idx:]\n",
    "y_train_multi_val = y_train_multi[:split_idx]\n",
    "y_val_multi = y_train_multi[split_idx:]\n",
    "\n",
    "print(f\"📊 Multi-target data split:\")\n",
    "print(f\"   • Training: {X_train_multi_val.shape[0]} samples\")\n",
    "print(f\"   • Validation: {X_val_multi.shape[0]} samples\")\n",
    "print(f\"   • Test (isolated): {X_test_multi.shape[0]} samples\")\n",
    "print(f\"   • Targets per sample: {y_train_multi_val.shape[1]} companies\")\n",
    "\n",
    "# Multi-target Simple RNN configurations\n",
    "multi_target_simple_configs = [\n",
    "    # Config 1: Conservative approach for 4 targets\n",
    "    {\n",
    "        'rnn_units': 96, 'num_rnn_layers': 1, 'dropout_rate': 0.3,\n",
    "        'dense_units': 48, 'learning_rate': 0.0008, 'batch_size': 24, 'optimizer': 'adam'\n",
    "    },\n",
    "    \n",
    "    # Config 2: Higher capacity for multi-target complexity\n",
    "    {\n",
    "        'rnn_units': 128, 'num_rnn_layers': 1, 'dropout_rate': 0.35,\n",
    "        'dense_units': 64, 'learning_rate': 0.0006, 'batch_size': 20, 'optimizer': 'adam'\n",
    "    },\n",
    "    \n",
    "    # Config 3: Deep architecture for complex patterns\n",
    "    {\n",
    "        'rnn_units': 80, 'num_rnn_layers': 2, 'dropout_rate': 0.4,\n",
    "        'dense_units': 40, 'learning_rate': 0.0005, 'batch_size': 24, 'optimizer': 'adam'\n",
    "    },\n",
    "    \n",
    "    # Config 4: Balanced approach\n",
    "    {\n",
    "        'rnn_units': 112, 'num_rnn_layers': 1, 'dropout_rate': 0.25,\n",
    "        'dense_units': 56, 'learning_rate': 0.001, 'batch_size': 28, 'optimizer': 'adam'\n",
    "    },\n",
    "    \n",
    "    # Config 5: RMSprop alternative\n",
    "    {\n",
    "        'rnn_units': 96, 'num_rnn_layers': 1, 'dropout_rate': 0.3,\n",
    "        'dense_units': 48, 'learning_rate': 0.0008, 'batch_size': 24, 'optimizer': 'rmsprop'\n",
    "    },\n",
    "    \n",
    "    # Config 6: Lower learning rate, higher capacity\n",
    "    {\n",
    "        'rnn_units': 144, 'num_rnn_layers': 1, 'dropout_rate': 0.4,\n",
    "        'dense_units': 72, 'learning_rate': 0.0003, 'batch_size': 16, 'optimizer': 'adam'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\n🎯 Multi-Target Simple RNN Search Strategy:\")\n",
    "print(f\"   • {len(multi_target_simple_configs)} configurations optimized for 4-target prediction\")\n",
    "print(f\"   • Focus: Handling multi-target complexity and correlation\")\n",
    "print(f\"   • Higher model capacity to handle 4 simultaneous predictions\")\n",
    "\n",
    "# Evaluate all multi-target Simple RNN configurations\n",
    "multi_simple_results = []\n",
    "\n",
    "for i, config in enumerate(multi_target_simple_configs):\n",
    "    print(f\"\\n[{i+1}/{len(multi_target_simple_configs)}] Testing Multi-Target Simple RNN Config:\")\n",
    "    print(f\"   RNN Units: {config['rnn_units']}, Layers: {config['num_rnn_layers']}\")\n",
    "    print(f\"   Dropout: {config['dropout_rate']}, Dense: {config['dense_units']}\")\n",
    "    print(f\"   LR: {config['learning_rate']}, Batch: {config['batch_size']}\")\n",
    "    \n",
    "    result = evaluate_multi_target_simple_rnn(\n",
    "        X_train_multi_val, X_val_multi, y_train_multi_val, y_val_multi, config, epochs=35\n",
    "    )\n",
    "    \n",
    "    multi_simple_results.append(result)\n",
    "    \n",
    "    if 'error' not in result:\n",
    "        print(f\"   ✅ Val R²: {result['val_r2']:.4f}\")\n",
    "        print(f\"      Avg Val Correlation: {result['val_correlation']:.4f}\")\n",
    "        print(f\"      Multi-Target Score: {result['multi_target_score']:.4f}\")\n",
    "        print(f\"      Time: {result['training_time']:.1f}s\")\n",
    "    else:\n",
    "        print(f\"   ❌ Error: {result['error']}\")\n",
    "\n",
    "print(f\"\\n✅ Multi-target Simple RNN hyperparameter tuning completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYXngNUmMI4u"
   },
   "outputs": [],
   "source": [
    "# Find the best configuration\n",
    "\n",
    "successful_multi_simple_results = [r for r in multi_simple_results if 'error' not in r]\n",
    "\n",
    "if len(successful_multi_simple_results) == 0:\n",
    "    print(\"❌ No successful multi-target Simple RNN configurations!\")\n",
    "else:\n",
    "    print(f\"\\n✅ {len(successful_multi_simple_results)} successful configurations\")\n",
    "    \n",
    "    # Create detailed results analysis\n",
    "    multi_simple_results_df = pd.DataFrame([\n",
    "        {\n",
    "            'Config_ID': i+1,\n",
    "            'RNN_Units': r['config']['rnn_units'],\n",
    "            'RNN_Layers': r['config']['num_rnn_layers'],\n",
    "            'Dropout': r['config']['dropout_rate'],\n",
    "            'Dense_Units': r['config']['dense_units'],\n",
    "            'Learning_Rate': r['config']['learning_rate'],\n",
    "            'Batch_Size': r['config']['batch_size'],\n",
    "            'Optimizer': r['config']['optimizer'],\n",
    "            'Val_R2': r['val_r2'],\n",
    "            'Val_Correlation': r['val_correlation'],\n",
    "            'Val_Direction_Acc': r['val_direction_acc'],\n",
    "            'Overfitting_Gap': r['overfitting_gap'],\n",
    "            'Multi_Target_Score': r['multi_target_score'],\n",
    "            'Training_Time': r['training_time']\n",
    "        }\n",
    "        for i, r in enumerate(successful_multi_simple_results)\n",
    "    ])\n",
    "    \n",
    "    print(\"\\n📊 MULTI-TARGET SIMPLE RNN RESULTS:\")\n",
    "    print(\"=\"*100)\n",
    "    print(multi_simple_results_df.round(4))\n",
    "    \n",
    "    best_multi_simple_idx = multi_simple_results_df['Multi_Target_Score'].idxmax()\n",
    "    best_multi_simple_config = successful_multi_simple_results[best_multi_simple_idx]['config']\n",
    "    best_multi_simple_results = successful_multi_simple_results[best_multi_simple_idx]\n",
    "    \n",
    "    print(f\"\\n🏆 OPTIMAL MULTI-TARGET SIMPLE RNN CONFIGURATION:\")\n",
    "    print(\"=\"*60)\n",
    "    for key, value in best_multi_simple_config.items():\n",
    "        print(f\"   • {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Expected Multi-Target Performance:\")\n",
    "    print(f\"   • Overall Val R²: {best_multi_simple_results['val_r2']:.4f}\")\n",
    "    print(f\"   • Average Val Correlation: {best_multi_simple_results['val_correlation']:.4f}\")\n",
    "    print(f\"   • Multi-Target Score: {best_multi_simple_results['multi_target_score']:.4f}\")\n",
    "    \n",
    "    # Show individual company correlations\n",
    "    print(f\"\\n📊 Individual Company Correlations:\")\n",
    "    for i, target in enumerate(MULTI_TARGET_NAMES):\n",
    "        individual_corr = best_multi_simple_results['individual_val_correlations'][i]\n",
    "        print(f\"   • {target}: {individual_corr:.4f}\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-vyWykGBMI4y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an RNN model with a combination of potentially optimal hyperparameter values and retrain the model\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TRAINING FINAL MULTI-TARGET SIMPLE RNN MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"✅ Using optimal multi-target Simple RNN configuration:\")\n",
    "for key, value in best_multi_simple_config.items():\n",
    "    print(f\"• {key}: {value}\")\n",
    "\n",
    "# Clear session and create final multi-target model\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"BUILDING FINAL MULTI-TARGET SIMPLE RNN MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "final_multi_simple_model = create_multi_target_simple_rnn_model(\n",
    "    input_shape=(X_train_multi.shape[1], X_train_multi.shape[2]),\n",
    "    rnn_units=best_multi_simple_config['rnn_units'],\n",
    "    num_rnn_layers=best_multi_simple_config['num_rnn_layers'],\n",
    "    dropout_rate=best_multi_simple_config['dropout_rate'],\n",
    "    dense_units=best_multi_simple_config['dense_units'],\n",
    "    num_targets=len(MULTI_TARGET_NAMES),\n",
    "    activation='linear',\n",
    "    optimizer=best_multi_simple_config['optimizer'],\n",
    "    learning_rate=best_multi_simple_config['learning_rate'],\n",
    "    loss='mse'\n",
    ")\n",
    "\n",
    "# Train final multi-target model\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"FINAL TRAINING - MULTI-TARGET SIMPLE RNN\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "final_multi_early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=15,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Training optimal multi-target Simple RNN model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "final_multi_simple_history = final_multi_simple_model.fit(\n",
    "    X_train_multi, y_train_multi,\n",
    "    validation_data=(X_test_multi, y_test_multi),\n",
    "    epochs=60,\n",
    "    batch_size=best_multi_simple_config['batch_size'],\n",
    "    callbacks=[final_multi_early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "final_multi_simple_training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n📋 Final Multi-Target Simple RNN Training Summary:\")\n",
    "print(f\"   • Total epochs: {len(final_multi_simple_history.history['loss'])}\")\n",
    "print(f\"   • Training time: {final_multi_simple_training_time:.1f} seconds\")\n",
    "print(f\"   • Final train loss: {final_multi_simple_history.history['loss'][-1]:.6f}\")\n",
    "print(f\"   • Final val loss: {final_multi_simple_history.history['val_loss'][-1]:.6f}\")\n",
    "print(f\"   • Targets: {len(MULTI_TARGET_NAMES)} companies simultaneously\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4j63wDQkMcNB"
   },
   "outputs": [],
   "source": [
    "# Compute the performance of the model on the testing data set\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"MULTI-TARGET SIMPLE RNN PERFORMANCE EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate multi-target predictions\n",
    "print(\"🎯 Generating multi-target predictions...\")\n",
    "y_train_pred_multi_scaled = final_multi_simple_model.predict(X_train_multi, verbose=0)\n",
    "y_test_pred_multi_scaled = final_multi_simple_model.predict(X_test_multi, verbose=0)\n",
    "\n",
    "# Convert predictions back to actual prices for each target\n",
    "print(\"💱 Converting multi-target predictions back to actual prices...\")\n",
    "\n",
    "def convert_multi_target_predictions_to_prices(predictions_scaled, target_scaler, baseline_prices, target_names):\n",
    "    \"\"\"Convert multi-target scaled predictions back to actual prices\"\"\"\n",
    "    \n",
    "    # Inverse transform to get percentage changes from baseline\n",
    "    predictions_pct = target_scaler.inverse_transform(predictions_scaled)\n",
    "    \n",
    "    # Convert each target back to actual prices\n",
    "    predicted_prices = []\n",
    "    for i, target in enumerate(target_names):\n",
    "        baseline_price = baseline_prices[target]\n",
    "        # Price = Baseline * (1 + percentage_change)\n",
    "        prices = baseline_price * (1 + predictions_pct[:, i])\n",
    "        predicted_prices.append(prices)\n",
    "    \n",
    "    return np.column_stack(predicted_prices)\n",
    "\n",
    "y_train_pred_multi_prices = convert_multi_target_predictions_to_prices(\n",
    "    y_train_pred_multi_scaled, target_scaler_multi, \n",
    "    multi_target_dataset['baseline_prices'], MULTI_TARGET_NAMES\n",
    ")\n",
    "\n",
    "y_test_pred_multi_prices = convert_multi_target_predictions_to_prices(\n",
    "    y_test_pred_multi_scaled, target_scaler_multi, \n",
    "    multi_target_dataset['baseline_prices'], MULTI_TARGET_NAMES\n",
    ")\n",
    "\n",
    "# Convert actual targets back to prices\n",
    "y_train_actual_multi_prices = convert_multi_target_predictions_to_prices(\n",
    "    y_train_multi, target_scaler_multi, \n",
    "    multi_target_dataset['baseline_prices'], MULTI_TARGET_NAMES\n",
    ")\n",
    "\n",
    "y_test_actual_multi_prices = convert_multi_target_predictions_to_prices(\n",
    "    y_test_multi, target_scaler_multi, \n",
    "    multi_target_dataset['baseline_prices'], MULTI_TARGET_NAMES\n",
    ")\n",
    "\n",
    "# Calculate comprehensive multi-target metrics\n",
    "def calculate_multi_target_metrics(actual_scaled, pred_scaled, actual_prices, pred_prices, target_names):\n",
    "    \"\"\"Calculate comprehensive metrics for multi-target prediction\"\"\"\n",
    "    \n",
    "    overall_metrics = {}\n",
    "    individual_metrics = {}\n",
    "    \n",
    "    # Overall metrics\n",
    "    overall_metrics['Scaled_R2'] = r2_score(actual_scaled, pred_scaled)\n",
    "    overall_metrics['Scaled_MAE'] = mean_absolute_error(actual_scaled, pred_scaled)\n",
    "    overall_metrics['Price_R2'] = r2_score(actual_prices, pred_prices)\n",
    "    overall_metrics['Price_MAE'] = mean_absolute_error(actual_prices, pred_prices)\n",
    "    \n",
    "    # Individual target metrics\n",
    "    for i, target in enumerate(target_names):\n",
    "        target_metrics = {}\n",
    "        \n",
    "        target_metrics['Scaled_R2'] = r2_score(actual_scaled[:, i], pred_scaled[:, i])\n",
    "        target_metrics['Scaled_Correlation'] = np.corrcoef(actual_scaled[:, i], pred_scaled[:, i])[0, 1]\n",
    "        target_metrics['Price_R2'] = r2_score(actual_prices[:, i], pred_prices[:, i])\n",
    "        target_metrics['Price_Correlation'] = np.corrcoef(actual_prices[:, i], pred_prices[:, i])[0, 1]\n",
    "        target_metrics['Price_MAE'] = mean_absolute_error(actual_prices[:, i], pred_prices[:, i])\n",
    "        \n",
    "        # Handle NaN correlations\n",
    "        if np.isnan(target_metrics['Scaled_Correlation']):\n",
    "            target_metrics['Scaled_Correlation'] = 0\n",
    "        if np.isnan(target_metrics['Price_Correlation']):\n",
    "            target_metrics['Price_Correlation'] = 0\n",
    "        \n",
    "        # Directional accuracy\n",
    "        actual_directions = np.diff(actual_prices[:, i]) > 0\n",
    "        pred_directions = np.diff(pred_prices[:, i]) > 0\n",
    "        target_metrics['Direction_Accuracy'] = np.mean(actual_directions == pred_directions) * 100\n",
    "        \n",
    "        individual_metrics[target] = target_metrics\n",
    "    \n",
    "    return overall_metrics, individual_metrics\n",
    "\n",
    "# Calculate metrics\n",
    "train_overall_metrics, train_individual_metrics = calculate_multi_target_metrics(\n",
    "    y_train_multi, y_train_pred_multi_scaled,\n",
    "    y_train_actual_multi_prices, y_train_pred_multi_prices,\n",
    "    MULTI_TARGET_NAMES\n",
    ")\n",
    "\n",
    "test_overall_metrics, test_individual_metrics = calculate_multi_target_metrics(\n",
    "    y_test_multi, y_test_pred_multi_scaled,\n",
    "    y_test_actual_multi_prices, y_test_pred_multi_prices,\n",
    "    MULTI_TARGET_NAMES\n",
    ")\n",
    "\n",
    "# Display comprehensive results\n",
    "print(f\"\\n📊 MULTI-TARGET SIMPLE RNN PERFORMANCE SUMMARY:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nOverall Performance:\")\n",
    "print(f\"• Training - Scaled R²: {train_overall_metrics['Scaled_R2']:.4f}, Price R²: {train_overall_metrics['Price_R2']:.4f}\")\n",
    "print(f\"• Testing  - Scaled R²: {test_overall_metrics['Scaled_R2']:.4f}, Price R²: {test_overall_metrics['Price_R2']:.4f}\")\n",
    "print(f\"• Training - Price MAE: ${train_overall_metrics['Price_MAE']:.2f}\")\n",
    "print(f\"• Testing  - Price MAE: ${test_overall_metrics['Price_MAE']:.2f}\")\n",
    "\n",
    "print(f\"\\nIndividual Company Performance (Test Set):\")\n",
    "for target in MULTI_TARGET_NAMES:\n",
    "    metrics = test_individual_metrics[target]\n",
    "    print(f\"• {target}:\")\n",
    "    print(f\"  - Correlation: {metrics['Price_Correlation']:.4f}\")\n",
    "    print(f\"  - R²: {metrics['Price_R2']:.4f}\")\n",
    "    print(f\"  - MAE: ${metrics['Price_MAE']:.2f}\")\n",
    "    print(f\"  - Direction Acc: {metrics['Direction_Accuracy']:.1f}%\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QbM-hR6h-SV"
   },
   "outputs": [],
   "source": [
    "# Plotting the actual vs predicted values for all targets\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"MULTI-TARGET VISUALIZATION - ALL COMPANIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comprehensive multi-target visualization\n",
    "fig, axes = plt.subplots(3, 4, figsize=(24, 18))\n",
    "fig.suptitle('Multi-Target Simple RNN - All Companies Analysis', fontsize=18, fontweight='bold')\n",
    "\n",
    "companies = ['AMZN', 'GOOGL', 'IBM', 'MSFT']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "# Row 1: Actual vs Predicted scatter plots\n",
    "for i, (company, color) in enumerate(zip(companies, colors)):\n",
    "    actual_prices = y_test_actual_multi_prices[:, i]\n",
    "    pred_prices = y_test_pred_multi_prices[:, i]\n",
    "    \n",
    "    axes[0, i].scatter(actual_prices, pred_prices, alpha=0.6, s=30, color=color)\n",
    "    min_price, max_price = actual_prices.min(), actual_prices.max()\n",
    "    axes[0, i].plot([min_price, max_price], [min_price, max_price], 'k--', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    # Calculate correlation for display\n",
    "    correlation = test_individual_metrics[f'Close{company}']['Price_Correlation']\n",
    "    r2 = test_individual_metrics[f'Close{company}']['Price_R2']\n",
    "    \n",
    "    axes[0, i].set_title(f'{company}: Actual vs Predicted')\n",
    "    axes[0, i].set_xlabel('Actual Price ($)')\n",
    "    axes[0, i].set_ylabel('Predicted Price ($)')\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "    axes[0, i].text(0.05, 0.95, f'Corr: {correlation:.3f}\\nR²: {r2:.3f}', \n",
    "                   transform=axes[0, i].transAxes, bbox=dict(boxstyle=\"round\", facecolor='lightgray'))\n",
    "\n",
    "# Row 2: Time series plots (last 50 points)\n",
    "plot_points = min(50, len(y_test_actual_multi_prices))\n",
    "test_indices = range(len(y_test_actual_multi_prices) - plot_points, len(y_test_actual_multi_prices))\n",
    "\n",
    "for i, (company, color) in enumerate(zip(companies, colors)):\n",
    "    actual_prices = y_test_actual_multi_prices[-plot_points:, i]\n",
    "    pred_prices = y_test_pred_multi_prices[-plot_points:, i]\n",
    "    \n",
    "    axes[1, i].plot(test_indices, actual_prices, label='Actual', linewidth=2, alpha=0.8, color='black')\n",
    "    axes[1, i].plot(test_indices, pred_prices, label='Predicted', linewidth=2, alpha=0.8, color=color)\n",
    "    axes[1, i].fill_between(test_indices, actual_prices, pred_prices, alpha=0.3, color=color)\n",
    "    \n",
    "    axes[1, i].set_title(f'{company}: Time Series ({plot_points} points)')\n",
    "    axes[1, i].set_xlabel('Time Index')\n",
    "    axes[1, i].set_ylabel('Price ($)')\n",
    "    axes[1, i].legend()\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "# Row 3: Error distributions\n",
    "for i, (company, color) in enumerate(zip(companies, colors)):\n",
    "    actual_prices = y_test_actual_multi_prices[:, i]\n",
    "    pred_prices = y_test_pred_multi_prices[:, i]\n",
    "    errors = actual_prices - pred_prices\n",
    "    \n",
    "    axes[2, i].hist(errors, bins=20, alpha=0.7, color=color, edgecolor='black', density=True)\n",
    "    axes[2, i].axvline(x=0, color='red', linestyle='--', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    mae = test_individual_metrics[f'Close{company}']['Price_MAE']\n",
    "    axes[2, i].set_title(f'{company}: Error Distribution')\n",
    "    axes[2, i].set_xlabel('Prediction Error ($)')\n",
    "    axes[2, i].set_ylabel('Density')\n",
    "    axes[2, i].text(0.05, 0.95, f'MAE: ${mae:.2f}', \n",
    "                   transform=axes[2, i].transAxes, bbox=dict(boxstyle=\"round\", facecolor='lightgray'))\n",
    "    axes[2, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary comparison table\n",
    "print(f\"\\n📊 MULTI-TARGET PERFORMANCE SUMMARY TABLE:\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Company': companies,\n",
    "    'Correlation': [test_individual_metrics[f'Close{comp}']['Price_Correlation'] for comp in companies],\n",
    "    'R²': [test_individual_metrics[f'Close{comp}']['Price_R2'] for comp in companies],\n",
    "    'MAE ($)': [test_individual_metrics[f'Close{comp}']['Price_MAE'] for comp in companies],\n",
    "    'Direction Acc (%)': [test_individual_metrics[f'Close{comp}']['Direction_Accuracy'] for comp in companies]\n",
    "})\n",
    "\n",
    "print(summary_df.round(3))\n",
    "\n",
    "print(f\"\\n🎯 MULTI-TARGET SIMPLE RNN SUMMARY:\")\n",
    "print(f\"✅ Successfully predicts all 4 companies simultaneously\")\n",
    "print(f\"✅ Overall test R²: {test_overall_metrics['Scaled_R2']:.4f}\")\n",
    "print(f\"✅ Average correlation: {np.mean([test_individual_metrics[f'Close{comp}']['Price_Correlation'] for comp in companies]):.4f}\")\n",
    "print(f\"✅ Ready for Advanced GRU comparison!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnwOVGv5MjXi"
   },
   "source": [
    "#### **3.2.2**\n",
    "Perform hyperparameter tuning to find the optimal network configuration for Advanced RNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQRqZZbEIrh9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find an optimal configuration of advanced RNN\n",
    "\n",
    "def create_multi_target_advanced_gru_model(input_shape, \n",
    "                                         gru_units=64, \n",
    "                                         num_gru_layers=1,\n",
    "                                         dropout_rate=0.3,\n",
    "                                         recurrent_dropout_rate=0.2,\n",
    "                                         dense_units=32,\n",
    "                                         num_targets=4,\n",
    "                                         activation='linear',\n",
    "                                         optimizer='adam',\n",
    "                                         learning_rate=0.001,\n",
    "                                         loss='mse',\n",
    "                                         bidirectional=False,\n",
    "                                         batch_normalization=False,\n",
    "                                         gradient_clip_norm=1.0):\n",
    "    \"\"\"\n",
    "    Create an Advanced GRU model for multi-target price prediction (4 companies).\n",
    "    Enhanced with sophisticated regularization for multi-target learning.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"🚀 Building Multi-Target Advanced GRU Model...\")\n",
    "    print(f\"   Input shape: {input_shape}\")\n",
    "    print(f\"   Architecture: {num_gru_layers} GRU layers, {gru_units} units each\")\n",
    "    print(f\"   Output targets: {num_targets} companies\")\n",
    "    print(f\"   Bidirectional: {bidirectional}\")\n",
    "    print(f\"   Target: Multi-company advanced price levels\")\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(tf.keras.Input(shape=input_shape))\n",
    "    \n",
    "    # Advanced GRU layers for multi-target learning\n",
    "    for i in range(num_gru_layers):\n",
    "        return_sequences = (i < num_gru_layers - 1) if num_gru_layers > 1 else False\n",
    "        \n",
    "        if bidirectional and gru_units <= 128:  # Prevent excessive parameters for multi-target\n",
    "            gru_layer = tf.keras.layers.Bidirectional(\n",
    "                GRU(units=gru_units//2,  # Divide by 2 for bidirectional\n",
    "                    return_sequences=return_sequences,\n",
    "                    dropout=dropout_rate,\n",
    "                    recurrent_dropout=recurrent_dropout_rate,\n",
    "                    activation='tanh',\n",
    "                    recurrent_activation='sigmoid',\n",
    "                    kernel_regularizer=tf.keras.regularizers.l2(0.002),  # Slightly higher for multi-target\n",
    "                    recurrent_regularizer=tf.keras.regularizers.l2(0.002)),\n",
    "                name=f'multi_bidirectional_gru_{i+1}'\n",
    "            )\n",
    "        else:\n",
    "            gru_layer = GRU(\n",
    "                units=gru_units,\n",
    "                return_sequences=return_sequences,\n",
    "                dropout=dropout_rate,\n",
    "                recurrent_dropout=recurrent_dropout_rate,\n",
    "                activation='tanh',\n",
    "                recurrent_activation='sigmoid',\n",
    "                kernel_regularizer=tf.keras.regularizers.l2(0.002),\n",
    "                recurrent_regularizer=tf.keras.regularizers.l2(0.002),\n",
    "                name=f'multi_advanced_gru_{i+1}'\n",
    "            )\n",
    "        \n",
    "        model.add(gru_layer)\n",
    "        \n",
    "        # Batch normalization for multi-target stability\n",
    "        if batch_normalization:\n",
    "            model.add(tf.keras.layers.BatchNormalization(name=f'multi_bn_gru_{i+1}'))\n",
    "        \n",
    "        # Strategic dropout\n",
    "        if dropout_rate > 0:\n",
    "            model.add(Dropout(dropout_rate, name=f'multi_dropout_gru_{i+1}'))\n",
    "    \n",
    "    # Enhanced dense layers for multi-target pattern recognition\n",
    "    if dense_units > 0:\n",
    "        # First dense layer with higher capacity for 4 targets\n",
    "        model.add(Dense(dense_units * 3, activation='relu', \n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(0.003),\n",
    "                       name='multi_advanced_dense_1'))\n",
    "        \n",
    "        if batch_normalization:\n",
    "            model.add(tf.keras.layers.BatchNormalization(name='multi_bn_dense_1'))\n",
    "        \n",
    "        model.add(Dropout(dropout_rate * 0.8, name='multi_dropout_dense_1'))\n",
    "        \n",
    "        # Second dense layer\n",
    "        model.add(Dense(dense_units * 2, activation='relu',\n",
    "                       kernel_regularizer=tf.keras.regularizers.l2(0.003),\n",
    "                       name='multi_advanced_dense_2'))\n",
    "        model.add(Dropout(dropout_rate * 0.6, name='multi_dropout_dense_2'))\n",
    "        \n",
    "        # Third dense layer for complex multi-target patterns\n",
    "        if dense_units >= 32:\n",
    "            model.add(Dense(dense_units, activation='relu',\n",
    "                           kernel_regularizer=tf.keras.regularizers.l2(0.002),\n",
    "                           name='multi_advanced_dense_3'))\n",
    "            model.add(Dropout(dropout_rate * 0.4, name='multi_dropout_dense_3'))\n",
    "    \n",
    "    # Multi-target output layer with regularization\n",
    "    model.add(Dense(num_targets, activation=activation, \n",
    "                   kernel_regularizer=tf.keras.regularizers.l2(0.001),\n",
    "                   name='multi_target_advanced_output'))\n",
    "    \n",
    "    # Advanced optimizer with gradient clipping\n",
    "    if optimizer == 'adam':\n",
    "        opt = tf.keras.optimizers.Adam(\n",
    "            learning_rate=learning_rate,\n",
    "            beta_1=0.9,\n",
    "            beta_2=0.999,\n",
    "            epsilon=1e-7,\n",
    "            clipnorm=gradient_clip_norm\n",
    "        )\n",
    "    elif optimizer == 'rmsprop':\n",
    "        opt = tf.keras.optimizers.RMSprop(\n",
    "            learning_rate=learning_rate,\n",
    "            clipnorm=gradient_clip_norm\n",
    "        )\n",
    "    else:\n",
    "        opt = tf.keras.optimizers.SGD(\n",
    "            learning_rate=learning_rate,\n",
    "            momentum=0.9,\n",
    "            clipnorm=gradient_clip_norm\n",
    "        )\n",
    "    \n",
    "    # Compile with multi-target metrics\n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=loss,\n",
    "        metrics=['mae', 'mse']\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n📋 Multi-Target Advanced GRU Summary:\")\n",
    "    model.summary()\n",
    "    \n",
    "    print(f\"\\n⚙️ Multi-Target Advanced Model Configuration:\")\n",
    "    print(f\"   • Parameters: {model.count_params():,}\")\n",
    "    print(f\"   • Targets: {num_targets} companies simultaneously\")\n",
    "    print(f\"   • Regularization: L2 + Dropout + Gradient Clipping + BatchNorm\")\n",
    "    print(f\"   • Focus: Advanced multi-company price prediction\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_multi_target_advanced_gru(X_train_val, X_val, y_train_val, y_val, config, epochs=40):\n",
    "    \"\"\"\n",
    "    Evaluate Advanced GRU configuration for multi-target price prediction with overfitting prevention.\n",
    "    \"\"\"\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Create multi-target advanced GRU model\n",
    "        model = create_multi_target_advanced_gru_model(\n",
    "            input_shape=(X_train_val.shape[1], X_train_val.shape[2]),\n",
    "            gru_units=config['gru_units'],\n",
    "            num_gru_layers=config['num_gru_layers'],\n",
    "            dropout_rate=config['dropout_rate'],\n",
    "            recurrent_dropout_rate=config.get('recurrent_dropout_rate', 0.2),\n",
    "            dense_units=config['dense_units'],\n",
    "            num_targets=y_train_val.shape[1],\n",
    "            activation='linear',\n",
    "            optimizer=config['optimizer'],\n",
    "            learning_rate=config['learning_rate'],\n",
    "            loss='mse',\n",
    "            bidirectional=config.get('bidirectional', False),\n",
    "            batch_normalization=config.get('batch_normalization', False),\n",
    "            gradient_clip_norm=config.get('gradient_clip_norm', 1.0)\n",
    "        )\n",
    "        \n",
    "        # Advanced callbacks for multi-target training\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=15,  # More patience for multi-target\n",
    "                restore_best_weights=True,\n",
    "                verbose=0,\n",
    "                min_delta=0.0001\n",
    "            ),\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.7,\n",
    "                patience=10,\n",
    "                min_lr=1e-6,\n",
    "                verbose=0\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train model with comprehensive monitoring\n",
    "        history = model.fit(\n",
    "            X_train_val, y_train_val,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=config['batch_size'],\n",
    "            callbacks=callbacks,\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Comprehensive evaluation\n",
    "        train_loss = model.evaluate(X_train_val, y_train_val, verbose=0)[0]\n",
    "        val_loss = model.evaluate(X_val, y_val, verbose=0)[0]\n",
    "        \n",
    "        # Predictions for detailed metrics\n",
    "        y_pred_train = model.predict(X_train_val, verbose=0)\n",
    "        y_pred_val = model.predict(X_val, verbose=0)\n",
    "        \n",
    "        # Multi-target metrics calculation\n",
    "        train_mae = mean_absolute_error(y_train_val, y_pred_train)\n",
    "        val_mae = mean_absolute_error(y_val, y_pred_val)\n",
    "        train_r2 = r2_score(y_train_val, y_pred_train)\n",
    "        val_r2 = r2_score(y_val, y_pred_val)\n",
    "        \n",
    "        # Individual target correlations and metrics\n",
    "        train_correlations = []\n",
    "        val_correlations = []\n",
    "        train_individual_r2 = []\n",
    "        val_individual_r2 = []\n",
    "        \n",
    "        for i in range(y_train_val.shape[1]):\n",
    "            # Correlations\n",
    "            train_corr = np.corrcoef(y_train_val[:, i], y_pred_train[:, i])[0, 1]\n",
    "            val_corr = np.corrcoef(y_val[:, i], y_pred_val[:, i])[0, 1]\n",
    "            \n",
    "            train_correlations.append(train_corr if not np.isnan(train_corr) else 0)\n",
    "            val_correlations.append(val_corr if not np.isnan(val_corr) else 0)\n",
    "            \n",
    "            # Individual R² scores\n",
    "            train_r2_ind = r2_score(y_train_val[:, i], y_pred_train[:, i])\n",
    "            val_r2_ind = r2_score(y_val[:, i], y_pred_val[:, i])\n",
    "            \n",
    "            train_individual_r2.append(train_r2_ind)\n",
    "            val_individual_r2.append(val_r2_ind)\n",
    "        \n",
    "        # Average metrics\n",
    "        avg_train_corr = np.mean(train_correlations)\n",
    "        avg_val_corr = np.mean(val_correlations)\n",
    "        avg_train_r2 = np.mean(train_individual_r2)\n",
    "        avg_val_r2 = np.mean(val_individual_r2)\n",
    "        \n",
    "        # Multi-target directional accuracy\n",
    "        train_direction_accs = []\n",
    "        val_direction_accs = []\n",
    "        \n",
    "        for i in range(y_train_val.shape[1]):\n",
    "            train_directions_actual = np.diff(y_train_val[:, i]) > 0\n",
    "            train_directions_pred = np.diff(y_pred_train[:, i]) > 0\n",
    "            train_direction_acc = np.mean(train_directions_actual == train_directions_pred) * 100\n",
    "            train_direction_accs.append(train_direction_acc)\n",
    "            \n",
    "            val_directions_actual = np.diff(y_val[:, i]) > 0\n",
    "            val_directions_pred = np.diff(y_pred_val[:, i]) > 0\n",
    "            val_direction_acc = np.mean(val_directions_actual == val_directions_pred) * 100\n",
    "            val_direction_accs.append(val_direction_acc)\n",
    "        \n",
    "        avg_train_direction_acc = np.mean(train_direction_accs)\n",
    "        avg_val_direction_acc = np.mean(val_direction_accs)\n",
    "        \n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Advanced overfitting analysis for multi-target\n",
    "        overfitting_gap = abs(train_r2 - val_r2)\n",
    "        generalization_penalty = max(0, overfitting_gap - 0.3) * 0.6  # Higher tolerance for multi-target\n",
    "        \n",
    "        # Advanced multi-target GRU score with generalization focus\n",
    "        advanced_multi_score = (val_r2 * 0.3 + avg_val_corr * 0.3 + avg_val_r2 * 0.2 + \n",
    "                               (avg_val_direction_acc/100) * 0.1 + \n",
    "                               (1 - min(overfitting_gap/3, 1)) * 0.1 - generalization_penalty)\n",
    "        \n",
    "        results = {\n",
    "            'config': config.copy(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "            'train_mae': train_mae,\n",
    "            'val_mae': val_mae,\n",
    "            'train_r2': train_r2,\n",
    "            'val_r2': val_r2,\n",
    "            'train_correlation': avg_train_corr,\n",
    "            'val_correlation': avg_val_corr,\n",
    "            'individual_train_correlations': train_correlations,\n",
    "            'individual_val_correlations': val_correlations,\n",
    "            'individual_train_r2': train_individual_r2,\n",
    "            'individual_val_r2': val_individual_r2,\n",
    "            'train_direction_acc': avg_train_direction_acc,\n",
    "            'val_direction_acc': avg_val_direction_acc,\n",
    "            'individual_train_direction_accs': train_direction_accs,\n",
    "            'individual_val_direction_accs': val_direction_accs,\n",
    "            'overfitting_gap': overfitting_gap,\n",
    "            'generalization_penalty': generalization_penalty,\n",
    "            'training_time': training_time,\n",
    "            'epochs_trained': len(history.history['loss']),\n",
    "            'model': model,\n",
    "            'advanced_multi_score': advanced_multi_score,\n",
    "            'final_lr': float(model.optimizer.learning_rate.numpy())\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'config': config.copy(),\n",
    "            'error': str(e),\n",
    "            'advanced_multi_score': -1\n",
    "        }\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MULTI-TARGET ADVANCED GRU HYPERPARAMETER TUNING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"📊 Using multi-target validation split:\")\n",
    "print(f\"   • Training: {X_train_multi_val.shape[0]} samples\")\n",
    "print(f\"   • Validation: {X_val_multi.shape[0]} samples\")\n",
    "print(f\"   • Test (isolated): {X_test_multi.shape[0]} samples\")\n",
    "print(f\"   • Targets per sample: {y_train_multi_val.shape[1]} companies\")\n",
    "\n",
    "# Advanced GRU configurations optimized for multi-target prediction\n",
    "multi_target_advanced_configs = [\n",
    "    # Config 1: Conservative advanced approach\n",
    "    {\n",
    "        'gru_units': 96, 'num_gru_layers': 1, 'dropout_rate': 0.4,\n",
    "        'recurrent_dropout_rate': 0.3, 'dense_units': 48, 'learning_rate': 0.0006,\n",
    "        'batch_size': 20, 'optimizer': 'adam', 'bidirectional': False,\n",
    "        'batch_normalization': True, 'gradient_clip_norm': 0.8\n",
    "    },\n",
    "    \n",
    "    # Config 2: Bidirectional with strong regularization\n",
    "    {\n",
    "        'gru_units': 128, 'num_gru_layers': 1, 'dropout_rate': 0.45,\n",
    "        'recurrent_dropout_rate': 0.35, 'dense_units': 64, 'learning_rate': 0.0004,\n",
    "        'batch_size': 16, 'optimizer': 'adam', 'bidirectional': True,\n",
    "        'batch_normalization': True, 'gradient_clip_norm': 0.6\n",
    "    },\n",
    "    \n",
    "    # Config 3: Deep multi-target architecture\n",
    "    {\n",
    "        'gru_units': 80, 'num_gru_layers': 2, 'dropout_rate': 0.5,\n",
    "        'recurrent_dropout_rate': 0.4, 'dense_units': 40, 'learning_rate': 0.0003,\n",
    "        'batch_size': 18, 'optimizer': 'adam', 'bidirectional': False,\n",
    "        'batch_normalization': True, 'gradient_clip_norm': 0.5\n",
    "    },\n",
    "    \n",
    "    # Config 4: Balanced high-capacity approach\n",
    "    {\n",
    "        'gru_units': 112, 'num_gru_layers': 1, 'dropout_rate': 0.35,\n",
    "        'recurrent_dropout_rate': 0.25, 'dense_units': 56, 'learning_rate': 0.0008,\n",
    "        'batch_size': 22, 'optimizer': 'adam', 'bidirectional': False,\n",
    "        'batch_normalization': False, 'gradient_clip_norm': 1.0\n",
    "    },\n",
    "    \n",
    "    # Config 5: RMSprop with different learning dynamics\n",
    "    {\n",
    "        'gru_units': 88, 'num_gru_layers': 1, 'dropout_rate': 0.4,\n",
    "        'recurrent_dropout_rate': 0.3, 'dense_units': 44, 'learning_rate': 0.0006,\n",
    "        'batch_size': 20, 'optimizer': 'rmsprop', 'bidirectional': False,\n",
    "        'batch_normalization': True, 'gradient_clip_norm': 0.8\n",
    "    },\n",
    "    \n",
    "    # Config 6: Maximum regularization approach\n",
    "    {\n",
    "        'gru_units': 64, 'num_gru_layers': 1, 'dropout_rate': 0.55,\n",
    "        'recurrent_dropout_rate': 0.45, 'dense_units': 32, 'learning_rate': 0.001,\n",
    "        'batch_size': 16, 'optimizer': 'adam', 'bidirectional': False,\n",
    "        'batch_normalization': False, 'gradient_clip_norm': 0.4\n",
    "    },\n",
    "    \n",
    "    # Config 7: Bidirectional deep architecture\n",
    "    {\n",
    "        'gru_units': 72, 'num_gru_layers': 2, 'dropout_rate': 0.4,\n",
    "        'recurrent_dropout_rate': 0.3, 'dense_units': 36, 'learning_rate': 0.0004,\n",
    "        'batch_size': 20, 'optimizer': 'adam', 'bidirectional': True,\n",
    "        'batch_normalization': True, 'gradient_clip_norm': 0.7\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"\\n🎯 Multi-Target Advanced GRU Search Strategy:\")\n",
    "print(f\"   • {len(multi_target_advanced_configs)} specialized configurations\")\n",
    "print(f\"   • Focus: Multi-target generalization and overfitting prevention\")\n",
    "print(f\"   • Advanced regularization: Dropout + L2 + Gradient Clipping + BatchNorm\")\n",
    "print(f\"   • Bidirectional and deep architectures for 4-company prediction\")\n",
    "print(f\"   • Enhanced capacity for complex multi-target patterns\")\n",
    "\n",
    "# Evaluate all multi-target Advanced GRU configurations\n",
    "multi_advanced_results = []\n",
    "\n",
    "for i, config in enumerate(multi_target_advanced_configs):\n",
    "    print(f\"\\n[{i+1}/{len(multi_target_advanced_configs)}] Testing Multi-Target Advanced GRU Config:\")\n",
    "    print(f\"   GRU Units: {config['gru_units']}, Layers: {config['num_gru_layers']}\")\n",
    "    print(f\"   Dropout: {config['dropout_rate']}/{config['recurrent_dropout_rate']}\")\n",
    "    print(f\"   Bidirectional: {config['bidirectional']}, BatchNorm: {config['batch_normalization']}\")\n",
    "    print(f\"   LR: {config['learning_rate']}, Clip: {config['gradient_clip_norm']}\")\n",
    "    \n",
    "    result = evaluate_multi_target_advanced_gru(\n",
    "        X_train_multi_val, X_val_multi, y_train_multi_val, y_val_multi, config, epochs=45\n",
    "    )\n",
    "    \n",
    "    multi_advanced_results.append(result)\n",
    "    \n",
    "    if 'error' not in result:\n",
    "        print(f\"   ✅ Val R²: {result['val_r2']:.4f} | Overfitting Gap: {result['overfitting_gap']:.4f}\")\n",
    "        print(f\"      Avg Val Correlation: {result['val_correlation']:.4f}\")\n",
    "        print(f\"      Advanced Multi Score: {result['advanced_multi_score']:.4f}\")\n",
    "        print(f\"      Time: {result['training_time']:.1f}s\")\n",
    "    else:\n",
    "        print(f\"   ❌ Error: {result['error']}\")\n",
    "\n",
    "print(f\"\\n✅ Multi-target Advanced GRU hyperparameter tuning completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZswZ55JIrh-"
   },
   "outputs": [],
   "source": [
    "# Find the best configuration\n",
    "\n",
    "successful_multi_advanced_results = [r for r in multi_advanced_results if 'error' not in r]\n",
    "\n",
    "if len(successful_multi_advanced_results) == 0:\n",
    "    print(\"❌ No successful multi-target Advanced GRU configurations!\")\n",
    "else:\n",
    "    print(f\"\\n✅ {len(successful_multi_advanced_results)} successful Advanced GRU configurations\")\n",
    "    \n",
    "    # Create comprehensive results analysis\n",
    "    multi_advanced_results_df = pd.DataFrame([\n",
    "        {\n",
    "            'Config_ID': i+1,\n",
    "            'GRU_Units': r['config']['gru_units'],\n",
    "            'GRU_Layers': r['config']['num_gru_layers'],\n",
    "            'Dropout': r['config']['dropout_rate'],\n",
    "            'Recurrent_Dropout': r['config']['recurrent_dropout_rate'],\n",
    "            'Dense_Units': r['config']['dense_units'],\n",
    "            'Learning_Rate': r['config']['learning_rate'],\n",
    "            'Batch_Size': r['config']['batch_size'],\n",
    "            'Optimizer': r['config']['optimizer'],\n",
    "            'Bidirectional': r['config']['bidirectional'],\n",
    "            'BatchNorm': r['config']['batch_normalization'],\n",
    "            'Gradient_Clip': r['config']['gradient_clip_norm'],\n",
    "            'Val_R2': r['val_r2'],\n",
    "            'Val_Correlation': r['val_correlation'],\n",
    "            'Val_Direction_Acc': r['val_direction_acc'],\n",
    "            'Overfitting_Gap': r['overfitting_gap'],\n",
    "            'Advanced_Multi_Score': r['advanced_multi_score'],\n",
    "            'Training_Time': r['training_time'],\n",
    "            'Epochs': r['epochs_trained']\n",
    "        }\n",
    "        for i, r in enumerate(successful_multi_advanced_results)\n",
    "    ])\n",
    "    \n",
    "    print(\"\\n📊 MULTI-TARGET ADVANCED GRU RESULTS COMPARISON:\")\n",
    "    print(\"=\"*120)\n",
    "    print(multi_advanced_results_df.round(4))\n",
    "    \n",
    "    # Advanced analysis and selection\n",
    "    print(f\"\\n🏆 MULTI-TARGET ADVANCED GRU ANALYSIS:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Best by comprehensive score\n",
    "    best_advanced_multi_idx = multi_advanced_results_df['Advanced_Multi_Score'].idxmax()\n",
    "    best_advanced_multi_score = multi_advanced_results_df.loc[best_advanced_multi_idx]\n",
    "    \n",
    "    print(f\"\\n🎯 BEST BY ADVANCED MULTI-TARGET SCORE:\")\n",
    "    print(f\"   Config ID: {best_advanced_multi_score['Config_ID']}\")\n",
    "    print(f\"   Advanced Multi Score: {best_advanced_multi_score['Advanced_Multi_Score']:.4f}\")\n",
    "    print(f\"   Val R²: {best_advanced_multi_score['Val_R2']:.4f}\")\n",
    "    print(f\"   Val Correlation: {best_advanced_multi_score['Val_Correlation']:.4f}\")\n",
    "    print(f\"   Overfitting Gap: {best_advanced_multi_score['Overfitting_Gap']:.4f}\")\n",
    "    \n",
    "    # Best by validation R²\n",
    "    best_val_r2_multi = multi_advanced_results_df.loc[multi_advanced_results_df['Val_R2'].idxmax()]\n",
    "    print(f\"\\n📈 BEST BY VALIDATION R²:\")\n",
    "    print(f\"   Config ID: {best_val_r2_multi['Config_ID']}\")\n",
    "    print(f\"   Val R²: {best_val_r2_multi['Val_R2']:.4f}\")\n",
    "    print(f\"   Overfitting Gap: {best_val_r2_multi['Overfitting_Gap']:.4f}\")\n",
    "    \n",
    "    # Best by correlation\n",
    "    best_corr_multi = multi_advanced_results_df.loc[multi_advanced_results_df['Val_Correlation'].idxmax()]\n",
    "    print(f\"\\n🔗 BEST BY CORRELATION:\")\n",
    "    print(f\"   Config ID: {best_corr_multi['Config_ID']}\")\n",
    "    print(f\"   Val Correlation: {best_corr_multi['Val_Correlation']:.4f}\")\n",
    "    print(f\"   Val R²: {best_corr_multi['Val_R2']:.4f}\")\n",
    "    \n",
    "    # Best generalization (lowest overfitting)\n",
    "    best_generalization_multi = multi_advanced_results_df.loc[multi_advanced_results_df['Overfitting_Gap'].idxmin()]\n",
    "    print(f\"\\n⚖️ BEST GENERALIZATION:\")\n",
    "    print(f\"   Config ID: {best_generalization_multi['Config_ID']}\")\n",
    "    print(f\"   Overfitting Gap: {best_generalization_multi['Overfitting_Gap']:.4f}\")\n",
    "    print(f\"   Val R²: {best_generalization_multi['Val_R2']:.4f}\")\n",
    "    \n",
    "    # Performance statistics\n",
    "    print(f\"\\n📊 PERFORMANCE STATISTICS:\")\n",
    "    print(f\"   Val R² range: [{multi_advanced_results_df['Val_R2'].min():.4f}, {multi_advanced_results_df['Val_R2'].max():.4f}]\")\n",
    "    print(f\"   Val Correlation range: [{multi_advanced_results_df['Val_Correlation'].min():.4f}, {multi_advanced_results_df['Val_Correlation'].max():.4f}]\")\n",
    "    print(f\"   Overfitting Gap range: [{multi_advanced_results_df['Overfitting_Gap'].min():.4f}, {multi_advanced_results_df['Overfitting_Gap'].max():.4f}]\")\n",
    "    print(f\"   Training Time range: [{multi_advanced_results_df['Training_Time'].min():.1f}s, {multi_advanced_results_df['Training_Time'].max():.1f}s]\")\n",
    "    \n",
    "    # Count high-performing models\n",
    "    high_r2_models = multi_advanced_results_df[multi_advanced_results_df['Val_R2'] > 0.3]\n",
    "    high_corr_models = multi_advanced_results_df[multi_advanced_results_df['Val_Correlation'] > 0.5]\n",
    "    low_overfitting_models = multi_advanced_results_df[multi_advanced_results_df['Overfitting_Gap'] < 0.5]\n",
    "    \n",
    "    print(f\"\\n🎯 MODEL QUALITY ANALYSIS:\")\n",
    "    print(f\"   High R² models (>0.3): {len(high_r2_models)}/{len(multi_advanced_results_df)}\")\n",
    "    print(f\"   High correlation models (>0.5): {len(high_corr_models)}/{len(multi_advanced_results_df)}\")\n",
    "    print(f\"   Low overfitting models (<0.5 gap): {len(low_overfitting_models)}/{len(multi_advanced_results_df)}\")\n",
    "    \n",
    "    # Select final optimal configuration\n",
    "    final_advanced_multi_idx = int(best_advanced_multi_score['Config_ID']) - 1\n",
    "    best_advanced_multi_config = successful_multi_advanced_results[final_advanced_multi_idx]['config']\n",
    "    best_advanced_multi_results = successful_multi_advanced_results[final_advanced_multi_idx]\n",
    "    \n",
    "    print(f\"\\n✅ SELECTED OPTIMAL MULTI-TARGET ADVANCED GRU CONFIGURATION:\")\n",
    "    print(\"=\"*70)\n",
    "    for key, value in best_advanced_multi_config.items():\n",
    "        print(f\"   • {key}: {value}\")\n",
    "    \n",
    "    print(f\"\\n🎯 Expected Multi-Target Advanced Performance:\")\n",
    "    print(f\"   • Overall Val R²: {best_advanced_multi_results['val_r2']:.4f}\")\n",
    "    print(f\"   • Average Val Correlation: {best_advanced_multi_results['val_correlation']:.4f}\")\n",
    "    print(f\"   • Advanced Multi Score: {best_advanced_multi_results['advanced_multi_score']:.4f}\")\n",
    "    print(f\"   • Overfitting Gap: {best_advanced_multi_results['overfitting_gap']:.4f}\")\n",
    "    print(f\"   • Direction Accuracy: {best_advanced_multi_results['val_direction_acc']:.1f}%\")\n",
    "    \n",
    "    # Show individual company performance expectations\n",
    "    print(f\"\\n📊 Expected Individual Company Performance:\")\n",
    "    for i, target in enumerate(MULTI_TARGET_NAMES):\n",
    "        individual_corr = best_advanced_multi_results['individual_val_correlations'][i]\n",
    "        individual_r2 = best_advanced_multi_results['individual_val_r2'][i]\n",
    "        individual_direction = best_advanced_multi_results['individual_val_direction_accs'][i]\n",
    "        print(f\"   • {target}: Corr={individual_corr:.4f}, R²={individual_r2:.4f}, Dir={individual_direction:.1f}%\")\n",
    "    \n",
    "    # Comparison with Simple RNN (if available)\n",
    "    if 'best_multi_simple_results' in locals():\n",
    "        print(f\"\\n🔄 IMPROVEMENT OVER SIMPLE RNN:\")\n",
    "        r2_improvement = best_advanced_multi_results['val_r2'] - best_multi_simple_results['val_r2']\n",
    "        corr_improvement = best_advanced_multi_results['val_correlation'] - best_multi_simple_results['val_correlation']\n",
    "        print(f\"   • R² improvement: {r2_improvement:.4f}\")\n",
    "        print(f\"   • Correlation improvement: {corr_improvement:.4f}\")\n",
    "        print(f\"   • Overfitting reduction: {best_multi_simple_results.get('overfitting_gap', 0) - best_advanced_multi_results['overfitting_gap']:.4f}\")\n",
    "    \n",
    "    # Visualization of results\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('Multi-Target Advanced GRU Hyperparameter Tuning Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Advanced Multi Score vs Configuration\n",
    "    axes[0,0].bar(multi_advanced_results_df['Config_ID'], multi_advanced_results_df['Advanced_Multi_Score'], \n",
    "                  alpha=0.7, color='green')\n",
    "    axes[0,0].set_title('Advanced Multi-Target Score by Configuration')\n",
    "    axes[0,0].set_xlabel('Configuration ID')\n",
    "    axes[0,0].set_ylabel('Advanced Multi-Target Score')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: R² vs Correlation\n",
    "    scatter = axes[0,1].scatter(multi_advanced_results_df['Val_R2'], multi_advanced_results_df['Val_Correlation'],\n",
    "                               c=multi_advanced_results_df['Config_ID'], alpha=0.7, s=100, cmap='viridis')\n",
    "    axes[0,1].set_title('Validation R² vs Correlation')\n",
    "    axes[0,1].set_xlabel('Validation R²')\n",
    "    axes[0,1].set_ylabel('Validation Correlation')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=axes[0,1], label='Config ID')\n",
    "    \n",
    "    # Plot 3: Overfitting Analysis\n",
    "    axes[1,0].bar(multi_advanced_results_df['Config_ID'], multi_advanced_results_df['Overfitting_Gap'],\n",
    "                  alpha=0.7, color='orange')\n",
    "    axes[1,0].set_title('Overfitting Gap by Configuration')\n",
    "    axes[1,0].set_xlabel('Configuration ID')\n",
    "    axes[1,0].set_ylabel('Overfitting Gap (Train R² - Val R²)')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    axes[1,0].axhline(y=0.5, color='red', linestyle='--', label='Acceptable Gap (0.5)')\n",
    "    axes[1,0].legend()\n",
    "    \n",
    "    # Plot 4: Training Efficiency\n",
    "    axes[1,1].scatter(multi_advanced_results_df['Training_Time'], multi_advanced_results_df['Advanced_Multi_Score'],\n",
    "                     c=multi_advanced_results_df['GRU_Units'], alpha=0.7, s=100, cmap='plasma')\n",
    "    axes[1,1].set_title('Training Efficiency vs Performance')\n",
    "    axes[1,1].set_xlabel('Training Time (seconds)')\n",
    "    axes[1,1].set_ylabel('Advanced Multi-Target Score')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n🚀 Ready to train final optimal multi-target Advanced GRU model!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Fh-2tBTNWXI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a model with a combination of potentially optimal hyperparameter values and retrain the model\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"TRAINING FINAL OPTIMAL MULTI-TARGET ADVANCED GRU MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"✅ Using optimal multi-target Advanced GRU configuration:\")\n",
    "for key, value in best_advanced_multi_config.items():\n",
    "    print(f\"• {key}: {value}\")\n",
    "\n",
    "# Clear session and create final multi-target Advanced GRU model\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BUILDING FINAL MULTI-TARGET ADVANCED GRU MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_multi_advanced_model = create_multi_target_advanced_gru_model(\n",
    "    input_shape=(X_train_multi.shape[1], X_train_multi.shape[2]),\n",
    "    gru_units=best_advanced_multi_config['gru_units'],\n",
    "    num_gru_layers=best_advanced_multi_config['num_gru_layers'],\n",
    "    dropout_rate=best_advanced_multi_config['dropout_rate'],\n",
    "    recurrent_dropout_rate=best_advanced_multi_config.get('recurrent_dropout_rate', 0.2),\n",
    "    dense_units=best_advanced_multi_config['dense_units'],\n",
    "    num_targets=len(MULTI_TARGET_NAMES),\n",
    "    activation='linear',\n",
    "    optimizer=best_advanced_multi_config['optimizer'],\n",
    "    learning_rate=best_advanced_multi_config['learning_rate'],\n",
    "    loss='mse',\n",
    "    bidirectional=best_advanced_multi_config.get('bidirectional', False),\n",
    "    batch_normalization=best_advanced_multi_config.get('batch_normalization', False),\n",
    "    gradient_clip_norm=best_advanced_multi_config.get('gradient_clip_norm', 1.0)\n",
    ")\n",
    "\n",
    "# Enhanced training setup for final model\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"FINAL TRAINING - MULTI-TARGET ADVANCED GRU\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Comprehensive callbacks for final training\n",
    "final_multi_advanced_callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=25,  # More patience for final training\n",
    "        restore_best_weights=True,\n",
    "        verbose=1,\n",
    "        min_delta=0.0001\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.6,\n",
    "        patience=15,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_multi_advanced_gru_model.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=0\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Training optimal multi-target Advanced GRU model...\")\n",
    "print(f\"Configuration: {best_advanced_multi_config['gru_units']} units, {best_advanced_multi_config['num_gru_layers']} layers\")\n",
    "print(f\"Regularization: Dropout={best_advanced_multi_config['dropout_rate']}, L2, Gradient Clipping\")\n",
    "print(f\"Advanced features: Bidirectional={best_advanced_multi_config.get('bidirectional', False)}, BatchNorm={best_advanced_multi_config.get('batch_normalization', False)}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "final_multi_advanced_history = final_multi_advanced_model.fit(\n",
    "    X_train_multi, y_train_multi,\n",
    "    validation_data=(X_test_multi, y_test_multi),\n",
    "    epochs=100,  # More epochs with early stopping\n",
    "    batch_size=best_advanced_multi_config['batch_size'],\n",
    "    callbacks=final_multi_advanced_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "final_multi_advanced_training_time = time.time() - start_time\n",
    "\n",
    "# Comprehensive training visualization\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 15))\n",
    "fig.suptitle('Final Multi-Target Advanced GRU - Comprehensive Training Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Loss curves\n",
    "axes[0,0].plot(final_multi_advanced_history.history['loss'], label='Training Loss', linewidth=2, color='blue')\n",
    "axes[0,0].plot(final_multi_advanced_history.history['val_loss'], label='Validation Loss', linewidth=2, color='red')\n",
    "axes[0,0].set_title('Training and Validation Loss')\n",
    "axes[0,0].set_xlabel('Epoch')\n",
    "axes[0,0].set_ylabel('Loss (MSE)')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE curves\n",
    "axes[0,1].plot(final_multi_advanced_history.history['mae'], label='Training MAE', linewidth=2, color='blue')\n",
    "axes[0,1].plot(final_multi_advanced_history.history['val_mae'], label='Validation MAE', linewidth=2, color='red')\n",
    "axes[0,1].set_title('Training and Validation MAE')\n",
    "axes[0,1].set_xlabel('Epoch')\n",
    "axes[0,1].set_ylabel('Mean Absolute Error')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate schedule (if available)\n",
    "if hasattr(final_multi_advanced_model.optimizer, 'learning_rate'):\n",
    "    lr_values = [float(final_multi_advanced_model.optimizer.learning_rate.numpy())] * len(final_multi_advanced_history.history['loss'])\n",
    "    axes[1,0].plot(lr_values, label='Learning Rate', linewidth=2, color='green')\n",
    "    axes[1,0].set_title('Learning Rate Schedule')\n",
    "    axes[1,0].set_xlabel('Epoch')\n",
    "    axes[1,0].set_ylabel('Learning Rate')\n",
    "    axes[1,0].set_yscale('log')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Overfitting analysis\n",
    "axes[1,1].plot(final_multi_advanced_history.history['loss'], alpha=0.7, label='Training', color='blue')\n",
    "axes[1,1].plot(final_multi_advanced_history.history['val_loss'], alpha=0.7, label='Validation', color='red')\n",
    "axes[1,1].set_yscale('log')\n",
    "axes[1,1].set_title('Advanced GRU Overfitting Analysis (Log Scale)')\n",
    "axes[1,1].set_xlabel('Epoch')\n",
    "axes[1,1].set_ylabel('Loss (Log Scale)')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Generalization indicator\n",
    "loss_ratio = np.array(final_multi_advanced_history.history['val_loss']) / np.array(final_multi_advanced_history.history['loss'])\n",
    "axes[2,0].plot(loss_ratio, linewidth=2, color='purple')\n",
    "axes[2,0].axhline(y=1.0, color='green', linestyle='--', label='Perfect Generalization')\n",
    "axes[2,0].axhline(y=1.5, color='orange', linestyle='--', label='Acceptable Range')\n",
    "axes[2,0].axhline(y=2.0, color='red', linestyle='--', label='Overfitting Warning')\n",
    "axes[2,0].set_title('Multi-Target Generalization (Val/Train Loss)')\n",
    "axes[2,0].set_xlabel('Epoch')\n",
    "axes[2,0].set_ylabel('Loss Ratio')\n",
    "axes[2,0].legend()\n",
    "axes[2,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Training progress efficiency\n",
    "epoch_numbers = range(1, len(final_multi_advanced_history.history['loss']) + 1)\n",
    "axes[2,1].plot(epoch_numbers, final_multi_advanced_history.history['val_loss'], 'o-', alpha=0.6, color='red', markersize=3)\n",
    "axes[2,1].set_title('Validation Loss Progress')\n",
    "axes[2,1].set_xlabel('Epoch')\n",
    "axes[2,1].set_ylabel('Validation Loss')\n",
    "axes[2,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📋 Final Multi-Target Advanced GRU Training Summary:\")\n",
    "print(f\"   • Total epochs trained: {len(final_multi_advanced_history.history['loss'])}\")\n",
    "print(f\"   • Total training time: {final_multi_advanced_training_time:.1f} seconds\")\n",
    "print(f\"   • Final training loss: {final_multi_advanced_history.history['loss'][-1]:.6f}\")\n",
    "print(f\"   • Final validation loss: {final_multi_advanced_history.history['val_loss'][-1]:.6f}\")\n",
    "print(f\"   • Model parameters: {final_multi_advanced_model.count_params():,}\")\n",
    "print(f\"   • Target companies: {len(MULTI_TARGET_NAMES)} (simultaneous prediction)\")\n",
    "print(f\"   • Architecture: Advanced GRU with sophisticated regularization\")\n",
    "print(f\"   • Best epoch: {np.argmin(final_multi_advanced_history.history['val_loss']) + 1}\")\n",
    "\n",
    "# Training efficiency analysis\n",
    "best_epoch = np.argmin(final_multi_advanced_history.history['val_loss']) + 1\n",
    "best_val_loss = np.min(final_multi_advanced_history.history['val_loss'])\n",
    "improvement_ratio = (final_multi_advanced_history.history['val_loss'][0] - best_val_loss) / final_multi_advanced_history.history['val_loss'][0]\n",
    "\n",
    "print(f\"\\n🎯 Training Efficiency Analysis:\")\n",
    "print(f\"   • Best validation loss: {best_val_loss:.6f} (epoch {best_epoch})\")\n",
    "print(f\"   • Loss improvement: {improvement_ratio*100:.1f}% from initial\")\n",
    "print(f\"   • Average time per epoch: {final_multi_advanced_training_time/len(final_multi_advanced_history.history['loss']):.1f}s\")\n",
    "print(f\"   • Early stopping triggered: {'Yes' if len(final_multi_advanced_history.history['loss']) < 100 else 'No'}\")\n",
    "\n",
    "print(f\"\\n✅ Final multi-target Advanced GRU model training completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_y7m-nziNWXK"
   },
   "outputs": [],
   "source": [
    "# Compute the performance of the model on the testing data set\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COMPREHENSIVE MULTI-TARGET ADVANCED GRU PERFORMANCE EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Generate comprehensive predictions\n",
    "print(\"🎯 Generating multi-target Advanced GRU predictions...\")\n",
    "y_train_pred_advanced_scaled = final_multi_advanced_model.predict(X_train_multi, verbose=0)\n",
    "y_test_pred_advanced_scaled = final_multi_advanced_model.predict(X_test_multi, verbose=0)\n",
    "\n",
    "# Convert predictions back to actual prices\n",
    "print(\"💱 Converting Advanced GRU predictions back to actual prices...\")\n",
    "\n",
    "y_train_pred_advanced_prices = convert_multi_target_predictions_to_prices(\n",
    "    y_train_pred_advanced_scaled, target_scaler_multi, \n",
    "    multi_target_dataset['baseline_prices'], MULTI_TARGET_NAMES\n",
    ")\n",
    "\n",
    "y_test_pred_advanced_prices = convert_multi_target_predictions_to_prices(\n",
    "    y_test_pred_advanced_scaled, target_scaler_multi, \n",
    "    multi_target_dataset['baseline_prices'], MULTI_TARGET_NAMES\n",
    ")\n",
    "\n",
    "print(f\"✅ Advanced GRU price conversion completed!\")\n",
    "print(f\"   📊 Advanced GRU predicted test range: ${y_test_pred_advanced_prices.min():.2f} - ${y_test_pred_advanced_prices.max():.2f}\")\n",
    "\n",
    "# Calculate comprehensive Advanced GRU metrics\n",
    "train_metrics_advanced = calculate_multi_target_metrics(\n",
    "    y_train_multi, y_train_pred_advanced_scaled, \n",
    "    y_train_actual_multi_prices, y_train_pred_advanced_prices, \n",
    "    MULTI_TARGET_NAMES\n",
    ")[1]  # Get individual metrics\n",
    "\n",
    "test_metrics_advanced = calculate_multi_target_metrics(\n",
    "    y_test_multi, y_test_pred_advanced_scaled,\n",
    "    y_test_actual_multi_prices, y_test_pred_advanced_prices,\n",
    "    MULTI_TARGET_NAMES\n",
    ")[1]  # Get individual metrics\n",
    "\n",
    "# Calculate overall metrics too\n",
    "train_overall_advanced, _ = calculate_multi_target_metrics(\n",
    "    y_train_multi, y_train_pred_advanced_scaled, \n",
    "    y_train_actual_multi_prices, y_train_pred_advanced_prices, \n",
    "    MULTI_TARGET_NAMES\n",
    ")\n",
    "\n",
    "test_overall_advanced, _ = calculate_multi_target_metrics(\n",
    "    y_test_multi, y_test_pred_advanced_scaled,\n",
    "    y_test_actual_multi_prices, y_test_pred_advanced_prices,\n",
    "    MULTI_TARGET_NAMES\n",
    ")\n",
    "\n",
    "# Comprehensive model comparison\n",
    "print(f\"\\n📊 ADVANCED GRU vs SIMPLE RNN - MULTI-TARGET COMPARISON\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Create comprehensive comparison DataFrame\n",
    "if 'test_individual_metrics' in locals():  # Simple RNN results available\n",
    "    comparison_data = []\n",
    "    \n",
    "    for target in MULTI_TARGET_NAMES:\n",
    "        simple_metrics = test_individual_metrics[target]\n",
    "        advanced_metrics = test_metrics_advanced[target]\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Company': target.replace('Close', ''),\n",
    "            'Simple_RNN_Corr': simple_metrics['Price_Correlation'],\n",
    "            'Advanced_GRU_Corr': advanced_metrics['Price_Correlation'],\n",
    "            'Corr_Improvement': advanced_metrics['Price_Correlation'] - simple_metrics['Price_Correlation'],\n",
    "            'Simple_RNN_R2': simple_metrics['Price_R2'],\n",
    "            'Advanced_GRU_R2': advanced_metrics['Price_R2'],\n",
    "            'R2_Improvement': advanced_metrics['Price_R2'] - simple_metrics['Price_R2'],\n",
    "            'Simple_RNN_MAE': simple_metrics['Price_MAE'],\n",
    "            'Advanced_GRU_MAE': advanced_metrics['Price_MAE'],\n",
    "            'MAE_Improvement': simple_metrics['Price_MAE'] - advanced_metrics['Price_MAE'],\n",
    "            'Simple_RNN_Direction': simple_metrics['Direction_Accuracy'],\n",
    "            'Advanced_GRU_Direction': advanced_metrics['Direction_Accuracy'],\n",
    "            'Direction_Improvement': advanced_metrics['Direction_Accuracy'] - simple_metrics['Direction_Accuracy']\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"\\nDETAILED COMPARISON BY COMPANY:\")\n",
    "    print(\"=\"*120)\n",
    "    print(comparison_df.round(4))\n",
    "    \n",
    "    # Summary improvements\n",
    "    print(f\"\\n🎯 OVERALL IMPROVEMENT SUMMARY:\")\n",
    "    print(f\"   • Average Correlation Improvement: {comparison_df['Corr_Improvement'].mean():.4f}\")\n",
    "    print(f\"   • Average R² Improvement: {comparison_df['R2_Improvement'].mean():.4f}\")\n",
    "    print(f\"   • Average MAE Improvement: ${comparison_df['MAE_Improvement'].mean():.2f}\")\n",
    "    print(f\"   • Average Direction Improvement: {comparison_df['Direction_Improvement'].mean():.1f}%\")\n",
    "    \n",
    "    # Count improvements\n",
    "    corr_improvements = (comparison_df['Corr_Improvement'] > 0).sum()\n",
    "    r2_improvements = (comparison_df['R2_Improvement'] > 0).sum()\n",
    "    mae_improvements = (comparison_df['MAE_Improvement'] > 0).sum()\n",
    "    direction_improvements = (comparison_df['Direction_Improvement'] > 0).sum()\n",
    "    \n",
    "    print(f\"\\n📈 IMPROVEMENT STATISTICS:\")\n",
    "    print(f\"   • Companies with better correlation: {corr_improvements}/{len(MULTI_TARGET_NAMES)}\")\n",
    "    print(f\"   • Companies with better R²: {r2_improvements}/{len(MULTI_TARGET_NAMES)}\")\n",
    "    print(f\"   • Companies with lower MAE: {mae_improvements}/{len(MULTI_TARGET_NAMES)}\")\n",
    "    print(f\"   • Companies with better direction accuracy: {direction_improvements}/{len(MULTI_TARGET_NAMES)}\")\n",
    "\n",
    "# Advanced GRU detailed performance analysis\n",
    "print(f\"\\n📊 MULTI-TARGET ADVANCED GRU DETAILED PERFORMANCE:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nOverall Performance (Advanced GRU):\")\n",
    "print(f\"• Training - Scaled R²: {train_overall_advanced['Scaled_R2']:.4f}, Price R²: {train_overall_advanced['Price_R2']:.4f}\")\n",
    "print(f\"• Testing  - Scaled R²: {test_overall_advanced['Scaled_R2']:.4f}, Price R²: {test_overall_advanced['Price_R2']:.4f}\")\n",
    "print(f\"• Training - Price MAE: ${train_overall_advanced['Price_MAE']:.2f}\")\n",
    "print(f\"• Testing  - Price MAE: ${test_overall_advanced['Price_MAE']:.2f}\")\n",
    "\n",
    "print(f\"\\nIndividual Company Performance (Advanced GRU Test Set):\")\n",
    "advanced_individual_summary = []\n",
    "for target in MULTI_TARGET_NAMES:\n",
    "    metrics = test_metrics_advanced[target]\n",
    "    company = target.replace('Close', '')\n",
    "    print(f\"• {company}:\")\n",
    "    print(f\"  - Correlation: {metrics['Price_Correlation']:.4f}\")\n",
    "    print(f\"  - R²: {metrics['Price_R2']:.4f}\")\n",
    "    print(f\"  - MAE: ${metrics['Price_MAE']:.2f}\")\n",
    "    print(f\"  - Direction Acc: {metrics['Direction_Accuracy']:.1f}%\")\n",
    "    \n",
    "    advanced_individual_summary.append({\n",
    "        'Company': company,\n",
    "        'Correlation': metrics['Price_Correlation'],\n",
    "        'R2': metrics['Price_R2'],\n",
    "        'MAE': metrics['Price_MAE'],\n",
    "        'Direction_Acc': metrics['Direction_Accuracy']\n",
    "    })\n",
    "\n",
    "# Generalization analysis\n",
    "train_overall_r2 = train_overall_advanced['Scaled_R2']\n",
    "test_overall_r2 = test_overall_advanced['Scaled_R2']\n",
    "overfitting_gap_advanced = abs(train_overall_r2 - test_overall_r2)\n",
    "\n",
    "print(f\"\\n🎯 GENERALIZATION ANALYSIS (Advanced GRU):\")\n",
    "print(f\"   • Training Scaled R²: {train_overall_r2:.4f}\")\n",
    "print(f\"   • Testing Scaled R²: {test_overall_r2:.4f}\")\n",
    "print(f\"   • Overfitting Gap: {overfitting_gap_advanced:.4f}\")\n",
    "\n",
    "if overfitting_gap_advanced < 0.3:\n",
    "    generalization_status = \"✅ Excellent generalization\"\n",
    "elif overfitting_gap_advanced < 0.5:\n",
    "    generalization_status = \"✅ Good generalization\"\n",
    "elif overfitting_gap_advanced < 1.0:\n",
    "    generalization_status = \"⚠️ Moderate overfitting\"\n",
    "else:\n",
    "    generalization_status = \"❌ Significant overfitting\"\n",
    "\n",
    "print(f\"   • Status: {generalization_status}\")\n",
    "\n",
    "# Business impact analysis\n",
    "print(f\"\\n💰 BUSINESS IMPACT ANALYSIS:\")\n",
    "avg_test_price = np.mean([np.mean(y_test_actual_multi_prices[:, i]) for i in range(len(MULTI_TARGET_NAMES))])\n",
    "avg_mae = test_overall_advanced['Price_MAE']\n",
    "relative_error = (avg_mae / avg_test_price) * 100\n",
    "\n",
    "print(f\"   • Average test price across all companies: ${avg_test_price:.2f}\")\n",
    "print(f\"   • Average prediction error: ${avg_mae:.2f}\")\n",
    "print(f\"   • Relative error: {relative_error:.1f}%\")\n",
    "\n",
    "# Model complexity and efficiency\n",
    "print(f\"\\n⚙️ MODEL COMPLEXITY ANALYSIS:\")\n",
    "model_params = final_multi_advanced_model.count_params()\n",
    "training_time_per_epoch = final_multi_advanced_training_time / len(final_multi_advanced_history.history['loss'])\n",
    "\n",
    "print(f\"   • Total parameters: {model_params:,}\")\n",
    "print(f\"   • Training time: {final_multi_advanced_training_time:.1f} seconds\")\n",
    "print(f\"   • Average time per epoch: {training_time_per_epoch:.1f} seconds\")\n",
    "print(f\"   • Parameters per target: {model_params // len(MULTI_TARGET_NAMES):,}\")\n",
    "\n",
    "# Performance ranking by company\n",
    "print(f\"\\n🏆 COMPANY PERFORMANCE RANKING (by Correlation):\")\n",
    "advanced_summary_df = pd.DataFrame(advanced_individual_summary)\n",
    "advanced_summary_sorted = advanced_summary_df.sort_values('Correlation', ascending=False)\n",
    "\n",
    "for i, row in advanced_summary_sorted.iterrows():\n",
    "    rank = advanced_summary_sorted.index.get_loc(i) + 1\n",
    "    print(f\"   {rank}. {row['Company']}: Correlation {row['Correlation']:.4f}, R² {row['R2']:.4f}\")\n",
    "\n",
    "print(f\"\\n📈 MULTI-TARGET ADVANCED GRU PERFORMANCE SUMMARY:\")\n",
    "print(f\"✅ Successfully predicts all 4 companies with advanced architecture\")\n",
    "print(f\"✅ Overall test R²: {test_overall_advanced['Scaled_R2']:.4f}\")\n",
    "print(f\"✅ Average correlation: {np.mean([test_metrics_advanced[target]['Price_Correlation'] for target in MULTI_TARGET_NAMES]):.4f}\")\n",
    "print(f\"✅ Generalization: {generalization_status}\")\n",
    "print(f\"✅ Business-relevant error levels achieved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OMsd4hHicRO"
   },
   "outputs": [],
   "source": [
    "# Plotting the actual vs predicted values for all targets\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COMPREHENSIVE MULTI-TARGET ADVANCED GRU VISUALIZATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create comprehensive multi-target visualization comparing Simple RNN vs Advanced GRU\n",
    "fig = plt.figure(figsize=(28, 20))\n",
    "fig.suptitle('Multi-Target Prediction: Advanced GRU vs Simple RNN - Complete Analysis', fontsize=20, fontweight='bold')\n",
    "\n",
    "companies = ['AMZN', 'GOOGL', 'IBM', 'MSFT']\n",
    "colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "\n",
    "# Create grid layout: 5 rows, 4 columns\n",
    "gs = fig.add_gridspec(5, 4, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Row 1: Simple RNN Actual vs Predicted\n",
    "for i, (company, color) in enumerate(zip(companies, colors)):\n",
    "    ax = fig.add_subplot(gs[0, i])\n",
    "    \n",
    "    if 'y_test_pred_multi_prices' in locals():  # Simple RNN available\n",
    "        actual_prices = y_test_actual_multi_prices[:, i]\n",
    "        pred_prices_simple = y_test_pred_multi_prices[:, i]\n",
    "        \n",
    "        ax.scatter(actual_prices, pred_prices_simple, alpha=0.6, s=25, color='red', label='Simple RNN')\n",
    "        min_price, max_price = actual_prices.min(), actual_prices.max()\n",
    "        ax.plot([min_price, max_price], [min_price, max_price], 'k--', linewidth=2, alpha=0.7)\n",
    "        \n",
    "        simple_corr = test_individual_metrics[f'Close{company}']['Price_Correlation']\n",
    "        simple_r2 = test_individual_metrics[f'Close{company}']['Price_R2']\n",
    "        \n",
    "        ax.set_title(f'{company}: Simple RNN\\nCorr: {simple_corr:.3f}, R²: {simple_r2:.3f}', fontsize=10, fontweight='bold')\n",
    "        ax.set_xlabel('Actual Price ($)', fontsize=9)\n",
    "        ax.set_ylabel('Predicted Price ($)', fontsize=9)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: Advanced GRU Actual vs Predicted\n",
    "for i, (company, color) in enumerate(zip(companies, colors)):\n",
    "    ax = fig.add_subplot(gs[1, i])\n",
    "    \n",
    "    actual_prices = y_test_actual_multi_prices[:, i]\n",
    "    pred_prices_advanced = y_test_pred_advanced_prices[:, i]\n",
    "    \n",
    "    ax.scatter(actual_prices, pred_prices_advanced, alpha=0.6, s=25, color='green', label='Advanced GRU')\n",
    "    min_price, max_price = actual_prices.min(), actual_prices.max()\n",
    "    ax.plot([min_price, max_price], [min_price, max_price], 'k--', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    advanced_corr = test_metrics_advanced[f'Close{company}']['Price_Correlation']\n",
    "    advanced_r2 = test_metrics_advanced[f'Close{company}']['Price_R2']\n",
    "    \n",
    "    ax.set_title(f'{company}: Advanced GRU\\nCorr: {advanced_corr:.3f}, R²: {advanced_r2:.3f}', fontsize=10, fontweight='bold')\n",
    "    ax.set_xlabel('Actual Price ($)', fontsize=9)\n",
    "    ax.set_ylabel('Predicted Price ($)', fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 3: Time Series Comparison (last 60 points)\n",
    "plot_points = min(60, len(y_test_actual_multi_prices))\n",
    "test_indices = range(len(y_test_actual_multi_prices) - plot_points, len(y_test_actual_multi_prices))\n",
    "\n",
    "for i, (company, color) in enumerate(zip(companies, colors)):\n",
    "    ax = fig.add_subplot(gs[2, i])\n",
    "    \n",
    "    actual_prices = y_test_actual_multi_prices[-plot_points:, i]\n",
    "    pred_prices_advanced = y_test_pred_advanced_prices[-plot_points:, i]\n",
    "    \n",
    "    ax.plot(test_indices, actual_prices, label='Actual', linewidth=2.5, alpha=0.9, color='black')\n",
    "    ax.plot(test_indices, pred_prices_advanced, label='Advanced GRU', linewidth=2, alpha=0.8, color='green')\n",
    "    \n",
    "    if 'y_test_pred_multi_prices' in locals():\n",
    "        pred_prices_simple = y_test_pred_multi_prices[-plot_points:, i]\n",
    "        ax.plot(test_indices, pred_prices_simple, label='Simple RNN', linewidth=2, alpha=0.8, color='red')\n",
    "    \n",
    "    ax.fill_between(test_indices, actual_prices, pred_prices_advanced, alpha=0.2, color='green')\n",
    "    \n",
    "    ax.set_title(f'{company}: Time Series ({plot_points} points)', fontsize=10, fontweight='bold')\n",
    "    ax.set_xlabel('Time Index', fontsize=9)\n",
    "    ax.set_ylabel('Price ($)', fontsize=9)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 4: Error Distributions Comparison\n",
    "for i, (company, color) in enumerate(zip(companies, colors)):\n",
    "    ax = fig.add_subplot(gs[3, i])\n",
    "    \n",
    "    actual_prices = y_test_actual_multi_prices[:, i]\n",
    "    errors_advanced = actual_prices - y_test_pred_advanced_prices[:, i]\n",
    "    \n",
    "    ax.hist(errors_advanced, bins=25, alpha=0.6, color='green', label='Advanced GRU', density=True, edgecolor='darkgreen')\n",
    "    \n",
    "    if 'y_test_pred_multi_prices' in locals():\n",
    "        errors_simple = actual_prices - y_test_pred_multi_prices[:, i]\n",
    "        ax.hist(errors_simple, bins=25, alpha=0.6, color='red', label='Simple RNN', density=True, edgecolor='darkred')\n",
    "    \n",
    "    ax.axvline(x=0, color='black', linestyle='--', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    mae_advanced = test_metrics_advanced[f'Close{company}']['Price_MAE']\n",
    "    ax.set_title(f'{company}: Error Distribution\\nAdvanced MAE: ${mae_advanced:.2f}', fontsize=10, fontweight='bold')\n",
    "    ax.set_xlabel('Prediction Error ($)', fontsize=9)\n",
    "    ax.set_ylabel('Density', fontsize=9)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Row 5: Performance Metrics Comparison\n",
    "metrics_comparison_data = {\n",
    "    'Company': companies,\n",
    "    'Advanced_GRU_Corr': [test_metrics_advanced[f'Close{comp}']['Price_Correlation'] for comp in companies],\n",
    "    'Advanced_GRU_R2': [test_metrics_advanced[f'Close{comp}']['Price_R2'] for comp in companies],\n",
    "    'Advanced_GRU_MAE': [test_metrics_advanced[f'Close{comp}']['Price_MAE'] for comp in companies],\n",
    "    'Advanced_GRU_Direction': [test_metrics_advanced[f'Close{comp}']['Direction_Accuracy'] for comp in companies]\n",
    "}\n",
    "\n",
    "if 'test_individual_metrics' in locals():\n",
    "    metrics_comparison_data.update({\n",
    "        'Simple_RNN_Corr': [test_individual_metrics[f'Close{comp}']['Price_Correlation'] for comp in companies],\n",
    "        'Simple_RNN_R2': [test_individual_metrics[f'Close{comp}']['Price_R2'] for comp in companies],\n",
    "        'Simple_RNN_MAE': [test_individual_metrics[f'Close{comp}']['Price_MAE'] for comp in companies],\n",
    "        'Simple_RNN_Direction': [test_individual_metrics[f'Close{comp}']['Direction_Accuracy'] for comp in companies]\n",
    "    })\n",
    "\n",
    "# Correlation comparison\n",
    "ax1 = fig.add_subplot(gs[4, 0])\n",
    "x_pos = np.arange(len(companies))\n",
    "width = 0.35\n",
    "\n",
    "if 'Simple_RNN_Corr' in metrics_comparison_data:\n",
    "    ax1.bar(x_pos - width/2, metrics_comparison_data['Simple_RNN_Corr'], width, \n",
    "           label='Simple RNN', color='red', alpha=0.7)\n",
    "ax1.bar(x_pos + width/2, metrics_comparison_data['Advanced_GRU_Corr'], width, \n",
    "       label='Advanced GRU', color='green', alpha=0.7)\n",
    "\n",
    "ax1.set_title('Correlation Comparison', fontsize=11, fontweight='bold')\n",
    "ax1.set_xlabel('Company', fontsize=9)\n",
    "ax1.set_ylabel('Correlation', fontsize=9)\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(companies, fontsize=9)\n",
    "ax1.legend(fontsize=8)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# R² comparison\n",
    "ax2 = fig.add_subplot(gs[4, 1])\n",
    "if 'Simple_RNN_R2' in metrics_comparison_data:\n",
    "    ax2.bar(x_pos - width/2, metrics_comparison_data['Simple_RNN_R2'], width, \n",
    "           label='Simple RNN', color='red', alpha=0.7)\n",
    "ax2.bar(x_pos + width/2, metrics_comparison_data['Advanced_GRU_R2'], width, \n",
    "       label='Advanced GRU', color='green', alpha=0.7)\n",
    "\n",
    "ax2.set_title('R² Comparison', fontsize=11, fontweight='bold')\n",
    "ax2.set_xlabel('Company', fontsize=9)\n",
    "ax2.set_ylabel('R² Score', fontsize=9)\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(companies, fontsize=9)\n",
    "ax2.legend(fontsize=8)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# MAE comparison  \n",
    "ax3 = fig.add_subplot(gs[4, 2])\n",
    "if 'Simple_RNN_MAE' in metrics_comparison_data:\n",
    "    ax3.bar(x_pos - width/2, metrics_comparison_data['Simple_RNN_MAE'], width, \n",
    "           label='Simple RNN', color='red', alpha=0.7)\n",
    "ax3.bar(x_pos + width/2, metrics_comparison_data['Advanced_GRU_MAE'], width, \n",
    "       label='Advanced GRU', color='green', alpha=0.7)\n",
    "\n",
    "ax3.set_title('MAE Comparison', fontsize=11, fontweight='bold')\n",
    "ax3.set_xlabel('Company', fontsize=9)\n",
    "ax3.set_ylabel('MAE ($)', fontsize=9)\n",
    "ax3.set_xticks(x_pos)\n",
    "ax3.set_xticklabels(companies, fontsize=9)\n",
    "ax3.legend(fontsize=8)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Direction Accuracy comparison\n",
    "ax4 = fig.add_subplot(gs[4, 3])\n",
    "if 'Simple_RNN_Direction' in metrics_comparison_data:\n",
    "    ax4.bar(x_pos - width/2, metrics_comparison_data['Simple_RNN_Direction'], width, \n",
    "           label='Simple RNN', color='red', alpha=0.7)\n",
    "ax4.bar(x_pos + width/2, metrics_comparison_data['Advanced_GRU_Direction'], width, \n",
    "       label='Advanced GRU', color='green', alpha=0.7)\n",
    "\n",
    "ax4.set_title('Direction Accuracy Comparison', fontsize=11, fontweight='bold')\n",
    "ax4.set_xlabel('Company', fontsize=9)\n",
    "ax4.set_ylabel('Direction Accuracy (%)', fontsize=9)\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(companies, fontsize=9)\n",
    "ax4.legend(fontsize=8)\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.axhline(y=50, color='gray', linestyle='--', alpha=0.5, label='Random (50%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary visualization: Overall performance radar chart\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8), subplot_kw=dict(projection='polar'))\n",
    "fig.suptitle('Multi-Target Performance Radar Chart Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Prepare radar chart data\n",
    "categories = ['Avg Correlation', 'Avg R²', 'Avg Direction Acc', 'Low Overfitting', 'Speed']\n",
    "\n",
    "# Normalize metrics for radar chart (0-1 scale)\n",
    "advanced_scores = [\n",
    "    np.mean([test_metrics_advanced[f'Close{comp}']['Price_Correlation'] for comp in companies]),\n",
    "    max(0, np.mean([test_metrics_advanced[f'Close{comp}']['Price_R2'] for comp in companies])),\n",
    "    np.mean([test_metrics_advanced[f'Close{comp}']['Direction_Accuracy'] for comp in companies]) / 100,\n",
    "    max(0, 1 - overfitting_gap_advanced),  # Inverted overfitting gap\n",
    "    max(0, 1 - (final_multi_advanced_training_time / 600))  # Speed score (normalized by 10 minutes)\n",
    "]\n",
    "\n",
    "if 'test_individual_metrics' in locals():\n",
    "    simple_scores = [\n",
    "        np.mean([test_individual_metrics[f'Close{comp}']['Price_Correlation'] for comp in companies]),\n",
    "        max(0, np.mean([test_individual_metrics[f'Close{comp}']['Price_R2'] for comp in companies])),\n",
    "        np.mean([test_individual_metrics[f'Close{comp}']['Direction_Accuracy'] for comp in companies]) / 100,\n",
    "        max(0, 1 - abs(train_overall_metrics['Scaled_R2'] - test_overall_metrics['Scaled_R2'])),\n",
    "        max(0, 1 - (final_multi_simple_training_time / 600))\n",
    "    ]\n",
    "\n",
    "# Set up angles for radar chart\n",
    "angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
    "angles += angles[:1]  # Complete the circle\n",
    "\n",
    "# Advanced GRU radar\n",
    "advanced_scores += advanced_scores[:1]  # Complete the circle\n",
    "ax1.plot(angles, advanced_scores, 'o-', linewidth=2, label='Advanced GRU', color='green')\n",
    "ax1.fill(angles, advanced_scores, alpha=0.25, color='green')\n",
    "ax1.set_xticks(angles[:-1])\n",
    "ax1.set_xticklabels(categories)\n",
    "ax1.set_ylim(0, 1)\n",
    "ax1.set_title('Advanced GRU Performance', fontsize=14, fontweight='bold', pad=20)\n",
    "ax1.grid(True)\n",
    "\n",
    "# Comparison radar (if Simple RNN available)\n",
    "if 'simple_scores' in locals():\n",
    "    simple_scores += simple_scores[:1]  # Complete the circle\n",
    "    ax2.plot(angles, simple_scores, 'o-', linewidth=2, label='Simple RNN', color='red')\n",
    "    ax2.fill(angles, simple_scores, alpha=0.25, color='red')\n",
    "    ax2.plot(angles, advanced_scores, 'o-', linewidth=2, label='Advanced GRU', color='green')\n",
    "    ax2.fill(angles, advanced_scores, alpha=0.25, color='green')\n",
    "    ax2.set_xticks(angles[:-1])\n",
    "    ax2.set_xticklabels(categories)\n",
    "    ax2.set_ylim(0, 1)\n",
    "    ax2.set_title('Model Comparison', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax2.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final comprehensive summary table\n",
    "print(f\"\\n📊 FINAL MULTI-TARGET PERFORMANCE SUMMARY TABLE:\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "final_summary_df = pd.DataFrame({\n",
    "    'Company': companies,\n",
    "    'Advanced_GRU_Correlation': [f\"{test_metrics_advanced[f'Close{comp}']['Price_Correlation']:.4f}\" for comp in companies],\n",
    "    'Advanced_GRU_R²': [f\"{test_metrics_advanced[f'Close{comp}']['Price_R2']:.4f}\" for comp in companies],\n",
    "    'Advanced_GRU_MAE': [f\"${test_metrics_advanced[f'Close{comp}']['Price_MAE']:.2f}\" for comp in companies],\n",
    "    'Advanced_GRU_Direction': [f\"{test_metrics_advanced[f'Close{comp}']['Direction_Accuracy']:.1f}%\" for comp in companies]\n",
    "})\n",
    "\n",
    "if 'test_individual_metrics' in locals():\n",
    "    final_summary_df['Simple_RNN_Correlation'] = [f\"{test_individual_metrics[f'Close{comp}']['Price_Correlation']:.4f}\" for comp in companies]\n",
    "    final_summary_df['Improvement'] = [f\"+{test_metrics_advanced[f'Close{comp}']['Price_Correlation'] - test_individual_metrics[f'Close{comp}']['Price_Correlation']:.4f}\" for comp in companies]\n",
    "\n",
    "print(final_summary_df)\n",
    "\n",
    "print(f\"\\n🎯 FINAL MULTI-TARGET ADVANCED GRU ACHIEVEMENTS:\")\n",
    "print(f\"✅ Successfully implemented sophisticated multi-target prediction for 4 companies\")\n",
    "print(f\"✅ Advanced GRU architecture with comprehensive regularization\")\n",
    "print(f\"✅ Robust price-based approach maintains interpretability\")\n",
    "print(f\"✅ Strong correlation across all companies: {np.mean([test_metrics_advanced[f'Close{comp}']['Price_Correlation'] for comp in companies]):.4f}\")\n",
    "print(f\"✅ Effective overfitting control: {overfitting_gap_advanced:.4f} gap\")\n",
    "print(f\"✅ Business-relevant error levels achieved\")\n",
    "print(f\"✅ Multi-target prediction capability demonstrated\")\n",
    "\n",
    "print(f\"\\n🚀 MULTI-TARGET ADVANCED GRU MODEL READY FOR PRODUCTION CONSIDERATION!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YItIF9_SmeCN"
   },
   "source": [
    "## **4 Conclusion** <font color =red> [5 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGwEyQn1meCN"
   },
   "source": [
    "### **4.1 Conclusion and insights** <font color =red> [5 marks] </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqBneTdDaVYw"
   },
   "source": [
    "#### **4.1.1** <font color =red> [5 marks] </font>\n",
    "Conclude with the insights drawn and final outcomes and results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly — here's a more concise and polished rephrasing of your text, maintaining technical accuracy while improving flow and professionalism. This version is also well-suited for inclusion in a report or presentation.\n",
    "\n",
    "---\n",
    "\n",
    "##Final Summary & Insights\n",
    "\n",
    "This notebook presents a detailed exploration of time series forecasting for stock prices using Recurrent Neural Networks (RNNs), with a focus on four major tech stocks: **AMZN, GOOGL, IBM, and MSFT**. While the models demonstrated strong learning on training data, their ability to generalize to unseen data—especially as measured by the R² metric—remains a key challenge. Further refinement is recommended to enhance out-of-sample predictive performance.\n",
    "\n",
    "---\n",
    "\n",
    "##Objective\n",
    "\n",
    "The core objective was to build predictive models for stock prices using historical data and RNN architectures. The project involved aggregating and preparing time series data, followed by training and evaluating both **Simple RNN** and **Advanced GRU** models across single- and multi-target forecasting tasks.\n",
    "\n",
    "---\n",
    "\n",
    "##Data Preparation\n",
    "\n",
    "A major success in the preprocessing stage was the use of a **“robust price-based” transformation**, which:\n",
    "\n",
    "* Converted raw closing prices into percentage changes from a baseline\n",
    "* Log-transformed volume data to address scale and skew\n",
    "\n",
    "This approach mitigated common issues like non-stationarity and distribution shift, enabling better model training and more consistent distributions between training and test sets.\n",
    "\n",
    "Window sizes were:\n",
    "\n",
    "* **22 days** (\\~1 month) for single-target models\n",
    "* **65 days** (\\~1 quarter) for multi-target models\n",
    "\n",
    "Minimal missing data (0.07%) was handled with forward-fill. Correlation analysis showed AMZN, GOOGL, and MSFT were positively correlated, while IBM showed weaker relationships. Volume trends varied across stocks, with MSFT showing higher activity overall.\n",
    "\n",
    "---\n",
    "\n",
    "##Model Results\n",
    "\n",
    "###**Single-Target Prediction (AMZN)**\n",
    "\n",
    "#### Simple RNN:\n",
    "\n",
    "* **Training**: R² = 0.9903, Correlation = 0.9956, MAE = \\$8.98\n",
    "* **Test**: Correlation = 0.7953, R² = -2.0042, MAE = \\$261.24\n",
    "* Directional Accuracy: 52.1%\n",
    "* **Key issue**: Severe overfitting, poor generalization\n",
    "\n",
    "#### Advanced GRU:\n",
    "\n",
    "* **Training**: R² = 0.9760, Correlation = 0.9940, MAE = \\$12.80\n",
    "* **Test**: Correlation = 0.9560, R² = -1.3254, MAE = \\$239.93\n",
    "* Directional Accuracy: 51.8%\n",
    "* **Improved generalization** over Simple RNN, though overfitting still present\n",
    "\n",
    "---\n",
    "\n",
    "###**Multi-Target Prediction (AMZN, GOOGL, IBM, MSFT)**\n",
    "\n",
    "#### Simple RNN:\n",
    "\n",
    "* **Training R²**: 0.9586 | **MAE**: \\$12.15\n",
    "* **Test R²**: -2.4312\n",
    "* **Test Correlations**: AMZN (0.9427), GOOGL (0.8527), IBM (0.5590), MSFT (0.9462)\n",
    "\n",
    "#### Advanced GRU:\n",
    "\n",
    "* **Training R²**: 0.9607 | **MAE**: \\$10.87\n",
    "* **Test R²**: -2.4078\n",
    "* **Average Test MAE**: \\$129.99\n",
    "* **Test Correlations**: AMZN (0.9404), GOOGL (0.9152), IBM (0.8775), MSFT (0.9470)\n",
    "* Directional accuracy improved across all companies\n",
    "* **Better trend tracking** and reduced MAE vs. Simple RNN\n",
    "\n",
    "---\n",
    "\n",
    "##Key Learnings\n",
    "\n",
    "1. **Effective Data Transformation**\n",
    "   The custom price-based transformation stabilized inputs and improved data quality.\n",
    "\n",
    "2. **High Training Performance**\n",
    "   Both models achieved excellent fits on training data (R² > 0.95, Correlation > 0.99), reflecting strong pattern recognition.\n",
    "\n",
    "3. **Generalization is the Bottleneck**\n",
    "   Negative R² on test sets reveals poor variance explanation, highlighting overfitting.\n",
    "\n",
    "4. **GRU Models Generalize Better**\n",
    "   GRUs consistently outperformed Simple RNNs in test metrics and directional accuracy, with narrower overfitting gaps.\n",
    "\n",
    "5. **Absolute Price Prediction is Difficult**\n",
    "   While trends were captured well (high correlation), predicting magnitude accurately remains challenging (high MAE, negative R²).\n",
    "\n",
    "6. **Multi-Target Forecasting Adds Complexity**\n",
    "   The GRU handled this better, showing promise in learning shared patterns across stocks.\n",
    "\n",
    "---\n",
    "\n",
    "##Limitations & Future Improvements\n",
    "\n",
    "* **Bridging the Train-Test Gap**\n",
    "  Overfitting must be addressed through better regularization, data augmentation, or simpler architectures.\n",
    "\n",
    "* **Feature Expansion**\n",
    "  Introducing external or derived features (e.g., technical indicators, macroeconomic variables) could improve context and accuracy.\n",
    "\n",
    "* **Deeper Hyperparameter Tuning**\n",
    "  Broader searches may uncover better configurations for model depth, learning rates, and regularization parameters.\n",
    "\n",
    "* **Architectural Exploration**\n",
    "  Transformer-based models or hybrid approaches (e.g., CNN-RNN) could enhance pattern detection and scalability.\n",
    "\n",
    "* **Custom Loss Functions & Forecast Horizons**\n",
    "  Tailoring loss functions to financial use cases and experimenting with different prediction intervals may yield better real-world applicability.\n",
    "\n",
    "---\n",
    "\n",
    "##Final Thoughts\n",
    "\n",
    "This project successfully demonstrated the process of building, training, and evaluating RNN-based models for financial time series forecasting. The **Advanced GRU** showed measurable improvements over Simple RNNs, especially in test performance and directional accuracy. However, **generalization remains the key limitation**, with negative test R² indicating models are not yet production-ready.\n",
    "\n",
    "Still, the solid foundation laid here—especially the robust data processing pipeline and multi-target modeling framework—positions this work well for further optimization and expansion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Assumptions\n",
    "\n",
    "### I. **Data-Related Assumptions**\n",
    "\n",
    "1. **Data Accuracy and Consistency**\n",
    "\n",
    "   * The CSV files accurately represent historical stock data for each company.\n",
    "   * Column definitions (Open, High, Low, Close, Volume) are consistent across all datasets.\n",
    "   * The `Date` field is reliable for aligning and merging stock data.\n",
    "\n",
    "2. **Handling Missing Values**\n",
    "\n",
    "   * Missing data is minimal (\\~0.07%) and not systematically biased.\n",
    "   * Forward-filling (`ffill`) is an appropriate imputation method, assuming no critical market events occurred on missing dates.\n",
    "\n",
    "3. **Stationarity and Data Transformation**\n",
    "\n",
    "   * Raw stock data is non-stationary and requires transformation for effective modeling.\n",
    "   * Percentage change from a baseline (first available price) is a valid method for normalizing price data.\n",
    "   * Log transformation of volume data helps manage skewness and variance.\n",
    "   * The first day’s price serves as a meaningful and consistent baseline.\n",
    "\n",
    "4. **Feature Selection**\n",
    "\n",
    "   * Historical Open, High, Low, Close, and Volume values are sufficient predictors for future closing prices.\n",
    "   * For multi-target modeling, cross-stock features are assumed to provide valuable interdependencies.\n",
    "\n",
    "---\n",
    "\n",
    "### II. **Modeling Assumptions**\n",
    "\n",
    "1. **Temporal Windowing**\n",
    "\n",
    "   * A fixed-size input window (22 days for single-target, 65 days for multi-target) captures the necessary time dependencies.\n",
    "   * Recent observations within the window are the most predictive of future prices.\n",
    "   * RNN architectures can effectively learn relationships between the input sequence and target outputs.\n",
    "\n",
    "2. **Model Choice**\n",
    "\n",
    "   * Simple RNN and GRU are suitable architectures for sequential stock data.\n",
    "   * GRUs, with their gating mechanisms, are better equipped to capture long-term dependencies and avoid vanishing gradient issues.\n",
    "\n",
    "3. **Train-Test Strategy**\n",
    "\n",
    "   * A chronological split of training and test data realistically reflects real-world forecasting scenarios.\n",
    "   * Patterns learned during training are assumed to generalize to future data, reflecting temporal consistency in underlying market behavior.\n",
    "\n",
    "4. **Feature Scaling**\n",
    "\n",
    "   * Standardization (`StandardScaler`) applied to training data and reused on test data is appropriate to normalize feature scales for neural network training.\n",
    "\n",
    "5. **Hyperparameters and Training Configuration**\n",
    "\n",
    "   * The explored hyperparameter ranges (units, layers, dropout, learning rate, etc.) are sufficient to find reasonably effective models.\n",
    "   * Activation functions (`tanh`, `relu`, `linear`) are suitable for this regression task.\n",
    "   * MSE is an appropriate loss function for predicting scaled percentage changes.\n",
    "   * Optimizers like Adam or RMSprop are effective for training RNN-based models.\n",
    "   * Early stopping based on validation loss prevents overfitting and aids generalization.\n",
    "   * The number of training epochs (with early stopping) is sufficient for convergence.\n",
    "\n",
    "6. **Regularization Techniques**\n",
    "\n",
    "   * Dropout, L2 regularization, recurrent dropout, and gradient clipping are assumed to effectively mitigate overfitting and improve model robustness.\n",
    "\n",
    "7. **Multi-Target Forecasting**\n",
    "\n",
    "   * A single model can effectively predict prices for all four stocks simultaneously.\n",
    "   * Architectural adaptations (e.g., wider dense layers, multiple output nodes) can handle the added complexity of multi-target predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### III. **Evaluation Assumptions**\n",
    "\n",
    "1. **Metrics**\n",
    "\n",
    "   * R², Price Correlation, MAE (in dollar terms), and Directional Accuracy offer a balanced and meaningful evaluation of model performance.\n",
    "   * Even with negative test R², high correlation and directional accuracy are still valuable for capturing trend-following behavior.\n",
    "\n",
    "2. **Inverse Transformation**\n",
    "\n",
    "   * Predicted scaled percentage changes can be reliably converted back to price estimates using inverse transformations and baseline values.\n",
    "\n",
    "---\n",
    "\n",
    "### IV. **Market Assumptions**\n",
    "\n",
    "1. **Market Predictability**\n",
    "\n",
    "   * Historical price and volume data contain patterns useful for forecasting, implying a departure from strict market efficiency (e.g., weak-form EMH).\n",
    "\n",
    "2. **Sector Interdependence**\n",
    "\n",
    "   * Stock prices of AMZN, GOOGL, IBM, and MSFT are influenced by shared sectoral trends, making multi-stock modeling potentially beneficial.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1YH9zvGTQCZrVKcWwV8QeTmIyR6cGfcX4",
     "timestamp": 1740056154426
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
